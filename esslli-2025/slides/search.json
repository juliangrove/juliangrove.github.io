[
  {
    "objectID": "background/background.html#background",
    "href": "background/background.html#background",
    "title": "Probabilistic dynamic semantics",
    "section": "Background",
    "text": "Background\nWho might benefit from this course?\n\nPeople who have a semantics background and want to see how experimental methodologies for studying meaning might serve their goals.\nPeople who have a computational cog sci or experimental background and want to see how their approach might connect up with semantic theory."
  },
  {
    "objectID": "background/background.html#some-papers-this-course-is-based-on",
    "href": "background/background.html#some-papers-this-course-is-based-on",
    "title": "Probabilistic dynamic semantics",
    "section": "Some papers this course is based on",
    "text": "Some papers this course is based on"
  },
  {
    "objectID": "background/background.html#a-couple-resources",
    "href": "background/background.html#a-couple-resources",
    "title": "Probabilistic dynamic semantics",
    "section": "A couple resources",
    "text": "A couple resources\n\nCourse notes: https://juliangrove.github.io/nasslli-2025/\nSome code: https://juliangrove.github.io/pds/"
  },
  {
    "objectID": "background/background.html#the-bridge-from-theory-to-data",
    "href": "background/background.html#the-bridge-from-theory-to-data",
    "title": "Probabilistic dynamic semantics",
    "section": "The bridge from theory to data",
    "text": "The bridge from theory to data\nSemantic theory has achieved remarkable success in characterizing compositional structure of meaning\n\nThe opportunity: Connect elegant formal theories to messy, gradient patterns from large-scale experiments\n\n\nThe goal: Maintain theoretical insights while extending to account for empirical richness\n\n\nProbabilistic Dynamic Semantics as systematic bridge\n\n\nWelcome everyone. Today we’re starting our journey into Probabilistic Dynamic Semantics by first understanding the motivation - why do we need a new framework?\nSemantic theory has given us elegant tools for understanding how meaning works compositionally. Think of how we can understand “Every linguist saw a student” by understanding the parts and how they combine. This has been incredibly successful.\nBut now we face an exciting opportunity. With large-scale experimental methods, we’re collecting thousands of judgments from hundreds of speakers. And what we find is messy - gradience everywhere, even where theory predicts categorical distinctions.\nOur goal with PDS is not to replace traditional semantics but to extend it. We want to maintain those hard-won theoretical insights while being able to account for and predict the patterns we see in experimental data.\nPDS will be our systematic bridge - taking compositional analyses from traditional Montagovian semantics and mapping them to probabilistic models we can test quantitatively."
  },
  {
    "objectID": "background/background.html#semantic-theorys-success",
    "href": "background/background.html#semantic-theorys-success",
    "title": "Probabilistic dynamic semantics",
    "section": "Semantic theory’s success",
    "text": "Semantic theory’s success\nElegant formal systems capturing how complex meanings arise from systematic combination\n\n\nDecades of careful theoretical work\nExplains acceptability and inference judgments\nCompositional analyses via Montagovian methods\n\n\n\nThe challenge\nHow to test theoretical predictions at unprecedented scale while maintaining formal rigor?\n\n\nLet’s appreciate what semantic theory has accomplished. Over decades, semanticists have developed formal systems that elegantly capture compositionality - how complex meanings arise from simpler parts.\nThese theories explain two fundamental types of judgments speakers make: whether sentences are acceptable in context, and what inferences follow from what we say. The Montagovian tradition gave us tools to be precise about these intuitions.\nBut here’s our challenge: How do we test these theories when we move from a handful of carefully constructed examples to thousands of judgments across hundreds of predicates? How do we maintain formal rigor while engaging with messy empirical reality?\nThis is what motivates PDS - we need a framework that respects the insights of formal semantics while enabling quantitative evaluation against large-scale data."
  },
  {
    "objectID": "background/background.html#dynamic-semantics",
    "href": "background/background.html#dynamic-semantics",
    "title": "Probabilistic dynamic semantics",
    "section": "Dynamic semantics",
    "text": "Dynamic semantics\n\nSome linguist walked in. They gave a lecture.\n\n\nPopular idea\nMany frameworks for dynamic semantics model sentence meanings as maps from input states to sets of output states (e.g., Groenendijk and Stokhof 1991; Muskens 1996, i.a.).\n\n\n\n1st sentence: \\(λs.\\{ x{::}s^{\\prime} ∣ s^{\\prime} = s ∧ \\ct{ling}(x) ∧ \\ct{walk}(x)\\}\\)\n2nd sentence: \\(λs.\\{ s^{\\prime} ∣ s^{\\prime} = s ∧ \\ct{lecture}(\\ct{sel}(s))\\}\\)"
  },
  {
    "objectID": "background/background.html#formal-pragmatics",
    "href": "background/background.html#formal-pragmatics",
    "title": "Probabilistic dynamic semantics",
    "section": "Formal pragmatics",
    "text": "Formal pragmatics\nMuch work has built on these ideas by enriching the notion of a state to countenance broader aspects of discourse structure.\n\nBesides entities for anaphora:\n\nCommon grounds (Stalnaker 1978, et seq.).\nQuestions under discussion (QUDs, Roberts 2012; Ginzburg 1996).\nContexts combining these and other structures (Farkas and Bruce 2010).\n\n\n\n\nEnriched states: central to PDS."
  },
  {
    "objectID": "background/background.html#standard-methodology",
    "href": "background/background.html#standard-methodology",
    "title": "Probabilistic dynamic semantics",
    "section": "Standard methodology",
    "text": "Standard methodology\nAcceptability judgments: assess whether strings are well-formed in context (Chomsky 1957; see Schütze 2016)\n \n\nExample: island effects (Ross 1967; see Sprouse and Villata 2021)\n\nWhat would you like with your coffee? ✓\n#What would you like and your coffee? ✗\n\n\n\nTraditional semantic methodology centers on two types of judgments. First, acceptability judgments - is a string well-formed in a given context?\nConsider these examples. In a context where a host asks what a guest wants with coffee, the first is clearly acceptable - using “with” to indicate accompaniment. But the second, trying to coordinate “what” and “your coffee”, is unacceptable. This follows from Ross’s coordinate structure constraint.\nThese judgments have been the bread and butter of linguistic theory. They’re usually quite clear - speakers have strong intuitions about what’s acceptable and what isn’t. This clarity has allowed theorists to build elegant accounts of linguistic constraints.\nBut as we’ll see, when we scale up to hundreds of predicates and thousands of judgments, things get more complex."
  },
  {
    "objectID": "background/background.html#standard-methodology-1",
    "href": "background/background.html#standard-methodology-1",
    "title": "Probabilistic dynamic semantics",
    "section": "Standard methodology",
    "text": "Standard methodology\nInference judgments: assess relationships between strings (see Davis and Gillon 2004 and references therein)\n\n\nExample: factivity (see White 2019 and references therein)\n\nJo {loves, doesn’t love} that Mo left. ⟹ Mo left.\n\n\n\nThe second pillar of semantic methodology is inference judgments - what follows from what speakers say?\nHere’s a classic example. When speakers hear “Jo loved that Mo left,” they typically infer that Mo actually left. This inference is particularly interesting because it survives under negation and questioning - “Jo didn’t love that Mo left” and “Did Jo love that Mo left?” both still suggest Mo left.\nThis pattern led theorists to identify a class of predicates - factives - that trigger presuppositions about their complements being true. We’ll return to factivity as one of our main case studies, because it presents a fascinating puzzle when we look at experimental data.\nThese inference judgments are central to semantic theory, but they also reveal complexities we need to address."
  },
  {
    "objectID": "background/background.html#observational-adequacy",
    "href": "background/background.html#observational-adequacy",
    "title": "Probabilistic dynamic semantics",
    "section": "Observational adequacy",
    "text": "Observational adequacy\nCore desideratum: predict acceptability and inference patterns for any string (Chomsky 1964)\n\nRequires mapping vocabulary to abstractions predicting judgments parsimoniously\nAbstractions: discrete or continuous, simple or richly structured\n\n\nA core goal of semantic theory is observational adequacy - for any string, we should predict whether speakers find it acceptable and what inferences they draw.\nAchieving this requires mapping vocabulary to abstractions. These abstractions might be discrete categories or continuous dimensions, simple features or rich structures. The key is parsimony - explaining the most data with the fewest theoretical commitments."
  },
  {
    "objectID": "background/background.html#observational-adequacy-1",
    "href": "background/background.html#observational-adequacy-1",
    "title": "Probabilistic dynamic semantics",
    "section": "Observational adequacy",
    "text": "Observational adequacy\n\nExample:\n\nlove, hate, be surprised, know share properties\nInferences survive under negation/questioning (Kiparsky and Kiparsky 1970; cf. Karttunen 1971)\nLeads to positing shared properties, such as factivity\n\n\n\nThrough careful analysis, semanticists have identified powerful generalizations. Take predicates like “love,” “hate,” “be surprised,” and “know.” They all give rise to inferences about their complements that survive under negation and questioning. This pattern led Kiparsky and Kiparsky to posit a shared property - factivity.\nThis is theoretical success at its best: identifying deep regularities across superficially different expressions. But as we’ll see when we look at experimental data, these generalizations become more complex at scale."
  },
  {
    "objectID": "background/background.html#descriptive-adequacy",
    "href": "background/background.html#descriptive-adequacy",
    "title": "Probabilistic dynamic semantics",
    "section": "Descriptive adequacy",
    "text": "Descriptive adequacy\nCore desideratum: capturing data “in terms of significant generalizations that express underlying regularities” (Chomsky 1964, 63)\n\nTwo approaches:\n\nAnalysis-driven: Start with observationally adequate analyses, extract constraints (Chomsky 1973)\nHypothesis-driven: Begin with constrained formalisms, test empirical coverage (Stabler 1997)\n\n\n\nPDS adopts hypothesis-driven approach for semantics\n\n\nBeyond observational adequacy lies a deeper goal: descriptive adequacy. We don’t just want to predict the data - we want to capture it in terms of significant generalizations that reveal underlying regularities in language.\nThe history of generative syntax shows two approaches to achieving this. The analysis-driven approach starts with observationally adequate analyses in expressive formalisms, then extracts constraints. Think of how Chomsky started with transformational grammars and extracted island constraints.\nThe hypothesis-driven approach works differently - begin with constrained formalisms like CCG or minimalist grammars, then test their empirical coverage. This approach makes phenomena boundaries clearer through representational constraints.\nPDS takes the hypothesis-driven approach for semantics. We start with constrained semantic representations and test what phenomena they can capture. This becomes especially important when we want models that both accord with theory AND can be evaluated quantitatively against behavioral data."
  },
  {
    "objectID": "background/background.html#our-approach",
    "href": "background/background.html#our-approach",
    "title": "Probabilistic dynamic semantics",
    "section": "Our approach",
    "text": "Our approach\nProbabilistic Dynamic Semantics aims to:\n\nProvide a relatively unconstrained formalism\nState testable hypotheses about distribution of judgments\n\nTest hypotheses using behavioral data\n\n\nKey innovation\nDelineate phenomena through representational constraints while enabling quantitative evaluation\n\n\nSo what’s our approach with PDS? We aim to provide a formalism that’s relatively unconstrained - flexible enough to encode different theoretical positions - while still being principled and compositional.\nThe key is that we can state testable hypotheses about the distribution of judgments, not just individual data points. When hundreds of people judge whether “Jo knows that Mo left” implies Mo left, we get a distribution of responses. PDS lets us predict these distributions from our semantic theory.\nOur key innovation is maintaining the delineation of phenomena through representational constraints - the hallmark of the hypothesis-driven approach - while enabling quantitative evaluation against behavioral data.\nAs we’ll see in the implementation, this means building probability distributions into the semantics compositionally, so that when meanings combine, the uncertainty combines too. This lets us maintain theoretical commitments while engaging with empirical complexity."
  },
  {
    "objectID": "background/background.html#standard-methods",
    "href": "background/background.html#standard-methods",
    "title": "Probabilistic dynamic semantics",
    "section": "Standard methods",
    "text": "Standard methods\nProfound insights achieved:\n\nSemantic composition\nScope phenomena\n\nDiscourse dynamics\n\n\nLet’s acknowledge what traditional methods have achieved - profound insights into how meaning works. We understand semantic composition, how scope ambiguities arise, how discourse context affects interpretation, and the delicate dance between semantics and pragmatics."
  },
  {
    "objectID": "background/background.html#standard-methods-1",
    "href": "background/background.html#standard-methods-1",
    "title": "Probabilistic dynamic semantics",
    "section": "Standard methods",
    "text": "Standard methods\n\nNatural boundaries:\n\nHow well do generalizations from 5-10 predicates extend to thousands?\nWhat factors beyond semantic knowledge influence judgments?\nHow does abstract knowledge produce concrete behavioral responses?\n\n\n\nBut every methodology has natural boundaries. Traditional methods excel at deep analysis of carefully chosen examples, but face constraints when we scale up.\nFirst, how well do generalizations from examining 5-10 predicates extend to the thousands in the lexicon? When we test 300 attitude predicates, do we find the neat categories theory predicts?\nSecond, what factors beyond pure semantic knowledge influence judgments? When people rate inferences, they’re using world knowledge, contextual reasoning, and response strategies alongside semantic competence.\nThird, how exactly does abstract semantic knowledge produce concrete behavioral responses? What cognitive processes intervene between computing a meaning and moving a slider on a scale?\nThese aren’t failures of traditional methods - they’re opportunities for extension. And that’s where experimental semantics comes in."
  },
  {
    "objectID": "background/background.html#experimental-semantics-and-pragmatics",
    "href": "background/background.html#experimental-semantics-and-pragmatics",
    "title": "Probabilistic dynamic semantics",
    "section": "Experimental semantics and pragmatics",
    "text": "Experimental semantics and pragmatics\nMethodologies like inference judgment tasks study larger lexical areas\nExisting challenges:\n\nUnclear how modeling constructs relate to semantic theory\nWhat lexical properties are implicated?\nHow do they interact with language-external factors?\n\n\nThe gradience question: What theoretical constructs underlie distributions of judgments?\n\n\nExperimental semantics and pragmatics have made great strides in scaling up semantic investigation. Using inference judgment tasks and other methodologies, we can study entire lexical domains rather than handfuls of examples.\nBut challenges remain. Often it’s unclear how the constructs in our statistical models relate to semantic theory. When we find that some predicates cluster together in their inference patterns, what lexical semantic properties are we actually discovering? And how do these properties interact with language-external factors like world knowledge?\nThe central challenge is the gradience question: when we get distributions of judgments rather than categorical patterns, what theoretical constructs underlie these distributions? Traditional semantic theory often assumes categorical distinctions, but the data shows continuity.\nThis is where PDS comes in - providing a framework where gradience can emerge from principled semantic sources while maintaining theoretical clarity about what those sources are."
  },
  {
    "objectID": "background/background.html#the-experimental-turn",
    "href": "background/background.html#the-experimental-turn",
    "title": "Probabilistic dynamic semantics",
    "section": "The experimental turn",
    "text": "The experimental turn\nExperimental semantics brings behavioral experimentation to meaning questions\nScaling semantic investigation\n\nTraditional: handful of predicates\nExperimental: entire lexical domains\n\n\nThe experimental turn in semantics represents a methodological revolution. We’re bringing the tools of behavioral experimentation - controlled stimuli, statistical analysis, large participant samples - to questions about meaning.\nThe scale difference is dramatic. Where traditional methods might examine 5-10 predicates in detail, experimental approaches can investigate entire lexical domains. This isn’t just more of the same - at scale, new patterns emerge that are invisible in small samples.\nLet me give you a concrete example with the MegaAttitude project, which we’ll look at next. This project studies hundreds of clause-embedding predicates - virtually the entire lexical class. At this scale, we can see gradience and clustering patterns that would be invisible looking at just “know,” “believe,” and “think.”\nThis scaling reveals both the power of traditional generalizations and their limits. Some patterns hold beautifully; others reveal unexpected complexity."
  },
  {
    "objectID": "background/background.html#megaattitude-project-example",
    "href": "background/background.html#megaattitude-project-example",
    "title": "Probabilistic dynamic semantics",
    "section": "MegaAttitude project example",
    "text": "MegaAttitude project example\nLarge-scale investigation of clause-embedding predicates (White and Rawlins 2016, 2018, 2020; White et al. 2018; An and White 2020; Moon and White 2020; Kane, Gantt, and White 2022)\n\nHundreds of predicates\nMultiple contexts and inference types\nReveals subtle patterns difficult to see traditionally\n\n\nThe MegaAttitude project exemplifies the power of experimental semantics at scale. This massive undertaking studies hundreds of clause-embedding predicates across multiple inference types and contexts.\nThe scale is unprecedented - instead of the usual suspects like “know” and “believe,” we have data on predicates like “sense,” “intimate,” “let on,” and hundreds more. Each predicate is tested in multiple syntactic frames, with different inference types, across many experimental participants.\nWhat emerges is fascinating. We see continuous gradients where theory predicted discrete categories. We find predicates clustering in unexpected ways. “Admit” patterns differently than “confess” despite their apparent similarity. Some predicates show bimodal response distributions suggesting genuine ambiguity, while others show unimodal gradience.\nThis rich empirical landscape is exactly what PDS is designed to model. We can ask: what theoretical properties produce these patterns? How do discrete semantic features combine with continuous uncertainty to yield the distributions we observe?"
  },
  {
    "objectID": "background/background.html#teasing-apart-contributing-factors",
    "href": "background/background.html#teasing-apart-contributing-factors",
    "title": "Probabilistic dynamic semantics",
    "section": "Teasing apart contributing factors",
    "text": "Teasing apart contributing factors\nMultiple factors influence inference judgments:\n\nSemantic knowledge: core meanings of expressions\nWorld knowledge: prior beliefs about plausibility (Degen and Tonhauser 2021 among others)\nContextual factors: discourse context and QUD (Simons et al. 2017 among others)\n\n\nOne major advantage of experimental methods is the ability to tease apart different factors that influence inference judgments. When someone judges whether “Jo knows that Mo left” implies Mo left, multiple cognitive systems are at play.\nFirst, semantic knowledge - the core meaning of “know” and how it composes with its complement. This is what traditional semantics focuses on.\nSecond, world knowledge. If Mo leaving is highly plausible given the context, people are more likely to infer it’s true. Degen and Tonhauser showed this beautifully by manipulating prior probabilities while holding predicates constant. Even canonical factives like “know” show gradient behavior when world knowledge varies.\nThird, contextual factors like the discourse context and question under discussion. What’s at-issue can affect which inferences project.\n[Continue to next slide for remaining factors]"
  },
  {
    "objectID": "background/background.html#teasing-apart-contributing-factors-1",
    "href": "background/background.html#teasing-apart-contributing-factors-1",
    "title": "Probabilistic dynamic semantics",
    "section": "Teasing apart contributing factors",
    "text": "Teasing apart contributing factors\n\nIndividual differences: variation in interpretation\nResponse strategies: how participants use rating scales\n\n\nThese are windows into cognitive processes, not confounds!\n\n\n[Continuing from previous slide]\nFourth, individual differences. Different speakers may have slightly different lexical entries or interpretation strategies. What looks like gradience at the population level might reflect discrete differences between individuals.\nFifth, response strategies - how participants map their internal judgments onto the response scale. A 7 on a slider might mean different things to different people.\nCrucially, these aren’t confounds to be eliminated - they’re windows into the cognitive processes underlying semantic interpretation. PDS provides a framework where each factor can be modeled explicitly and their interactions studied systematically.\nThis multiplicity of factors is why we need sophisticated frameworks. Simple models that ignore this complexity will miss crucial patterns in the data. But we also need principled ways to separate semantic from non-semantic factors, which is what PDS provides through its modular architecture."
  },
  {
    "objectID": "background/background.html#making-linking-hypotheses-explicit",
    "href": "background/background.html#making-linking-hypotheses-explicit",
    "title": "Probabilistic dynamic semantics",
    "section": "Making linking hypotheses explicit",
    "text": "Making linking hypotheses explicit\nExperimental approaches force explicit theorizing about (Jasbi, Waldon, and Degen 2019; Waldon and Degen 2020; Phillips et al. 2021)\n\nLink between semantic representations and behavioral responses\nWhat cognitive processes produce judgments?\nHow do abstract representations map onto scale responses?\n\n\nKey insight: Different linking hypotheses → different predictions about response patterns\nWe can’t ignore how representations map onto task responses\n\n\nOne of the most important contributions of experimental semantics is forcing us to be explicit about linking hypotheses - how abstract semantic representations connect to concrete behavioral responses.\nTraditional semantics can remain agnostic about this link. We say “know” triggers a presupposition, but how does that abstract property produce a slider response? What cognitive processes intervene?\nDifferent linking hypotheses make different predictions. Maybe people directly report their certainty about inferences. Or maybe they perform pragmatic reasoning first. Maybe they’re influenced by task demands or response biases.\nThe key insight is that we can’t ignore this linking problem if we want to test theories against data. Even if our real interest is in semantic representations, we need theories of how those representations produce behavior.\nPDS makes linking hypotheses explicit through response functions that map semantic computations to response distributions. As we’ll see in the implementation, different response functions embody different assumptions about this crucial link."
  },
  {
    "objectID": "background/background.html#understanding-gradience",
    "href": "background/background.html#understanding-gradience",
    "title": "Probabilistic dynamic semantics",
    "section": "Understanding gradience",
    "text": "Understanding gradience\nStriking finding: Pervasive gradient judgments\n\nEven where theory assumes categorical distinctions\nExamples: gradable adjectives (expected) vs. factivity (puzzling)\n\n\nUnderstanding gradience crucial for connecting semantics to behavioral data\n\n\nThe most striking finding from experimental semantics is the pervasiveness of gradient judgments. This isn’t just in domains where we expect it, like gradable adjectives where “tall” has no sharp boundary. We find gradience even where traditional theory assumes categorical distinctions.\nFactivity is the perfect example. Theory says predicates either trigger presuppositions or they don’t - “know” is factive, “think” isn’t. But experiments reveal continuous variation. Mean projection ratings vary smoothly from “pretend” at the bottom to “be annoyed” at the top, with no clear categorical break.\nMultiple studies have found these gradient patterns. Whether using projection tasks, prosodic manipulations, or other paradigms, the gradience persists. This isn’t measurement noise - it’s a systematic phenomenon demanding theoretical explanation.\nUnderstanding this gradience is crucial for connecting semantic theory to behavioral data. We need frameworks that can generate gradient predictions while maintaining theoretical clarity about what produces the gradience. That’s a core goal of PDS."
  },
  {
    "objectID": "background/background.html#examples-of-unexpected-gradience",
    "href": "background/background.html#examples-of-unexpected-gradience",
    "title": "Probabilistic dynamic semantics",
    "section": "Examples of unexpected gradience",
    "text": "Examples of unexpected gradience\nLarge-scale datasets reveal gradience where theory might predict sharper distinctions (White and Rawlins 2018)\n\n\n\n\nLet’s look at some concrete examples of unexpected gradience from the MegaAttitude datasets. These large-scale studies reveal patterns that aren’t readily apparent when examining just a few predicates.\nThis plot shows veridicality judgments from White and colleagues’ MegaVeridicality dataset. Each point represents a predicate’s mean inference strength under positive and negative embeddings. If factivity were a discrete property, we’d expect to see clear clusters - factive predicates in one region, non-factives in another.\nInstead, we see continuous variation across both dimensions. There’s no natural boundary where we could draw a line between factive and non-factive predicates. This gradience presents a real challenge for theories that assume categorical distinctions.\nBut the challenge goes deeper when we start looking at relationships between different properties…"
  },
  {
    "objectID": "background/background.html#deriving-measures-from-gradience",
    "href": "background/background.html#deriving-measures-from-gradience",
    "title": "Probabilistic dynamic semantics",
    "section": "Deriving measures from gradience",
    "text": "Deriving measures from gradience\nTo study relationships between properties, we need to derive measures\n\n\n\n\nWhen we have gradient data, we need principled ways to derive measures of theoretical properties. For factivity, one approach is to take the maximum of veridicality ratings under positive and negative embedding.\nThe logic is that factive predicates should trigger inferences about their complements being true regardless of matrix polarity. So “know” should score high on both dimensions, while “think” might score high only under positive embedding.\nThis derived measure gives us a single factivity score for each predicate. But now we face a new challenge: how do we interpret relationships between such continuous measures? The theoretical predictions were developed for categorical properties, not gradients.\nLet me show you why this matters with a concrete example…"
  },
  {
    "objectID": "background/background.html#relating-gradient-measures",
    "href": "background/background.html#relating-gradient-measures",
    "title": "Probabilistic dynamic semantics",
    "section": "Relating gradient measures",
    "text": "Relating gradient measures\nExample: Factivity vs. Neg-raising\n \n\nNeg-raising: Predicates licensing inferences like:\n\nJo doesn’t think that Mo left.\nJo thinks that Mo didn’t leave.\n\n\n\nExpectation: Factives are not neg-raisers\n\n\nLet’s examine how gradient measures relate to each other. Consider neg-raising - a phenomenon where negation seems to “lower” into the complement clause.\nA predicate is neg-raising if “X doesn’t think that P” can be interpreted as “X thinks that not-P.” Classic neg-raisers include “think,” “believe,” and “want.” The inference isn’t logically valid but is strongly preferred by speakers.\nTraditional theory suggests factives shouldn’t be neg-raisers. The reasoning is that factives presuppose their complements are true, which conflicts with the neg-raised reading where the complement is false.\nSo we have a clear theoretical prediction about the relationship between two properties. Let’s see what happens when both properties are measured as gradients…"
  },
  {
    "objectID": "background/background.html#testing-predictions-with-gradient-data",
    "href": "background/background.html#testing-predictions-with-gradient-data",
    "title": "Probabilistic dynamic semantics",
    "section": "Testing predictions with gradient data",
    "text": "Testing predictions with gradient data\nMegaNegRaising dataset (An and White 2020)\n\n\n\n\nHere we compare neg-raising scores from An and White’s MegaNegRaising dataset with our derived factivity measure. Each point is a predicate.\nIf the traditional generalization holds categorically, we’d expect points in the upper left (non-neg-raisers that are factive) and lower right (neg-raisers that are non-factive), but few in the upper right (both neg-raising and factive).\nWe do see this general trend - there’s a negative correlation. But it’s far from categorical. Some predicates score moderately on both dimensions. How do we interpret this? Is the generalization wrong, or does gradience emerge from other sources while the underlying relationship remains categorical?\nThis challenge - interpreting relationships between continuous measures - becomes even more complex when we look at other theoretical predictions…"
  },
  {
    "objectID": "background/background.html#beliefs-v.-desires",
    "href": "background/background.html#beliefs-v.-desires",
    "title": "Probabilistic dynamic semantics",
    "section": "Beliefs v. Desires",
    "text": "Beliefs v. Desires\nProposed Generalization: Predicates that trigger both belief and desire inferences background the belief component (Anand and Hacquard 2014)\n\nLet’s look at another theoretical relationship - between belief and desire inferences. Anand and Hacquard proposed that when predicates trigger inferences about both beliefs and desires, the belief component gets backgrounded."
  },
  {
    "objectID": "background/background.html#belief-inferences",
    "href": "background/background.html#belief-inferences",
    "title": "Probabilistic dynamic semantics",
    "section": "Belief Inferences",
    "text": "Belief Inferences\nMeasure of belief inferences from MegaIntensionality dataset (Kane, Gantt, and White 2022)\n\n\n\n\nThis makes an interesting prediction: predicates shouldn’t trigger strong inferences about both beliefs and desires simultaneously. If they trigger desire inferences, belief inferences should be weak.\nKane and colleagues’ MegaIntensionality dataset lets us test this. They collected judgments about whether various attitude predicates trigger inferences about the attitude holder’s beliefs and desires.\nHere we see the belief inference measure - how strongly each predicate suggests the attitude holder believes the complement. There’s clear variation across predicates, from “know” and “discover” at the top to “hope” and “want” at the bottom."
  },
  {
    "objectID": "background/background.html#desire-inferences",
    "href": "background/background.html#desire-inferences",
    "title": "Probabilistic dynamic semantics",
    "section": "Desire Inferences",
    "text": "Desire Inferences\nMeasure of desire inferences from MegaIntensionality dataset (Kane, Gantt, and White 2022)\n\n\n\n\nNow here’s the desire inference measure from the same dataset. Notice how the pattern is almost complementary to the belief measure.\nPredicates like “want,” “hope,” and “wish” trigger strong desire inferences, while “know,” “discover,” and “realize” trigger weak ones. This already suggests the predicted trade-off might hold.\nBut to really test Anand and Hacquard’s hypothesis, we need to look at the relationship between these two measures directly…"
  },
  {
    "objectID": "background/background.html#relating-belief-and-desire-inferences",
    "href": "background/background.html#relating-belief-and-desire-inferences",
    "title": "Probabilistic dynamic semantics",
    "section": "Relating belief and desire inferences",
    "text": "Relating belief and desire inferences\n\n\n\n\nThis plot shows the relationship between desire and belief inference scores. The pattern is striking - there’s a strong negative correlation. Predicates cluster along a diagonal, with very few triggering strong inferences about both beliefs and desires.\nThis supports Anand and Hacquard’s theoretical claim remarkably well. But notice how different this relationship looks from the factivity/neg-raising plot we saw earlier. There, the relationship was noisy with many intermediate cases. Here, it’s much cleaner.\nThis raises a crucial point: different theoretical relationships manifest differently in gradient data. Some show clear trade-offs, others show noisy correlations. We need frameworks that can theorize about these continuous relationships, not just categorical ones.\nThis is exactly what PDS provides - a way to model different sources of gradience and predict what kinds of relationships we should see in the data."
  },
  {
    "objectID": "background/background.html#key-takeaways-about-gradience",
    "href": "background/background.html#key-takeaways-about-gradience",
    "title": "Probabilistic dynamic semantics",
    "section": "Key takeaways about gradience",
    "text": "Key takeaways about gradience\n\nGradience is pervasive - even where theory assumes categories\nRelationships vary - some clean trade-offs, others noisy correlations\n\nNeed new theoretical tools - to understand continuous relationships\n\n\nThis motivates our taxonomy of uncertainty types…\n\n\nThese examples illustrate three key points about gradience in semantic judgments.\nFirst, gradience is pervasive. We find it not just in obviously gradient domains like adjectives, but even where theory has assumed categorical distinctions like factivity.\nSecond, relationships between gradient measures vary dramatically. The belief/desire trade-off is clean and systematic. The factivity/neg-raising relationship is noisy and probabilistic. These different patterns suggest different underlying mechanisms.\nThird, we need new theoretical tools. Traditional semantic theory was developed for categorical distinctions. When everything is gradient, how do we state and test theoretical claims? How do we distinguish noise from systematic variation?\nThis is why we need a principled taxonomy of uncertainty types. Different sources of gradience make different predictions about data patterns. Let’s turn to that taxonomy now…"
  },
  {
    "objectID": "background/background.html#a-taxonomy-of-uncertainty",
    "href": "background/background.html#a-taxonomy-of-uncertainty",
    "title": "Probabilistic dynamic semantics",
    "section": "A taxonomy of uncertainty",
    "text": "A taxonomy of uncertainty\nTwo fundamental types producing gradience:\n\nSources of Gradience\n├── Resolved (Type-Level) Uncertainty\n│   ├── Ambiguity\n│   │   ├── Lexical (e.g., \"run\" = locomote vs. manage)\n│   │   ├── Syntactic (e.g., attachment ambiguities)\n│   │   └── Semantic (e.g., scope ambiguities)\n│   └── Discourse Status\n│       └── QUD (Question Under Discussion)\n└── Unresolved (Token-Level) Uncertainty\n    ├── Vagueness (e.g., height threshold for \"tall\")\n    ├── World knowledge (e.g., likelihood of facts)\n    └── Task effects\n        ├── Response strategies\n        └── Response error\n\n\nTo understand gradience, we need a taxonomy of uncertainty. I’ll argue there are two fundamental types, and this distinction is crucial for building adequate models.\nResolved or type-level uncertainty involves discrete choices. There are multiple possible interpretations, and uncertainty about which one applies. This includes lexical ambiguity (run as locomotion vs. management), syntactic ambiguity (attachment sites), semantic ambiguity (scope), and discourse ambiguity (what’s the QUD).\nUnresolved or token-level uncertainty persists even after all ambiguities are resolved. This includes vagueness (where exactly is the threshold for “tall”?), world knowledge (how likely is it that Mo actually left?), and task effects (how do I use this slider scale?).\nThis distinction matters theoretically. Resolved uncertainty suggests discrete representations with probabilistic selection - mixture models. Unresolved uncertainty suggests gradient representations or continuous parameters.\nDifferent phenomena may involve different types, and PDS can model both. The framework’s power is in letting data adjudicate which type of uncertainty explains which patterns."
  },
  {
    "objectID": "background/background.html#resolved-uncertainty",
    "href": "background/background.html#resolved-uncertainty",
    "title": "Probabilistic dynamic semantics",
    "section": "Resolved uncertainty",
    "text": "Resolved uncertainty\nMultiple discrete possibilities—speakers choose among interpretations\nExample: “My uncle is running the race”\n\nrun = locomotion or management?\n“How likely is it that my uncle has good managerial skills?”\n\nLocomotion interpretation → ~0.2\nManagement interpretation → ~0.8\nPopulation average → ~0.5 (mixture of discrete interpretations)\n\n\n\nUncertainty “resolved” because once interpretation fixed, inference follows determinately\n\n\nLet’s look at resolved uncertainty in detail. The key characteristic is that there are multiple discrete possibilities, and speakers must choose among them.\nTake “My uncle is running the race.” The verb “run” is ambiguous between locomotion and management senses. If asked about the uncle’s managerial skills, people who interpret “run” as locomotion might respond around 0.2 - runners don’t necessarily have management skills. Those interpreting it as management might respond around 0.8 - race organizers likely have such skills.\nThe population average might be 0.5, but this is a mixture of discrete interpretations, not genuine gradience. No individual thinks the uncle is simultaneously sort-of-running and sort-of-managing.\nThis uncertainty is “resolved” because once we fix the interpretation, the inference follows determinately. If running means locomotion, the managerial inference is weak. If it means managing, the inference is strong. The gradience emerges from averaging across resolutions, not from uncertainty within any single interpretation.\nThis pattern - bimodal or multimodal response distributions - is a signature of resolved uncertainty."
  },
  {
    "objectID": "background/background.html#unresolved-uncertainty",
    "href": "background/background.html#unresolved-uncertainty",
    "title": "Probabilistic dynamic semantics",
    "section": "Unresolved uncertainty",
    "text": "Unresolved uncertainty\nPersists even after fixing all ambiguities\nExample: “My uncle is tall”\n\nNo ambiguity about tall’s meaning\nSpeakers uncertain about height threshold\nClassic vagueness—inherently gradient application (Fine 1975; Graff 2000; Christopher Kennedy 2007; Rooij 2011; Sorensen 2023)\n\n\nWorld knowledge: even knowing someone runs races, uncertainty about speed, endurance, etc. remains\n\n\nUnresolved uncertainty contrasts sharply with resolved uncertainty - it persists even after we’ve fixed all ambiguities.\nConsider “My uncle is tall.” There’s no ambiguity about what “tall” means - it’s about height exceeding some threshold. But speakers remain uncertain about exactly where that threshold is. Is 5’11” tall? 6’0”? 6’1”? This is classic vagueness - the predicate’s application conditions are inherently gradient.\nThe philosophical literature on vagueness is vast because it’s genuinely puzzling. Unlike ambiguity, we can’t just pick an interpretation and be done. The uncertainty is baked into the meaning itself.\nWorld knowledge creates additional layers of unresolved uncertainty. Even knowing someone runs races (locomotion sense, ambiguity resolved), we remain uncertain about their speed, endurance, likelihood of finishing. These uncertainties appear within individual trials, not just across participants.\nThis type of uncertainty typically produces unimodal, continuous response distributions - very different from the multimodal patterns of resolved uncertainty."
  },
  {
    "objectID": "background/background.html#why-this-distinction-matters",
    "href": "background/background.html#why-this-distinction-matters",
    "title": "Probabilistic dynamic semantics",
    "section": "Why this distinction matters",
    "text": "Why this distinction matters\nType of uncertainty has profound implications:\n\nResolved uncertainty → discrete representations with probabilistic selection\nUnresolved uncertainty → gradient representations or probabilistic reasoning within fixed meanings\n\n\nDifferent phenomena may involve different types:\n\nVagueness: unresolved (inherently uncertain application)\nFactivity: puzzling—resolved ambiguity or unresolved projection?\n\n\n\nThis distinction between resolved and unresolved uncertainty isn’t just taxonomic - it has profound implications for semantic theory and how we model behavioral data.\nIf gradience comes from resolved uncertainty, we need discrete semantic representations with probabilistic selection mechanisms. Think mixture models where people probabilistically choose among interpretations. The semantics stays discrete; the probability is in the selection.\nIf gradience comes from unresolved uncertainty, we need gradient representations or probabilistic reasoning within fixed meanings. The uncertainty is in the semantics itself, not just in choosing between options.\nDifferent phenomena may involve different types. Vagueness seems clearly to involve unresolved uncertainty - there’s no discrete set of “tall” meanings to choose from. But factivity is puzzling. Is the gradience from resolved uncertainty (ambiguous predicates, variable discourse conditions) or unresolved uncertainty (gradient projection mechanisms)?\nPDS lets us implement both possibilities and test them against data. As we’ll see in our case studies, the empirical patterns can distinguish these theoretical alternatives."
  },
  {
    "objectID": "background/background.html#case-studies-testing-theory-at-scale",
    "href": "background/background.html#case-studies-testing-theory-at-scale",
    "title": "Probabilistic dynamic semantics",
    "section": "Case studies: Testing theory at scale",
    "text": "Case studies: Testing theory at scale\nTwo case studies exemplifying different framework aspects:\n\nCase Study 1: Vagueness and gradable adjectives\n\nIdeal starting point—everyone agrees on gradient uncertainty\ntall, expensive, old lack sharp boundaries\n\n\n\nCase Study 2: Factivity and projection\n\nTraditional theory treats as discrete\nExperimental data reveals pervasive gradience\nA theoretical puzzle\n\n\n\nWe’ll explore two case studies that exemplify different aspects of the PDS framework and show how it bridges theory and data.\nOur first case study examines vagueness in gradable adjectives. This is an ideal starting point because everyone agrees these involve gradient uncertainty. Words like “tall,” “expensive,” and “old” lack sharp boundaries - there’s no precise height where someone becomes tall. This makes them perfect for demonstrating how PDS incorporates gradience into compositional semantics.\nOur second case study tackles factivity and projection - a much more controversial domain. Traditional theory treats factivity as discrete: predicates either trigger presuppositions or they don’t. But experimental data reveals pervasive gradience that’s difficult to explain. This presents a theoretical puzzle that PDS can help resolve.\nThese case studies aren’t just examples - they represent two poles of semantic phenomena. Adjectives show uncontroversial gradience from vagueness. Factivity shows surprising gradience where theory expected discreteness. Together, they demonstrate PDS’s range and power."
  },
  {
    "objectID": "background/background.html#case-study-1-vagueness",
    "href": "background/background.html#case-study-1-vagueness",
    "title": "Probabilistic dynamic semantics",
    "section": "Case Study 1: Vagueness",
    "text": "Case Study 1: Vagueness\nVague predicates lack sharp boundaries (Lakoff 1973; Sadock 1977; Lasersohn 1999; Krifka 2007; Solt 2015)\nEveryone agrees they involve gradient uncertainty\n\nMakes vagueness ideal for demonstrating PDS framework\n\n\nVague predicates have been recognized as problematic for classical semantics since antiquity. If “tall” means exceeding some height, what height? There seems to be no principled answer.\nThe modern literature on vagueness is vast and sophisticated. Linguists and philosophers have developed numerous approaches - supervaluation, fuzzy logic, epistemic theories, degree-based theories. What they share is recognizing that vagueness involves gradient uncertainty.\nThis consensus makes vagueness ideal for demonstrating the PDS framework. We’re not trying to convince anyone that adjectives involve gradience - that’s accepted. Instead, we can focus on how PDS incorporates this gradience into compositional semantics while maintaining formal rigor.\nWe’ll see how degree-based theories of adjectives - already probabilistic in spirit - map naturally into PDS. The framework adds explicit probability distributions where traditional theories left things implicit."
  },
  {
    "objectID": "background/background.html#case-study-1-vagueness-1",
    "href": "background/background.html#case-study-1-vagueness-1",
    "title": "Probabilistic dynamic semantics",
    "section": "Case Study 1: Vagueness",
    "text": "Case Study 1: Vagueness\nDegree-based theories long recognized gradience (Klein 1980; Bierwisch 1989; Kamp 1975; Chris Kennedy 1999; Christopher Kennedy and McNally 2005; Christopher Kennedy 2007; Barker 2002)\nAnalysis: tall is true of \\(x\\) if height(\\(x\\)) \\(≥ d_{\\text{tall}}\\)(context) - Threshold varies with context - Gradient judgments even within fixed context"
  },
  {
    "objectID": "background/background.html#case-study-1-vagueness-2",
    "href": "background/background.html#case-study-1-vagueness-2",
    "title": "Probabilistic dynamic semantics",
    "section": "Case Study 1: Vagueness",
    "text": "Case Study 1: Vagueness\nPDS can:\n\nMaintain compositional degree-based analysis\nAdd probability distributions over thresholds\nModel context shifts\nLink distributions to slider responses\n\n\nMakes vagueness ideal for demonstrating framework"
  },
  {
    "objectID": "background/background.html#case-study-1-vagueness-3",
    "href": "background/background.html#case-study-1-vagueness-3",
    "title": "Probabilistic dynamic semantics",
    "section": "Case Study 1: Vagueness",
    "text": "Case Study 1: Vagueness\nRecent work reveals additional complexity:\n\nRelative adjectives (tall, wide): maximum gradience\nAbsolute adjectives (clean, dry): different threshold distributions\nMinimum vs. maximum standard: asymmetric patterns\n\n\nRecent experimental work on adjectives reveals complexity beyond what traditional theories predicted. Different types of adjectives show distinct patterns of gradience.\nRelative adjectives like “tall” and “wide” show maximum gradience in their positive form. There’s no inherent standard - tallness is always relative to a comparison class. This produces smooth, continuous judgment curves.\nAbsolute adjectives like “clean” and “dry” work differently. They have endpoint-oriented standards - “clean” means no dirt, “dry” means no moisture. But experimentally, they still show gradience, just with different threshold distributions clustered near the endpoints.\nThere are also asymmetries between minimum and maximum standard adjectives. “Dirty” (minimum standard - any dirt suffices) patterns differently from “clean” (maximum standard - no dirt allowed). These patterns both support and refine formal theories.\n[Continue to next slide for computational integration]"
  },
  {
    "objectID": "background/background.html#case-study-1-integration-with-models",
    "href": "background/background.html#case-study-1-integration-with-models",
    "title": "Probabilistic dynamic semantics",
    "section": "Case Study 1: Integration with models",
    "text": "Case Study 1: Integration with models\n\n(Lassiter and Goodman 2013, 2017; Qing and Franke 2014; Kao et al. 2014; Bumford and Rett 2021)\n\nPDS synthesizes and compares these approaches\nBoth support and refine formal theories"
  },
  {
    "objectID": "background/background.html#how",
    "href": "background/background.html#how",
    "title": "Probabilistic dynamic semantics",
    "section": "How?",
    "text": "How?\n\nMore or less: \n\\(⟦\\textit{how likely is it that X is tall}⟧ ⇒\\)\nparameters {\n  real v;\n  real&lt;lower=0.0, upper=1.0&gt; w;\n}\n\nmodel {\n  v ~ normal(0.0, 1.0);\n\n  target += normal_lpdf(y | 1.0 - normal_cdf(v, 0.0, 1.0), w);\n}\n\n\nRecent years have seen partial integration of adjective theories into computational models. These models make quantitative predictions about judgment patterns, but often in framework-specific ways.\nLassiter and colleagues model threshold uncertainty using Bayesian pragmatics. Qing and Franke use similar ideas for gradable adjectives. Kao et al. extend this to non-literal uses. Each approach captures important insights but uses different formal frameworks.\nPDS provides a unifying framework where these different approaches can be compared. By implementing different theories within PDS, we can test their predictions against the same data using the same evaluation metrics.\nThis synthesis is powerful. We can ask: Does threshold uncertainty alone explain the patterns, or do we need pragmatic reasoning too? How do different linking hypotheses affect model fit? PDS enables principled theory comparison while maintaining compositionality.\nThe experimental patterns both support formal theories (yes, there are thresholds) and refine them (the threshold distributions are more complex than expected)."
  },
  {
    "objectID": "background/background.html#case-study-2-factivity",
    "href": "background/background.html#case-study-2-factivity",
    "title": "Probabilistic dynamic semantics",
    "section": "Case Study 2: Factivity",
    "text": "Case Study 2: Factivity\nTraditional theory: predicates either trigger presuppositions or don’t (Kiparsky and Kiparsky 1970; Karttunen 1971)\n   Example: love appears factive\n\nJo loves that Mo left. ⟹ Mo left.\nJo doesn’t love that Mo left. ⟹ Mo left.\nDoes Jo love that Mo left? ⟹ Mo left.\n\n\nNow let’s turn to factivity - a phenomenon that presents a real puzzle for connecting theory to data.\nTraditional theory treats factivity as a discrete property. Predicates either trigger presuppositions about their complements or they don’t. “Know” is factive - it presupposes its complement is true. “Think” is non-factive - it doesn’t.\nThe classic diagnostic uses the “family of sentences” test. A predicate is factive if the inference about its complement survives under negation and questioning. “Love” appears factive because all three sentences suggest Mo left - the positive assertion, the negation, and the question.\nThis categorical view made sense given traditional methodology. Looking at clear cases like “know” vs. “think,” the distinction seems sharp. But what happens when we test hundreds of predicates with hundreds of participants?\nSpoiler: we find pervasive gradience that’s difficult to explain under the discrete view."
  },
  {
    "objectID": "background/background.html#case-study-2-factivity---definition",
    "href": "background/background.html#case-study-2-factivity---definition",
    "title": "Probabilistic dynamic semantics",
    "section": "Case Study 2: Factivity - Definition",
    "text": "Case Study 2: Factivity - Definition\nA predicate is factive if it triggers inferences about its complement that project through entailment-canceling operators\nInference about complement truth survives:\n\nNegation\nQuestions\nOther embeddings\n\n\nLet’s be precise about what factivity means. A predicate is factive if it triggers inferences about its complement that project through entailment-canceling operators.\nWhat are entailment-canceling operators? Negation is the classic example - “Jo laughed” entails Jo exists, but “Jo didn’t laugh” doesn’t. Questions are another - “Did Jo laugh?” doesn’t entail Jo laughed.\nBut with factive predicates, complement inferences survive these operators. “Jo knows that Mo left” suggests Mo left. So does “Jo doesn’t know that Mo left” and “Does Jo know that Mo left?” The inference projects through the operators.\nThis projection behavior is what makes factivity special and theoretically interesting. It seems to mark a natural class of predicates with shared semantic properties. But as we’ll see, the empirical reality is more complex than this neat categorical picture suggests."
  },
  {
    "objectID": "background/background.html#case-study-2-factivity-1",
    "href": "background/background.html#case-study-2-factivity-1",
    "title": "Probabilistic dynamic semantics",
    "section": "Case Study 2: Factivity",
    "text": "Case Study 2: Factivity\nWhite and Rawlins (2018) and Degen and Tonhauser (2022) found continuous variation:\n\nNo clear line between factive and non-factive predicates\nMean projection ratings vary continuously\npretend (lowest) to be annoyed (highest)\n\n\nHere’s where things get interesting. When White and colleagues tested hundreds of attitude predicates, they found continuous variation in projection strength - not the discrete categories theory predicted.\nLook at this plot. Each point is a predicate’s mean projection rating. If factivity were discrete, we’d expect clustering - a factive group with high projection, a non-factive group with low projection. Instead, we see a smooth gradient from “pretend” at the bottom to “be annoyed” at the top.\nWhere would you draw the line between factive and non-factive? At 0.5? Why not 0.6 or 0.4? The data doesn’t suggest any natural boundary.\nThis is a serious puzzle. Either our theoretical understanding is wrong - maybe factivity isn’t discrete - or something about the relationship between discrete semantic properties and gradient behavioral responses needs explaining. PDS lets us explore both possibilities formally."
  },
  {
    "objectID": "background/background.html#case-study-2-factivity-2",
    "href": "background/background.html#case-study-2-factivity-2",
    "title": "Probabilistic dynamic semantics",
    "section": "Case Study 2: Factivity",
    "text": "Case Study 2: Factivity\n\n\n\n\nHere’s where things get interesting. When White and colleagues tested hundreds of attitude predicates, they found continuous variation in projection strength - not the discrete categories theory predicted.\nLook at this plot. Each point is a predicate’s mean projection rating. If factivity were discrete, we’d expect clustering - a factive group with high projection, a non-factive group with low projection. Instead, we see a smooth gradient from “pretend” at the bottom to “be annoyed” at the top.\nWhere would you draw the line between factive and non-factive? At 0.5? Why not 0.6 or 0.4? The data doesn’t suggest any natural boundary.\nThis is a serious puzzle. Either our theoretical understanding is wrong - maybe factivity isn’t discrete - or something about the relationship between discrete semantic properties and gradient behavioral responses needs explaining. PDS lets us explore both possibilities formally."
  },
  {
    "objectID": "background/background.html#factivity-fundamental-discreteness",
    "href": "background/background.html#factivity-fundamental-discreteness",
    "title": "Probabilistic dynamic semantics",
    "section": "Factivity: Fundamental Discreteness",
    "text": "Factivity: Fundamental Discreteness\nHypothesis 1: Factivity is discrete; gradience arises from:\n\nMultiple predicate senses (factive and non-factive variants)\nStructural ambiguity affecting projection (Varlokosta 1994; Giannakidou 1998, 1999, 2009; Roussou 2010; Farudi 2007; Abrusán 2011; Kastner 2015; Ozyildiz 2017)\nContextual variation in whether complements are at-issue (Simons et al. 2017; Roberts and Simons 2024; Qing, Goodman, and Lassiter 2016)\n\n\nThe Fundamental Discreteness hypothesis maintains that factivity is discrete, but gradience emerges from other sources. This preserves traditional theoretical insights while explaining the puzzling data.\nFirst, predicates might have multiple senses - factive and non-factive variants. “Realize” might have a factive sense (coming to know) and a non-factive sense (understanding). Population-level gradience would reflect mixture across senses.\nSecond, structural ambiguity might affect projection. Different syntactic analyses of the same string might have different projection properties. The complement might be parsed as truly embedded under the attitude verb, or as a parenthetical, affecting whether presuppositions project.\nThird, contextual factors like at-issueness might vary. When complements address the question under discussion, their content might not project as strongly. This would create gradience even with discrete underlying mechanisms.\nUnder this view, factivity remains a discrete semantic property, but various factors create gradient behavioral patterns."
  },
  {
    "objectID": "background/background.html#factivity-fundamental-gradience",
    "href": "background/background.html#factivity-fundamental-gradience",
    "title": "Probabilistic dynamic semantics",
    "section": "Factivity: Fundamental Gradience",
    "text": "Factivity: Fundamental Gradience\nHypothesis 2: No discrete property exists\n\nGradient degrees of complement truth support\nContinuous variation reflects semantic reality (Tonhauser, Beaver, and Degen 2018)\n\n\nThe Fundamental Gradience hypothesis takes a more radical position: there’s no discrete factivity property at all. Instead, predicates provide gradient degrees of support for their complement’s truth.\nUnder this view, the continuous variation we observe in experiments directly reflects semantic reality. “Know” strongly supports its complement being true, “think” weakly supports it, and other predicates fall at various points between. There’s no categorical distinction, just a continuum.\nThis aligns with Tonhauser and colleagues’ Gradient Projection Principle - that projection strength varies continuously based on multiple factors including lexical semantics, world knowledge, and discourse structure.\nThis hypothesis is theoretically simpler in some ways - no need to explain away the gradience. But it’s more radical, abandoning a distinction that seemed well-motivated by traditional data. It also raises questions about how gradient projection would work compositionally.\nPDS lets us implement both hypotheses formally and test their predictions."
  },
  {
    "objectID": "background/background.html#factivity-testing-both-hypotheses",
    "href": "background/background.html#factivity-testing-both-hypotheses",
    "title": "Probabilistic dynamic semantics",
    "section": "Factivity: Testing both hypotheses",
    "text": "Factivity: Testing both hypotheses\n\nPDS enables testing both hypotheses against fine-grained response distributions\nNot just means, but entire patterns including multimodality\n\n\nThe power of PDS is that we can implement both hypotheses formally and test their predictions against fine-grained data patterns - not just average ratings but entire response distributions.\nIf factivity is discrete with gradience from mixture, we’d expect multimodal response distributions. Some participants interpret predicates as factive (high ratings), others as non-factive (low ratings), producing bimodality.\nIf factivity is fundamentally gradient, we’d expect unimodal distributions centered at different points for different predicates. The variation would be smooth and continuous.\nPDS can generate predictions for both patterns. We can implement discrete factivity with probabilistic selection between variants, or gradient factivity with continuous parameters. The data can then adjudicate between theories.\nAs we’ll see in the implementation section, the discrete hypothesis actually fits the data better - suggesting that despite surface gradience, factivity involves categorical distinctions. This vindicates the traditional view while explaining the puzzling experimental patterns."
  },
  {
    "objectID": "background/background.html#factivity-testing-paradigms",
    "href": "background/background.html#factivity-testing-paradigms",
    "title": "Probabilistic dynamic semantics",
    "section": "Factivity: Testing paradigms",
    "text": "Factivity: Testing paradigms\nVarious experimental paradigms:\n\nProsodic manipulations: Tonhauser (2016); Djärv and Bacovcin (2017); Jeong (2021)\nPrior beliefs: Degen and Tonhauser (2021)\nLarge-scale judgments: White and Rawlins (2018); White et al. (2018); Degen and Tonhauser (2022); Kane, Gantt, and White (2022)\n\n\nThe factivity puzzle has motivated diverse experimental paradigms, each revealing different aspects of the phenomenon.\nProsodic manipulation studies show that focus and accent placement affect projection strength. This supports the view that information structure matters - focused content is less likely to project.\nLarge-scale judgment studies like MegaVeridicality test hundreds of predicates systematically. These reveal the full gradient landscape rather than just endpoints.\nPrior belief manipulations tease apart semantic from world knowledge contributions. Even canonical factives show reduced projection when complements are implausible.\nEach paradigm provides a different window into the phenomenon. PDS models can incorporate insights from all of them - prosodic effects through discourse parameters, cognitive effects through response noise, prior beliefs through world knowledge distributions."
  },
  {
    "objectID": "background/background.html#theoretical-challenges-from-gradience",
    "href": "background/background.html#theoretical-challenges-from-gradience",
    "title": "Probabilistic dynamic semantics",
    "section": "Theoretical challenges from gradience",
    "text": "Theoretical challenges from gradience\nThe gradience poses challenges (Simons 2007; Simons et al. 2010, 2017; Tonhauser, Beaver, and Degen 2018)\n\nKey questions:\n\nIs the gradience fundamental or derived?\nWhat mechanisms produce continuous variation?\nHow do semantic and pragmatic factors interact?\n\n\n\nThe pervasive gradience in projection poses serious theoretical challenges that have motivated substantial recent work.\nThe fundamental question is whether gradience is a core property of projection or derived from other sources. If fundamental, we need gradient semantic representations - a major revision to standard theories. If derived, we need to identify the sources and show how they produce the observed patterns.\nWe also need mechanisms that produce continuous variation. Discrete categories usually produce discrete behaviors. How do we get smooth gradients? Through probabilistic selection? Continuous parameters? Multiple interacting factors?\nFinally, how do semantic and pragmatic factors interact? Is projection computed by semantic mechanisms then modulated by pragmatics? Or is it pragmatic from the start?\nPDS provides tools to address these questions formally. We can implement different theoretical positions, derive their predictions, and test them against data. This moves debates from intuition to empirical testing."
  },
  {
    "objectID": "background/background.html#what-pds-provides",
    "href": "background/background.html#what-pds-provides",
    "title": "Probabilistic dynamic semantics",
    "section": "What PDS provides",
    "text": "What PDS provides\nA possible discourse:\n\nPrior knowledge: Zoe is a math major\nSome predicate: X discovered that Zoe calculated the tip\nQuestion: How likely is it that Zoe calculated the tip?\n\n\n\\[⇒\\]\nmodel {\n  v ~ logit_normal(0.0, 1.0);\n  w ~ logit_normal(0.0, 1.0);\n\n  target += log_mix(v, truncated_normal_lpdf(y | 1.0, u, 0.0, 1.0), truncated_normal_lpdf(y | w, u, 0.0, 1.0));\n}"
  },
  {
    "objectID": "background/background.html#the-need-for-new-frameworks",
    "href": "background/background.html#the-need-for-new-frameworks",
    "title": "Probabilistic dynamic semantics",
    "section": "The need for new frameworks",
    "text": "The need for new frameworks\nCase studies illustrate four key requirements:\n1. Maintain Compositionality\n\nDerive meanings compositionally\nPreserve formal semantic insights\nCan’t abandon compositionality for gradience\n\n\n2. Model Uncertainty Explicitly\n\nRepresent resolved and unresolved uncertainty\nShow interaction during interpretation\n\n\n\nOur case studies illustrate what we need from a framework connecting formal semantics to experimental data.\nFirst, we must maintain compositionality. The meanings of complex expressions must be derived systematically from their parts. We can’t abandon this core principle just because judgments are gradient. Decades of semantic research have shown the power of compositional analysis - we need to preserve these insights.\nSecond, we need to model uncertainty explicitly. The framework must represent both types we’ve discussed - resolved uncertainty from discrete choices and unresolved uncertainty from inherent gradience. Crucially, it must show how these interact during interpretation.\n[Continue to next slide for requirements 3 and 4]"
  },
  {
    "objectID": "background/background.html#framework-requirements",
    "href": "background/background.html#framework-requirements",
    "title": "Probabilistic dynamic semantics",
    "section": "Framework requirements",
    "text": "Framework requirements\n3. Make Linking Hypotheses Precise\n\nExplicit theories: representation → behavior\nWhat processes between meaning and slider?\n\n\n4. Enable Quantitative Evaluation\n\nTestable predictions about distributions\nCompare theories using standard metrics\n\n\n\n[Continuing from previous slide]\nThird, we must make linking hypotheses precise. We need explicit theories of how semantic representations produce behavioral responses. What cognitive processes intervene between computing a meaning and moving a slider? Different assumptions lead to different predictions.\nFourth, the framework must enable quantitative evaluation. Theories should make testable predictions about response distributions, not just qualitative patterns. Different theories must be comparable using standard statistical metrics so we can determine which best explains the data.\nThese requirements are demanding. We need a framework that’s both theoretically principled and empirically adequate. Existing approaches often excel at one or the other. PDS aims to satisfy all four requirements simultaneously."
  },
  {
    "objectID": "background/background.html#moving-forward",
    "href": "background/background.html#moving-forward",
    "title": "Probabilistic dynamic semantics",
    "section": "Moving forward",
    "text": "Moving forward\nExisting computational approaches (e.g., RSA) bridge formal semantics with probabilistic reasoning (Frank and Goodman 2012; Goodman and Stuhlmüller 2013)\n\nChallenges with existing approaches:\n\nDifficulty maintaining modularity\nOften blur semantics/pragmatics distinction\nConnection to traditional theory somewhat opaque\n\n\n\nBefore introducing PDS in detail, let’s acknowledge existing computational approaches, particularly Rational Speech Act (RSA) models. These have made important strides in bridging formal semantics with probabilistic reasoning.\nRSA models formalize Gricean reasoning - speakers choose utterances to convey information, listeners interpret utterances by reasoning about speaker intentions. This provides a principled way to derive pragmatic inferences from literal meanings.\nBut RSA faces challenges when scaling to the phenomena we’ve discussed. It’s difficult to maintain modularity - semantic and pragmatic components become intertwined. The distinction between what’s semantic and what’s pragmatic often blurs.\nMost importantly, the connection to traditional compositional semantics is somewhat opaque. RSA typically operates on complete utterance meanings rather than building them compositionally. This makes it hard to leverage existing semantic insights.\n[Continue to next slide for PDS motivation]"
  },
  {
    "objectID": "background/background.html#motivating-pds",
    "href": "background/background.html#motivating-pds",
    "title": "Probabilistic dynamic semantics",
    "section": "Motivating PDS",
    "text": "Motivating PDS\nThis motivates Probabilistic Dynamic Semantics:\n\nPreserves semantic insights (the “semantics” part)\nAdds probabilistic tools for gradient data (the “probabilistic” part)\nModels experimental tasks as complex discourses (the “dynamic” part)\nMaintains theoretical commitments while enabling tests\n\n\nPDS provides the framework we need to bridge theory and data\n\n\nThis brings us to Probabilistic Dynamic Semantics. PDS is designed to address the challenges we’ve identified while building on the successes of both traditional and computational approaches.\nPDS preserves the insights of formal semantics - compositionality, precise truth conditions, systematic meaning derivation. But it adds probabilistic tools needed to model gradient behavioral data. Uncertainty is built into the semantic representations themselves, not added post-hoc.\nCrucially, PDS maintains theoretical commitments while enabling quantitative tests. You can implement your favorite semantic theory within PDS and test its predictions. Different theories become comparable through their empirical adequacy.\nThe framework provides exactly what we need - a principled bridge from compositional semantic theory to gradient behavioral data. It’s not a replacement for traditional semantics but an extension that enables new types of investigation.\nIn the next session, we’ll see how PDS works in detail, starting with its relationship to RSA models."
  },
  {
    "objectID": "background/background.html#interim-summary",
    "href": "background/background.html#interim-summary",
    "title": "Probabilistic dynamic semantics",
    "section": "Interim summary",
    "text": "Interim summary\n\nTraditional semantics achieved remarkable success\nExperimental methods open new opportunities\nGradience poses theoretical challenges\nNeed frameworks bridging theory and data\n\n\nNext: How Rational Speech Act models attempt this bridge, and what PDS can add\n\n\nLet’s summarize where we are. Traditional semantics has achieved remarkable success in characterizing compositionality and inference patterns. These insights are too valuable to abandon.\nBut experimental methods open new opportunities. We can test theories at unprecedented scale, investigate entire lexical domains, and discover patterns invisible to traditional methods. The data is rich but challenging.\nThe pervasive gradience in this data poses theoretical challenges. We need to understand what produces gradience - discrete choices, continuous parameters, or both? And we need frameworks that can model these sources while maintaining theoretical clarity.\nThis motivates frameworks that bridge theory and data. We need approaches that preserve semantic insights while engaging with empirical complexity. They must be both principled and practical.\nNext, we’ll examine how Rational Speech Act models attempt this bridge. We’ll see their successes and limitations, which will set the stage for understanding how PDS provides a complementary approach that maintains compositionality while adding probabilistic structure.\nThank you! Questions before we move to RSA?"
  },
  {
    "objectID": "background/background.html#two-kinds-of-models",
    "href": "background/background.html#two-kinds-of-models",
    "title": "Probabilistic dynamic semantics",
    "section": "Two kinds of models",
    "text": "Two kinds of models\n\nListener models\nSpeaker models"
  },
  {
    "objectID": "background/background.html#listener-models",
    "href": "background/background.html#listener-models",
    "title": "Probabilistic dynamic semantics",
    "section": "Listener models",
    "text": "Listener models\n\\(L_{0}\\)\n\\[\n\\begin{aligned}\nP_{L_0}(w | u) &∝ \\begin{cases}\nP_{L_0}(w) & ⟦u⟧^w = \\mathtt{T} \\\\\n0 & ⟦u⟧^w = \\mathtt{F}\n\\end{cases}\n\\end{aligned}\n\\]\n\n\\(u\\) is an utterance\n\\(w\\) is a meaning (e.g., state of the world to be communicated)\n\\(L_{0}\\) is just filtering the prior distribution over \\(w\\)"
  },
  {
    "objectID": "background/background.html#listener-models-1",
    "href": "background/background.html#listener-models-1",
    "title": "Probabilistic dynamic semantics",
    "section": "Listener models",
    "text": "Listener models\n\\(L_{i} (i &gt; 0)\\)\n\\[\n\\begin{aligned}\nP_{L_i}(w | u) &= \\frac{P_{L_i}(u | w) * P_{L_i}(w)}{∑_{w^\\prime}P_{L_i}(u | w^\\prime) *\nP_{L_i}(w^\\prime)}\n\\end{aligned}\n\\]\n\nBayes’ Theorem\n\nPosterior probability (given some observation) is proportional to prior probability, multiplied by the likelihood of the observation.\nDerivable from probability axioms."
  },
  {
    "objectID": "background/background.html#listener-models-2",
    "href": "background/background.html#listener-models-2",
    "title": "Probabilistic dynamic semantics",
    "section": "Listener models",
    "text": "Listener models\n\\(L_{i} (i &gt; 0)\\)\n\\[\n\\begin{aligned}\nP_{L_i}(w | u) &= \\frac{P_{L_i}(u | w) * P_{L_i}(w)}{∑_{w^\\prime}P_{L_i}(u | w^\\prime) *\nP_{L_i}(w^\\prime)}\n\\end{aligned}\n\\]\n\n\\(P_{L_{i}}(w) = P(w) \\hspace{7cm}\\) (prior over meanings)\n\\(P_{L_{i}}(u ∣ w)  = P_{S_{i}}(u ∣ w) \\hspace{2cm}\\) (utterance probability \\(∣ w\\))"
  },
  {
    "objectID": "background/background.html#listener-models-3",
    "href": "background/background.html#listener-models-3",
    "title": "Probabilistic dynamic semantics",
    "section": "Listener models",
    "text": "Listener models\n\\(L_{i} (i &gt; 0)\\)\n\\[\n\\begin{aligned}\nP_{L_i}(w | u) &= \\frac{P_{S_{i}}(u | w) * P(w)}{∑_{w^\\prime}P_{S_{i}}(u | w^\\prime) *\nP(w^\\prime)}\n\\end{aligned}\n\\]\n\n\\(P_{L_{i}}(w) = P(w) \\hspace{7cm}\\) (prior over meanings)\n\\(P_{L_{i}}(u ∣ w)  = P_{S_{i}}(u ∣ w) \\hspace{2cm}\\) (utterance probability \\(∣ w\\))"
  },
  {
    "objectID": "background/background.html#listener-models-4",
    "href": "background/background.html#listener-models-4",
    "title": "Probabilistic dynamic semantics",
    "section": "Listener models",
    "text": "Listener models\n\\(L_{i} (i &gt; 0)\\)\n\\[\n\\begin{aligned}\nP_{L_i}(w | u) &= \\frac{P_{S_{i}}(u | w) * P(w)}{∑_{w^\\prime}P_{S_{i}}(u | w^\\prime) *\nP(w^\\prime)}\n\\end{aligned}\n\\]\nIntuition\n\nThe most probable meaning is the one the speaker would’ve most likely chosen that utterance for to get you to infer it.\nGrice (1975)"
  },
  {
    "objectID": "background/background.html#haskell",
    "href": "background/background.html#haskell",
    "title": "Probabilistic dynamic semantics",
    "section": "Haskell",
    "text": "Haskell\n\nLearn you a Haskell: https://learnyouahaskell.com/\n\n\nThings we’ll rely on:\n\nAlgebraic data types\ndata SomeData = FirstThing | SecondThing String | ThirdThing Double deriving (Eq, Show)\nMaybe and Either\ndata Maybe a = Just a | Nothing deriving ...\n\ndata Either a b = Left a | Right b deriving ...\nLots of other stuff: we’ll explain as we go."
  },
  {
    "objectID": "background/background.html#references",
    "href": "background/background.html#references",
    "title": "Probabilistic dynamic semantics",
    "section": "",
    "text": "References\n\n\n\n\nAbrusán, Márta. 2011. “Predicting the Presuppositions of Soft Triggers.” Linguistics and Philosophy 34 (6): 491–535.\n\n\nAn, Hannah, and Aaron White. 2020. “The Lexical and Grammatical Sources of Neg-Raising Inferences.” Proceedings of the Society for Computation in Linguistics 3 (1): 220–33. https://doi.org/https://doi.org/10.7275/yts0-q989.\n\n\nAnand, Pranav, and Valentine Hacquard. 2014. “Factivity, Belief and Discourse.” In The Art and Craft of Semantics: A Festschrift for Irene Heim, edited by Luka Crni\\v{c} and Uli Sauerland, 1:69–90. MITWPL 70. MITWPL. https://semanticsarchive.net/Archive/jZiNmM4N/.\n\n\nBarker, Chris. 2002. “The Dynamics of Vagueness.” Linguistics and Philosophy 25 (1): 1–36. https://doi.org/10.1023/A:1014346114955.\n\n\nBierwisch, Manfred. 1989. “The Semantics of Gradation.” In Dimensional Adjectives, edited by Manfred Bierwisch and Ewald Lang, 71–261. Berlin: Springer-Verlag.\n\n\nBumford, Dylan, and Jessica Rett. 2021. “Rationalizing Evaluativity.” Proceedings of Sinn Und Bedeutung 25 (September): 187–204. https://doi.org/10.18148/sub/2021.v25i0.931.\n\n\nChomsky, Noam. 1957. Syntactic Structures. The Hague/Paris: Mouton & Co.\n\n\n———. 1964. “Current Issues in Linguistic Theory.” In The Structure of Language, edited by J. Fodor and J. Katz, 50–118. New York: Prentice Hall.\n\n\n———. 1973. “Conditions on Transformations.” In A Festschrift for Morris Halle, edited by S. Anderson and P. Kiparsky, 232–86. New York: Holt, Rinehart, & Winston.\n\n\nDavis, Steven, and Brendan S Gillon. 2004. Semantics: A Reader. New York: Oxford University Press.\n\n\nDegen, Judith. 2023. “The Rational Speech Act Framework.” Annual Review of Linguistics 9 (Volume 9, 2023): 519–40. https://doi.org/10.1146/annurev-linguistics-031220-010811.\n\n\nDegen, Judith, and Judith Tonhauser. 2021. “Prior Beliefs Modulate Projection.” Open Mind 5 (September): 59–70. https://doi.org/10.1162/opmi_a_00042.\n\n\n———. 2022. “Are There Factive Predicates? An Empirical Investigation.” Language 98 (3): 552–91. https://doi.org/10.1353/lan.0.0271.\n\n\nDjärv, Kajsa, and Hezekiah Akiva Bacovcin. 2017. “Prosodic Effects on Factive Presupposition Projection.” Semantics and Linguistic Theory 27 (0): 116–33. https://doi.org/10.3765/salt.v27i0.4134.\n\n\nFarkas, Donka F., and Kim B. Bruce. 2010. “On Reacting to Assertions and Polar Questions.” Journal of Semantics 27 (1): 81–118. https://doi.org/10.1093/jos/ffp010.\n\n\nFarudi, Annahita. 2007. “An Antisymmetric Approach to Persian Clausal Complements.” Ms., University of Massachusetts, Amherst.\n\n\nFine, Kit. 1975. “Vagueness, Truth and Logic.” Synthese 30 (3/4): 265–300. https://www.jstor.org/stable/20115033.\n\n\nFrank, Michael C., and Noah D. Goodman. 2012. “Predicting Pragmatic Reasoning in Language Games.” Science 336 (6084): 998–98. https://doi.org/10.1126/science.1218633.\n\n\nGiannakidou, Anastasia. 1998. Polarity Sensitivity as (Non) Veridical Dependency. Vol. 23. John Benjamins Publishing.\n\n\n———. 1999. “Affective Dependencies.” Linguistics and Philosophy 22 (4): 367–421.\n\n\n———. 2009. “The Dependency of the Subjunctive Revisited: Temporal Semantics and Polarity.” Lingua 119 (12): 1883–1908.\n\n\nGinzburg, Jonathan. 1996. “Dynamics and the Semantics of Dialogue.” In Logic, Language, and Computation, edited by Jerry Seligman and Dag Westerståhl, 1:221–37. Stanford: CSLI Publications.\n\n\nGoodman, Noah D., and Michael C. Frank. 2016. “Pragmatic Language Interpretation as Probabilistic Inference.” Trends in Cognitive Sciences 20 (11): 818–29. https://doi.org/10.1016/j.tics.2016.08.005.\n\n\nGoodman, Noah D., and Andreas Stuhlmüller. 2013. “Knowledge and Implicature: Modeling Language Understanding as Social Cognition.” Topics in Cognitive Science 5 (1): 173–84. https://doi.org/10.1111/tops.12007.\n\n\nGraff, Delia. 2000. “Shifting Sands: An Interest-Relative Theory of Vagueness.” Philosophical Topics 28 (1): 45–81. https://www.jstor.org/stable/43154331.\n\n\nGrice, H. Paul. 1975. “Logic and Conversation.” In Syntax and Semantics, edited by Peter Cole and Jerry L. Morgan, 3, Speech Acts:41–58. New York: Academic Press.\n\n\nGroenendijk, Jeroen A., and Martin B. J. Stokhof. 1991. “Dynamic Predicate Logic.” Linguistics and Philosophy 14 (1): 39–100. https://doi.org/10.1007/BF00628304.\n\n\nJasbi, Masoud, Brandon Waldon, and Judith Degen. 2019. “Linking Hypothesis and Number of Response Options Modulate Inferred Scalar Implicature Rate.” Frontiers in Psychology 10 (February). https://doi.org/10.3389/fpsyg.2019.00189.\n\n\nJeong, Sunwoo. 2021. “Prosodically-Conditioned Factive Inferences in Korean: An Experimental Study.” Semantics and Linguistic Theory 30 (0): 1–21. https://doi.org/10.3765/salt.v30i0.4798.\n\n\nKamp, J. A. W. 1975. “Two Theories about Adjectives.” In Formal Semantics of Natural Language, edited by Edward L. Keenan, 123–55. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511897696.011.\n\n\nKane, Benjamin, Will Gantt, and Aaron Steven White. 2022. “Intensional Gaps: Relating Veridicality, Factivity, Doxasticity, Bouleticity, and Neg-Raising.” Semantics and Linguistic Theory 31 (0): 570–605. https://doi.org/10.3765/salt.v31i0.5137.\n\n\nKao, Justine T., Jean Y. Wu, Leon Bergen, and Noah D. Goodman. 2014. “Nonliteral Understanding of Number Words.” Proceedings of the National Academy of Sciences 111 (33): 12002–7. https://doi.org/10.1073/pnas.1407479111.\n\n\nKarttunen, Lauri. 1971. “Some Observations on Factivity.” Paper in Linguistics 4 (1): 55–69. https://doi.org/10.1080/08351817109370248.\n\n\nKastner, Itamar. 2015. “Factivity Mirrors Interpretation: The Selectional Requirements of Presuppositional Verbs.” Lingua 164: 156–88.\n\n\nKennedy, Chris. 1999. Projecting the Adjective: The Syntax and Semantics of Gradability and Comparison. New York: Garland.\n\n\nKennedy, Christopher. 2007. “Vagueness and Grammar: The Semantics of Relative and Absolute Gradable Adjectives.” Linguistics and Philosophy 30 (1): 1–45. https://doi.org/10.1007/s10988-006-9008-0.\n\n\nKennedy, Christopher, and Louise McNally. 2005. “Scale Structure, Degree Modification, and the Semantics of Gradable Predicates.” Language 81 (2): 345–81. https://doi.org/10.1353/lan.2005.0071.\n\n\nKiparsky, Paul, and Carol Kiparsky. 1970. “FACT.” In Progress in Linguistics, 143–73. De Gruyter Mouton. https://doi.org/10.1515/9783111350219.143.\n\n\nKlein, Ewan. 1980. “A Semantics for Positive and Comparative Adjectives.” Linguistics and Philosophy 4 (1): 1–45. https://doi.org/10.1007/BF00351812.\n\n\nKrifka, Manfred. 2007. “Approximate Interpretation of Number Words: A Case for Strategic Communication.” In Cognitive Foundations of Interpretation, edited by Gerlof Bouma, Irene Krämer, and Joost Zwartz, 111–26. Amsterdam: Koninklijke Nederlandse Akademie van Wetenschapen. https://doi.org/10.18452/9508.\n\n\nLakoff, George. 1973. “Hedges: A Study in Meaning Criteria and the Logic of Fuzzy Concepts.” Journal of Philosophical Logic 2 (4): 458–508. https://doi.org/10.1007/BF00262952.\n\n\nLasersohn, Peter. 1999. “Pragmatic Halos.” Language 75 (3): 522–51. https://doi.org/10.2307/417059.\n\n\nLassiter, Daniel, and Noah D. Goodman. 2013. “Context, Scale Structure, and Statistics in the Interpretation of Positive-Form Adjectives.” Semantics and Linguistic Theory 23 (0): 587–610. https://doi.org/10.3765/salt.v23i0.2658.\n\n\n———. 2017. “Adjectival Vagueness in a Bayesian Model of Interpretation.” Synthese 194 (10): 3801–36. https://doi.org/10.1007/s11229-015-0786-1.\n\n\nMoon, Ellise, and Aaron White. 2020. “The Source of Nonfinite Temporal Interpretation.” In Proceedings of the 50th Annual Meeting of the North East Linguistic Society, edited by Mariam Asatryan, Yixiao Song, and Ayana Whitmal, 3:11–24. Amherst: GLSA Publications.\n\n\nMuskens, Reinhard. 1996. “Combining Montague Semantics and Discourse Representation.” Linguistics and Philosophy 19 (2): 143–86. https://doi.org/10.1007/BF00635836.\n\n\nOzyildiz, Deniz. 2017. “Attitude Reports with and Without True Belief.” In Semantics and Linguistic Theory, edited by Dan Burgdorf, Jacob Collard, Sireemas Maspong, and Brynhildur Stefánsdóttir, 27:397–417. Linguistic Society of America.\n\n\nPhillips, Colin, Phoebe Gaston, Nick Huang, and Hanna Muller. 2021. “Theories All the Way Down: Remarks on ‘Theoretical’ and ‘Experimental’ Linguistics.” In The Cambridge Handbook of Experimental Syntax, edited by Grant Goodall, 587–616. Cambridge Handbooks in Language and Linguistics. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108569620.023.\n\n\nQing, Ciyang, and Michael Franke. 2014. “Gradable Adjectives, Vagueness, and Optimal Language Use: A Speaker-Oriented Model.” Semantics and Linguistic Theory, August, 23–41. https://doi.org/10.3765/salt.v24i0.2412.\n\n\nQing, Ciyang, Noah D. Goodman, and Daniel Lassiter. 2016. “A Rational Speech-Act Model of Projective Content.” In Proceedings of the 38th Annual Meeting of the Cognitive Science Society: Recognising and Representing Events, 1110–15. The Cognitive Science Society. https://www.research.ed.ac.uk/en/publications/a-rational-speech-act-model-of-projective-content.\n\n\nRoberts, Craige. 2012. “Information Structure: Towards an Integrated Formal Theory of Pragmatics.” Semantics and Pragmatics 5 (December): 6:1–69. https://doi.org/10.3765/sp.5.6.\n\n\nRoberts, Craige, and Mandy Simons. 2024. “Preconditions and Projection: Explaining Non-Anaphoric Presupposition.” Linguistics and Philosophy 47 (4): 703–48. https://doi.org/10.1007/s10988-024-09413-9.\n\n\nRooij, Robert van. 2011. “Vagueness and Linguistics.” In Vagueness: A Guide, edited by Giuseppina Ronzitti, 123–70. Dordrecht: Springer Netherlands. https://doi.org/10.1007/978-94-007-0375-9_6.\n\n\nRoss, John Robert. 1967. “Constraints on Variables in Syntax.” PhD thesis, Massachusetts Institute of Technology.\n\n\nRoussou, Anna. 2010. “Selecting Complementizers.” Lingua 120 (3): 582–603.\n\n\nSadock, Jerrold M. 1977. “Truth and Approximations.” Annual Meeting of the Berkeley Linguistics Society, September, 430–39. https://doi.org/10.3765/bls.v3i0.2268.\n\n\nSchütze, Carson T. 2016. The Empirical Base of Linguistics. Classics in Linguistics 2. Berlin: Language Science Press. https://doi.org/10.17169/langsci.b89.100.\n\n\nSimons, Mandy. 2007. “Observations on Embedding Verbs, Evidentiality, and Presupposition.” Lingua 117 (6): 1034–56. https://doi.org/10.1016/j.lingua.2006.05.006.\n\n\nSimons, Mandy, David Beaver, Craige Roberts, and Judith Tonhauser. 2017. “The Best Question: Explaining the Projection Behavior of Factives.” Discourse Processes 54 (3): 187–206.\n\n\nSimons, Mandy, Judith Tonhauser, David Beaver, and Craige Roberts. 2010. “What Projects and Why.” In Semantics and Linguistic Theory, edited by Nan Li and David Lutz, 20:309–27. University of British Columbia; Simon Fraser University: Linguistic Society of America. https://doi.org/10.3765/salt.v20i0.2584.\n\n\nSolt, Stephanie. 2015. “Vagueness and Imprecision: Empirical Foundations.” Annual Review of Linguistics 1 (Volume 1, 2015): 107–27. https://doi.org/10.1146/annurev-linguist-030514-125150.\n\n\nSorensen, Roy. 2023. “Vagueness.” In The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta and Uri Nodelman, Winter 2023. Metaphysics Research Lab, Stanford University. https://plato.stanford.edu/archives/win2023/entries/vagueness/.\n\n\nSprouse, Jon, and Sandra Villata. 2021. “Island Effects.” In The Cambridge Handbook of Experimental Syntax, edited by Grant Goodall, 227–57. Cambridge Handbooks in Language and Linguistics. Cambridge University Press. https://doi.org/10.1017/9781108569620.010.\n\n\nStabler, Edward. 1997. “Derivational Minimalism.” In Logical Aspects of Computational Linguistics, edited by Christian Retoré, 68–95. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer. https://doi.org/10.1007/BFb0052152.\n\n\nStalnaker, Robert. 1978. “Assertion.” In Pragmatics, edited by Peter Cole, 9:315–32. New York: Academic Press.\n\n\nTonhauser, Judith. 2016. “Prosodic Cues to Presupposition Projection.” Semantics and Linguistic Theory 26 (0): 934–60. https://doi.org/10.3765/salt.v26i0.3788.\n\n\nTonhauser, Judith, David I. Beaver, and Judith Degen. 2018. “How Projective Is Projective Content? Gradience in Projectivity and At-Issueness.” Journal of Semantics 35 (3): 495–542. https://doi.org/10.1093/jos/ffy007.\n\n\nVarlokosta, Spyridoula. 1994. “Issues in Modern Greek Sentential Complementation.” PhD thesis, University of Maryland, College Park.\n\n\nWaldon, Brandon, and Judith Degen. 2020. “Modeling Behavior in Truth Value Judgment Task Experiments.” In Proceedings of the Society for Computation in Linguistics 2020, edited by Allyson Ettinger, Gaja Jarosz, and Joe Pater, 238–47. New York, New York: Association for Computational Linguistics. https://aclanthology.org/2020.scil-1.29/.\n\n\nWhite, Aaron Steven. 2019. “Lexically Triggered Veridicality Inferences.” In Handbook of Pragmatics, 22:115–48. John Benjamins Publishing Company. https://doi.org/10.1075/hop.22.lex4.\n\n\nWhite, Aaron Steven, and Kyle Rawlins. 2016. “A Computational Model of S-Selection.” Semantics and Linguistic Theory 26 (0): 641–63. https://doi.org/10.3765/salt.v26i0.3819.\n\n\n———. 2018. “The Role of Veridicality and Factivity in Clause Selection.” In NELS 48: Proceedings of the Forty-Eighth Annual Meeting of the North East Linguistic Society, edited by Sherry Hucklebridge and Max Nelson, 48:221–34. University of Iceland: GLSA (Graduate Linguistics Student Association), Department of Linguistics, University of Massachusetts.\n\n\n———. 2020. “Frequency, Acceptability, and Selection: A Case Study of Clause-Embedding.” Glossa: A Journal of General Linguistics 5 (1). https://doi.org/10.5334/gjgl.1001.\n\n\nWhite, Aaron Steven, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme. 2018. “Lexicosyntactic Inference in Neural Models.” In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 4717–24. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/D18-1501."
  }
]