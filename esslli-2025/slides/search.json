[
  {
    "objectID": "adjectives/gradable-adjectives.html#vague-adjectives",
    "href": "adjectives/gradable-adjectives.html#vague-adjectives",
    "title": "Gradable adjectives",
    "section": "Vague adjectives",
    "text": "Vague adjectives\n\ntall, wide, expensive, happy, ‚Ä¶\n\n\nThe coffee in Rome is expensive. (Kennedy 2007)\n\n\nTrue if: the cost of coffee in Rome is as great as a salient threshold, \\(d\\): \\[c ‚â• d\\]\nMaybe the cost of coffee is 3 euros.\n\\(d\\) is vague‚Ä¶ maybe it ranges somewhere from 2 euros to 4 euros."
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#vagueness-strange-inference-patterns",
    "href": "adjectives/gradable-adjectives.html#vagueness-strange-inference-patterns",
    "title": "Gradable adjectives",
    "section": "Vagueness: strange inference patterns",
    "text": "Vagueness: strange inference patterns\nVague adjectives such as expensive\n\nadmit borderline cases:\n\nMud Blend: $1.50/lb ¬†¬†üôÅ\nOrganic Kona: $20/lb ¬†¬†ü•∞\nSwell Start Blend: $9.25/lb ??"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#vagueness-strange-inference-patterns-1",
    "href": "adjectives/gradable-adjectives.html#vagueness-strange-inference-patterns-1",
    "title": "Gradable adjectives",
    "section": "Vagueness: strange inference patterns",
    "text": "Vagueness: strange inference patterns\n\nproduce sorites paradoxes:"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#sorites-10",
    "href": "adjectives/gradable-adjectives.html#sorites-10",
    "title": "Gradable adjectives",
    "section": "Sorites: $10",
    "text": "Sorites: $10\nIs a $10 cup of coffee expensive?"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#sorites-9.99",
    "href": "adjectives/gradable-adjectives.html#sorites-9.99",
    "title": "Gradable adjectives",
    "section": "Sorites: $9.99",
    "text": "Sorites: $9.99\nTake $0.01 away. Is a $9.99 cup of coffee expensive?"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#sorites-9.98",
    "href": "adjectives/gradable-adjectives.html#sorites-9.98",
    "title": "Gradable adjectives",
    "section": "Sorites: $9.98",
    "text": "Sorites: $9.98\nTake $0.01 away. Is a $9.98 cup of coffee expensive?"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#vagueness-strange-inference-patterns-2",
    "href": "adjectives/gradable-adjectives.html#vagueness-strange-inference-patterns-2",
    "title": "Gradable adjectives",
    "section": "Vagueness: strange inference patterns",
    "text": "Vagueness: strange inference patterns\n\nproduce sorites paradoxes:\n\nPremise 1: A $10 cup of coffee is expensive.\nPremise 2: If an expensive cup of coffee were 1 cent cheaper, it would still be expensive.\nConclusion: Therefore, a free cup of coffee is expensive!"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#why-tricky",
    "href": "adjectives/gradable-adjectives.html#why-tricky",
    "title": "Gradable adjectives",
    "section": "Why tricky?",
    "text": "Why tricky?\nBorderline cases and sorites paradoxes: inference patterns that do not fall into any traditional semantic classification.\n\nBorderline cases: things to which the adjective neither applies nor doesn‚Äôt apply.\nThe inference is not simply ‚Äúon‚Äù or ‚Äúoff‚Äù.\nThe sorites paradox: troublesome because‚Ä¶\n\ninferences should be closed under implication: if $10 ‚Üù $9.99, and $9.99 ‚Üù $9.98, then $10.00 ‚Üù $9.98.\n\na $10 cup is expensive ‚Üù a free cup is expensive"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#inference-judgments",
    "href": "adjectives/gradable-adjectives.html#inference-judgments",
    "title": "Gradable adjectives",
    "section": "Inference judgments",
    "text": "Inference judgments\nOngoing work with Helena Aparicio (Cornell Linguistics):\n\nCollect scale norming data.\n\n‚ÄúX drank a coffee. Guess how expensive it was?‚Äù\n\nCollect adjectival inference data.\n\n‚ÄúX drank a coffee. How likely is it that it was expensive?‚Äù (vague)"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#modeling-pipeline",
    "href": "adjectives/gradable-adjectives.html#modeling-pipeline",
    "title": "Gradable adjectives",
    "section": "Modeling pipeline",
    "text": "Modeling pipeline\nUsing the norming data to help model the adjectival inference data:\n\nAssume that people make likelihood judgments based on their probabilistic estimate of where an object lands on the adjective‚Äôs scale.\nThen, we can model the distribution over these estimates using a model fit to the norming data."
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#norming-task",
    "href": "adjectives/gradable-adjectives.html#norming-task",
    "title": "Gradable adjectives",
    "section": "Norming task",
    "text": "Norming task\n\n\n\n\n\n6 relative adjectives\n3 possible contexts for each adjective (‚Äúhigh‚Äù, ‚Äúmedium‚Äù, ‚Äúlow‚Äù)"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#norming-data",
    "href": "adjectives/gradable-adjectives.html#norming-data",
    "title": "Gradable adjectives",
    "section": "Norming data",
    "text": "Norming data"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#adjectival-inference-task",
    "href": "adjectives/gradable-adjectives.html#adjectival-inference-task",
    "title": "Gradable adjectives",
    "section": "Adjectival inference task",
    "text": "Adjectival inference task\n\n\n\n\n\nSame 6 adjectives (six relative, six absolute)\nSame 3 possible contexts for each adjective (‚Äúhigh‚Äù, ‚Äúmedium‚Äù, ‚Äúlow‚Äù)"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#adjectival-inference-data",
    "href": "adjectives/gradable-adjectives.html#adjectival-inference-data",
    "title": "Gradable adjectives",
    "section": "Adjectival inference data",
    "text": "Adjectival inference data\n\n \n\n\n  \\(\\scriptsize \\text{Likelihood that adj. is true}\\)"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#the-compilation-challenge",
    "href": "adjectives/gradable-adjectives.html#the-compilation-challenge",
    "title": "Gradable adjectives",
    "section": "The compilation challenge",
    "text": "The compilation challenge\n\nSemantic theory: Œª-terms, compositional rules\nStatistical models: Parameters, likelihoods, inference\n\nThe gap: Need explicit linking hypotheses\n\n\nPDS automates the core translation while leaving room for statistical expertise"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#what-pds-produces",
    "href": "adjectives/gradable-adjectives.html#what-pds-produces",
    "title": "Gradable adjectives",
    "section": "What PDS produces",
    "text": "What PDS produces\n\nPDS outputs what we term a kernel model‚Äîthe semantic core that corresponds directly to the lexical and compositional semantics\nThis kernel can in principle be augmented with other components:\n\nRandom effects\nHierarchical priors\n\nOther statistical machinery\n\nThe current implementation focuses on producing just the semantic kernel"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#why-stan",
    "href": "adjectives/gradable-adjectives.html#why-stan",
    "title": "Gradable adjectives",
    "section": "Why Stan?",
    "text": "Why Stan?\n\nStan is a probabilistic programming language designed for statistical inference\nUnlike general-purpose programming languages, Stan is specialized for stating distributional assumptions\nThese assumptions are translated to a C++ backend that performs statistical inference\nUsually uses a form of Markov Chain Monte Carlo (MCMC)"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#why-stan-1",
    "href": "adjectives/gradable-adjectives.html#why-stan-1",
    "title": "Gradable adjectives",
    "section": "Why Stan?",
    "text": "Why Stan?\n\nDeclarative(-ish): We specify probability models, not procedures\nSpecialized: Designed for Bayesian inference\nParallel to semantics: Both declare ‚Äúwhat‚Äù not ‚Äúhow‚Äù\n\nSemantics: truth conditions\nStan: probabilistic relationships"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#stans-block-structure",
    "href": "adjectives/gradable-adjectives.html#stans-block-structure",
    "title": "Gradable adjectives",
    "section": "Stan‚Äôs block structure",
    "text": "Stan‚Äôs block structure\ndata {\n  int&lt;lower=1&gt; N;           // observed\n  vector[N] y;              \n}\nparameters {\n  real mu;                  // inferred\n  real&lt;lower=0&gt; sigma;      \n}\nmodel {\n  mu ~ normal(0, 10);       // prior\n  y ~ normal(mu, sigma);    // likelihood\n}\n\nKey idea: This approach aligns well with our goals:\n\nFormal semantic analyses declare some representation of the semantic content\nStan declares probabilistic relationships rather than sampling algorithms"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#our-first-example",
    "href": "adjectives/gradable-adjectives.html#our-first-example",
    "title": "Gradable adjectives",
    "section": "Our first example",
    "text": "Our first example\n\nQuestion: ‚ÄúHow tall is Jo?‚Äù\nTask: Participants report degrees on scales\nGoal: Infer the ‚Äútrue‚Äù degree for each item\n\n\nThis seemingly simple task will reveal the complexity of the PDS-to-Stan pipeline"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#the-experimental-data",
    "href": "adjectives/gradable-adjectives.html#the-experimental-data",
    "title": "Gradable adjectives",
    "section": "The experimental data",
    "text": "The experimental data\n\n\n\n\nparticipant\nitem\nadjective\ncondition\nresponse\n\n\n\n\n1\ntall_high\ntall\nhigh\n0.82\n\n\n1\nwide_low\nwide\nlow\n0.34\n\n\n1\nexpensive_mid\nexpensive\nmid\n0.62\n\n\n1\nfull_high\nfull\nhigh\n1.00\n\n\n\n\n\n\nItems = adjective √ó condition pairs\nResponses on 0-1 scale (slider)\nChallenge: Handle boundary responses (0s and 1s)"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#building-the-data-block---counts",
    "href": "adjectives/gradable-adjectives.html#building-the-data-block---counts",
    "title": "Gradable adjectives",
    "section": "Building the data block - counts",
    "text": "Building the data block - counts\ndata {\n  int&lt;lower=1&gt; N_item;        // number of items\n  int&lt;lower=1&gt; N_participant; // number of participants  \n  int&lt;lower=1&gt; N_data;        // number of data points\n\nWhy constraints?\n\n&lt;lower=1&gt; ensures valid data\nHelps Stan optimize algorithms\nCatches errors early"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#handling-boundary-responses",
    "href": "adjectives/gradable-adjectives.html#handling-boundary-responses",
    "title": "Gradable adjectives",
    "section": "Handling boundary responses",
    "text": "Handling boundary responses\n  // Censoring at boundaries\n  int&lt;lower=1&gt; N_0;           // number of 0s\n  int&lt;lower=1&gt; N_1;           // number of 1s\n\nWhy separate these?\n\nSlider endpoints are special\n0 might mean ‚Äúeven less than 0‚Äù\n1 might mean ‚Äúeven more than 1‚Äù\nCensored data needs special treatment"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#response-data-and-indexing",
    "href": "adjectives/gradable-adjectives.html#response-data-and-indexing",
    "title": "Gradable adjectives",
    "section": "Response data and indexing",
    "text": "Response data and indexing\n  vector&lt;lower=0, upper=1&gt;[N_data] y; // responses in (0,1)\n  \n  // Which item/participant for each response\n  array[N_data] int&lt;lower=1, upper=N_item&gt; item;\n  array[N_data] int&lt;lower=1, upper=N_participant&gt; participant;\n}\n\nThese arrays are lookup tables: item[5] = 3 means the 5th response is about item #3"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#parameters-what-we-want-to-learn",
    "href": "adjectives/gradable-adjectives.html#parameters-what-we-want-to-learn",
    "title": "Gradable adjectives",
    "section": "Parameters: What we want to learn",
    "text": "Parameters: What we want to learn\nparameters {\n  // Semantic parameters - what PDS cares about\n  vector[N_item] mu_guess;    // degree for each item\n\n\nThese means represent our best guess, as researchers, about each item‚Äôs true degree on its scale\nThey capture the theoretical degrees that the semantic analysis posits\nCrucially, they can also be viewed as representing subjects‚Äô uncertainty about these degrees\nThey reflect what unresolved uncertainty subjects maintain when they make these guesses"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#random-effects-structure",
    "href": "adjectives/gradable-adjectives.html#random-effects-structure",
    "title": "Gradable adjectives",
    "section": "Random effects structure",
    "text": "Random effects structure\n  // How much people vary\n  real&lt;lower=0&gt; sigma_epsilon_guess;\n  \n  // Each person's deviation (z-scores)\n  vector[N_participant] z_epsilon_guess;\n\nNon-centered parameterization\n\nSeparate scale (sigma) from standardized deviations (z)\nHelps Stan‚Äôs algorithms converge\nMathematically equivalent but computationally superior"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#measurement-and-censoring",
    "href": "adjectives/gradable-adjectives.html#measurement-and-censoring",
    "title": "Gradable adjectives",
    "section": "Measurement and censoring",
    "text": "Measurement and censoring\n  real&lt;lower=0,upper=1&gt; sigma_e;  // response noise\n  \n  // True values for censored data\n  array[N_0] real&lt;upper=0&gt; y_0;    // true values for 0s\n  array[N_1] real&lt;lower=1&gt; y_1;    // true values for 1s\n}\n\nWe infer what the ‚Äútrue‚Äù values might have been beyond scale boundaries"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#transformed-parameters",
    "href": "adjectives/gradable-adjectives.html#transformed-parameters",
    "title": "Gradable adjectives",
    "section": "Transformed parameters",
    "text": "Transformed parameters\ntransformed parameters {\n  // Convert z-scores to actual effects\n  vector[N_participant] epsilon_guess = \n    sigma_epsilon_guess * z_epsilon_guess;\n\nIf sigma = 0.2 and z[3] = 1.5, then participant 3 gives responses 0.3 units higher than average"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#computing-predictions",
    "href": "adjectives/gradable-adjectives.html#computing-predictions",
    "title": "Gradable adjectives",
    "section": "Computing predictions",
    "text": "Computing predictions\n  vector[N_data] guess;\n  for (i in 1:N_data) {\n    guess[i] = mu_guess[item[i]] + \n               epsilon_guess[participant[i]];\n  }\n}\n\nExample trace\n\nResponse i: item 5, participant 3\nmu_guess[5] = 0.7 (item‚Äôs degree)\nepsilon_guess[3] = 0.1 (participant‚Äôs adjustment)\nguess[i] = 0.7 + 0.1 = 0.8"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#the-model-block",
    "href": "adjectives/gradable-adjectives.html#the-model-block",
    "title": "Gradable adjectives",
    "section": "The model block",
    "text": "The model block\nmodel {\n  // Priors\n  sigma_epsilon_guess ~ exponential(1);\n  z_epsilon_guess ~ std_normal();\n  \n  // Likelihood\n  for (i in 1:N_data) {\n    y[i] ~ normal(guess[i], sigma_e);\n  }\n}"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#the-pds-implementation",
    "href": "adjectives/gradable-adjectives.html#the-pds-implementation",
    "title": "Gradable adjectives",
    "section": "The PDS implementation",
    "text": "The PDS implementation\n-- From sources/pds/src/\ns1'        = termOf $ getSemantics @Adjectives 1 \n             [\"jo\", \"is\", \"a\", \"soccer player\"]\nq1'        = termOf $ getSemantics @Adjectives 0 \n             [\"how\", \"tall\", \"jo\", \"is\"]\ndiscourse' = ty tau $ assert s1' &gt;&gt;&gt; ask q1'\nscaleNormingExample = asTyped tau \n  (betaŒîNormal Œ¥Rules . \n   adjectivesRespond scaleNormingPrior) discourse'\n\nThis establishes context and asks for Jo‚Äôs height\n\n\nNote the use of certain convenience functions:\n\ngetSemantics retrieves one of the meanings (in the Œª-calculus) for the expression\nIt takes a string of strings as input\nUses the parser implemented at Grammar.Parser"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#lexical-entries-for-degree-questions",
    "href": "adjectives/gradable-adjectives.html#lexical-entries-for-degree-questions",
    "title": "Gradable adjectives",
    "section": "Lexical entries for degree questions",
    "text": "Lexical entries for degree questions\n-- From Grammar.Lexica.SynSem.Adjectives\n\"tall\" -&gt; [ SynSem {\n  syn = AP :\\: Deg,\n  sem = ty tau (purePP (lam d (lam x (lam i \n    (sCon \"(‚â•)\" @@ (sCon \"height\" @@ i @@ x) @@ d)))))\n  }, ... ]\n\n\"how\" -&gt; [ SynSem {\n  syn = Qdeg :/: (S :/: AP) :/: (AP :\\: Deg),\n  sem = ty tau (purePP (lam x (lam y (lam z \n    (y @@ (x @@ z))))))\n  }, ... ]\n\nThe degree-argument version of adjectives enables degree questions"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#compositional-semantics",
    "href": "adjectives/gradable-adjectives.html#compositional-semantics",
    "title": "Gradable adjectives",
    "section": "Compositional semantics",
    "text": "Compositional semantics\nFor how tall is Jo?, composition yields:\n\\[Œªd, i.\\ct{height}(i)(\\ct{j}) ‚â• d\\]\n\nWhen \\(\\abbr{respond}\\) comes into the picture, some index \\(i^{\\prime}\\) is sampled from the common ground, and the maximal answer to the question is determined to be:\n\\[\\ct{max}(Œªd.\\ct{height}(i^{\\prime})(\\ct{j}) ‚â• d)\\]"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#Œ¥-rules-what-are-they",
    "href": "adjectives/gradable-adjectives.html#Œ¥-rules-what-are-they",
    "title": "Gradable adjectives",
    "section": "Œ¥-rules: What are they?",
    "text": "Œ¥-rules: What are they?\ntype DeltaRule = Term -&gt; Maybe Term\n\nŒ¥-rules enable different semantic computations while preserving semantic equivalence\nOrder of rule application doesn‚Äôt affect the final result\nThey should be sound\n\nMeaning: the equalities they encode should be (mathematically) true equalities"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#the-indices-and-maxes-Œ¥-rules",
    "href": "adjectives/gradable-adjectives.html#the-indices-and-maxes-Œ¥-rules",
    "title": "Gradable adjectives",
    "section": "The indices and maxes Œ¥-rules",
    "text": "The indices and maxes Œ¥-rules\n-- From Lambda.Œî module\nindices :: DeltaRule\nindices = \\case\n  Height (UpdHeight p _) -&gt; Just p\n  Height (UpdSocPla _ i) -&gt; Just (Height i)\n  SocPla (UpdSocPla p _) -&gt; Just p\n  SocPla (UpdHeight _ i) -&gt; Just (SocPla i)\n  -- ... other cases for Ling, Epi\n  _                      -&gt; Nothing\n\nExtracts values from the discourse state\n\n\nmaxes :: DeltaRule\nmaxes = \\case\n   Max (Lam y (GE x (Var y'))) | y' == y -&gt; Just x\n   _                                     -&gt; Nothing  \n\n\nExtracts the unique degree satisfying the inequality"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#Œ¥-reduction-walkthrough",
    "href": "adjectives/gradable-adjectives.html#Œ¥-reduction-walkthrough",
    "title": "Gradable adjectives",
    "section": "Œ¥-reduction walkthrough",
    "text": "Œ¥-reduction walkthrough\nFor degree questions like how tall is Jo?, the compositional semantics produces: \\[\\ct{max}(Œªd.\\ct{height}(i^{\\prime})(\\ct{j}) ‚â• d)\\]\n\nApply indices rule to extract height: \\[\\text{becomes}\\,\\,\\, \\ct{max}(Œªd.h ‚â• d)\\] where \\(h\\) represents Jo‚Äôs actual height at index \\(i\\)\n\n\nApply max extraction using the following Œ¥-rule: becomes \\(h\\)"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#key-transformations",
    "href": "adjectives/gradable-adjectives.html#key-transformations",
    "title": "Gradable adjectives",
    "section": "Key transformations",
    "text": "Key transformations\n\nChallenge: translating abstract semantic computations into Stan‚Äôs parameter space\nTranslation embodies (some of) our linking hypothesis between semantic competence and performance\n\n\nDegree extraction feeds parameter inference\n\n\\(\\ct{max}(Œªd.\\ct{height}(i)(\\ct{j}) ‚â• d)\\) ‚Üí Infer parameter height_jo\nThe unique degree satisfying the equation becomes a parameter to estimate"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#key-transformations-1",
    "href": "adjectives/gradable-adjectives.html#key-transformations-1",
    "title": "Gradable adjectives",
    "section": "Key transformations",
    "text": "Key transformations\n\n\nFunctions become arrays\n\n\\(\\ct{height} : \\iota \\to e \\to r\\): Array height[person]\nFunction application: Array indexing\n\nPropositions become probabilities\n\nTruth values: Real numbers in [0,1]\nLogical operations: Probabilistic operations\n\nThe probabilistic bind statements become Stan‚Äôs target\n\nThe \\(‚àº\\) structures sequential computation\nThis determines Stan‚Äôs log probability"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#pds-kernel-output",
    "href": "adjectives/gradable-adjectives.html#pds-kernel-output",
    "title": "Gradable adjectives",
    "section": "PDS kernel output",
    "text": "PDS kernel output\nThe PDS system outputs the following kernel model, given the semantic fragment:\nmodel {\n  // FIXED EFFECTS\n  w ~ normal(0.0, 1.0);\n  \n  // LIKELIHOOD\n  target += normal_lpdf(y | w, sigma);\n}\n\n\n\n\nCaptures the essential degree-based semantics where w represents the degree on the height scale."
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#from-kernel-to-full-model",
    "href": "adjectives/gradable-adjectives.html#from-kernel-to-full-model",
    "title": "Gradable adjectives",
    "section": "From kernel to full model",
    "text": "From kernel to full model\nThe full model with analyst augmentations looks like:\nmodel {\n  // PRIORS (analyst-added)\n  sigma_epsilon_guess ~ exponential(1);\n  sigma_e ~ beta(2, 10);\n  \n  // FIXED EFFECTS (PDS kernel)\n  mu_guess ~ normal(0.0, 1.0);\n  \n  // RANDOM EFFECTS (analyst-added)\n  z_epsilon_guess ~ std_normal();\n  \n  // LIKELIHOOD (PDS kernel with modifications)\n  y[i] ~ normal(mu_guess[item[i]] + epsilon_guess[participant[i]], sigma_e);\n}\nHighlighted lines show the kernel model from PDS"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#from-kernel-to-full-model-1",
    "href": "adjectives/gradable-adjectives.html#from-kernel-to-full-model-1",
    "title": "Gradable adjectives",
    "section": "From kernel to full model",
    "text": "From kernel to full model\n\n\nThe unhighlighted portions add statistical machinery for real data:\n\nHierarchical priors\nRandom effects\n\nIndexed parameters"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#from-degrees-to-likelihood-judgments",
    "href": "adjectives/gradable-adjectives.html#from-degrees-to-likelihood-judgments",
    "title": "Gradable adjectives",
    "section": "From degrees to likelihood judgments",
    "text": "From degrees to likelihood judgments\nOur next model addresses how speakers reason about the likelihood that gradable adjectives apply: how likely (is it) that Jo is tall?\n\nMetalinguistic judgment about adjective application\nRequires probabilistic reasoning\nKey: Threshold uncertainty creates vagueness"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#the-pds-implementation-1",
    "href": "adjectives/gradable-adjectives.html#the-pds-implementation-1",
    "title": "Gradable adjectives",
    "section": "The PDS implementation",
    "text": "The PDS implementation\n-- From sources/pds/src/\nexpr1 = [\"jo\", \"is\", \"a\", \"soccer\", \"player\"]\nexpr2 = [\"how\", \"likely\", \"that\", \"jo\", \"is\", \"tall\"]\ns1 = getSemantics @Adjectives 0 expr1\nq1 = getSemantics @Adjectives 0 expr2\ndiscourse = ty tau $ assert s1 &gt;&gt;&gt; ask q1\nlikelihoodExample = asTyped tau \n  (betaŒîNormal Œ¥Rules . \n   adjectivesRespond likelihoodPrior) discourse"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#lexical-entries-part-1",
    "href": "adjectives/gradable-adjectives.html#lexical-entries-part-1",
    "title": "Gradable adjectives",
    "section": "Lexical entries (Part 1)",
    "text": "Lexical entries (Part 1)\n-- From Grammar.Lexica.SynSem.Adjectives\n\"tall\" -&gt; [ SynSem {\n    syn = AP,\n    sem = ty tau (lam s (purePP (lam x (lam i \n      (sCon \"(‚â•)\" @@ (sCon \"height\" @@ i @@ x) @@ \n       (sCon \"d_tall\" @@ s)))) @@ s))\n    } ]\n\nKey semantic components:\n\nheight: \\(\\iota \\to e \\to r\\) (entity heights)\nd_tall: \\(\\sigma \\to r\\) (contextual threshold)\n(‚â•): \\(r \\to r \\to t\\) (comparison)"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#lexical-entries-part-2",
    "href": "adjectives/gradable-adjectives.html#lexical-entries-part-2",
    "title": "Gradable adjectives",
    "section": "Lexical entries (Part 2)",
    "text": "Lexical entries (Part 2)\n\"likely\" -&gt; [ SynSem {\n    syn = S :\\: Deg :/: S,\n    sem = ty tau (lam s (purePP (lam p (lam d (lam _' \n      (sCon \"(‚â•)\" @@ \n       (Pr (let' i (CG s) (Return (p @@ i)))) @@ \n       d)))) @@ s))\n    } ]\n\nCompares probability of embedded proposition to a degree"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#Œ¥-reduction-for-vagueness",
    "href": "adjectives/gradable-adjectives.html#Œ¥-reduction-for-vagueness",
    "title": "Gradable adjectives",
    "section": "Œ¥-reduction for vagueness",
    "text": "Œ¥-reduction for vagueness\nStarting with Jo is tall: \\[\\ct{(‚â•)}(\\ct{height}(i)(\\ct{j}))(\\ct{d\\_tall}(s))\\]\n\nApply states rule to extract threshold: \\[\\text{becomes}\\,\\,\\,\\ct{(‚â•)}(\\ct{height}(i)(\\ct{j}))(d)\\]\n\n\nApply indices rule to extract height: \\[\\text{becomes}\\,\\,\\,\\ct{(‚â•)}(h)(d)\\]"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#the-states-Œ¥-rule",
    "href": "adjectives/gradable-adjectives.html#the-states-Œ¥-rule",
    "title": "Gradable adjectives",
    "section": "The states Œ¥-rule",
    "text": "The states Œ¥-rule\n-- From Lambda.Œî (lines 167-183)\nstates :: DeltaRule\nstates = \\case\n  CG      (UpdCG cg _)     -&gt; Just cg\n  CG      (UpdDTall _ s)   -&gt; Just (CG s)\n  DTall   (UpdDTall d _)   -&gt; Just d\n  DTall   (UpdCG _ s)      -&gt; Just (DTall s)\n  QUD     (UpdQUD q _)     -&gt; Just q\n  QUD     (UpdCG _ s)      -&gt; Just (QUD s)\n  TauKnow (UpdTauKnow b _) -&gt; Just b\n  TauKnow (UpdCG _ s)      -&gt; Just (TauKnow s)\n  _                        -&gt; Nothing"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#the-probabilities-Œ¥-rule",
    "href": "adjectives/gradable-adjectives.html#the-probabilities-Œ¥-rule",
    "title": "Gradable adjectives",
    "section": "The probabilities Œ¥-rule",
    "text": "The probabilities Œ¥-rule\n-- From Lambda.Œî (lines 156-164)\nprobabilities :: DeltaRule\nprobabilities = \\case\n  Pr (Return Tr)         -&gt; Just 1\n  Pr (Return Fa)         -&gt; Just 0\n  Pr (Bern x)            -&gt; Just x\n  Pr (Disj x t u)        -&gt; Just (x * Pr t + (1 - x) * Pr u)\n  Pr (Let v (Normal x y) (Return (GE t (Var v')))) \n    | v' == v -&gt; Just (NormalCDF x y t)\n  Pr (Let v (Normal x y) (Return (GE (Var v') t))) \n    | v' == v -&gt; Just (NormalCDF (- x) y t)\n  _                      -&gt; Nothing"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#probabilistic-computation",
    "href": "adjectives/gradable-adjectives.html#probabilistic-computation",
    "title": "Gradable adjectives",
    "section": "Probabilistic computation",
    "text": "Probabilistic computation\nThe system computes \\(P(h \\geq d)\\) when both are uncertain:\n\n\\(h \\sim \\ct{Normal}(\\mu_h, \\sigma_h)\\) (height)\n\\(d \\sim \\ct{Normal}(\\mu_d, \\sigma_d)\\) (threshold)\nResult: normal_cdf computation in Stan\n\n\nresponse = 1 - normal_cdf(d[item[i]] | \n                         mu_guess[i], \n                         sigma_guess);"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#pds-kernel-for-vagueness",
    "href": "adjectives/gradable-adjectives.html#pds-kernel-for-vagueness",
    "title": "Gradable adjectives",
    "section": "PDS kernel for vagueness",
    "text": "PDS kernel for vagueness\nmodel {\n  v ~ normal(0.0, 1.0);\n  target += normal_lpdf(y | \n              normal_cdf(v, -0.0, 1.0), \n              sigma);\n}\n\nThe kernel captures likelihood via normal_cdf"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#full-vagueness-model",
    "href": "adjectives/gradable-adjectives.html#full-vagueness-model",
    "title": "Gradable adjectives",
    "section": "Full vagueness model",
    "text": "Full vagueness model\ntransformed parameters {\n  for (i in 1:N_data) {\n    // Add participant adjustment\n    real threshold_logit = mu_guess0[item[i]] + \n                          epsilon_mu_guess[participant[i]];\n    mu_guess[i] = inv_logit(threshold_logit);\n    \n    // KEY SEMANTIC COMPUTATION: P(adjective applies)\n    response_rel[i] = 1 - normal_cdf(d[item[i]] | \n                                     mu_guess[i], \n                                     sigma_guess);\n  }\n}"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#key-ideas",
    "href": "adjectives/gradable-adjectives.html#key-ideas",
    "title": "Gradable adjectives",
    "section": "Key ideas",
    "text": "Key ideas\n\nŒ¥-rules bridge theory and computation\nKernel models isolate semantic content from statistical machinery\nThis distinction is crucial:\n\nPDS automates the translation from compositional semantics to the core statistical model‚Ä¶\nwhile leaving room for analysts to add domain-specific statistical structure\n\nCompilation embodies our linking hypothesis between semantic competence and performance"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#what-weve-covered",
    "href": "adjectives/gradable-adjectives.html#what-weve-covered",
    "title": "Gradable adjectives",
    "section": "What we‚Äôve covered",
    "text": "What we‚Äôve covered\n\nDegree inference from norming questions using degree-argument adjectives\nVagueness as threshold uncertainty in gradable adjectives\nLikelihood judgments via probabilistic computation with normal_cdf\nFull Œ¥-rule walkthroughs showing how complex Œª-terms become Stan parameters"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#conclusion",
    "href": "adjectives/gradable-adjectives.html#conclusion",
    "title": "Gradable adjectives",
    "section": "Conclusion",
    "text": "Conclusion\n\nPDS bridges formal semantics and statistical modeling\nŒ¥-rules enable computational tractability\nKernel models preserve theoretical clarity\nThis approach scales to complex phenomena\n\n\nNext: How this handles factivity‚Äîwhere gradience poses even deeper theoretical puzzles"
  },
  {
    "objectID": "adjectives/gradable-adjectives.html#references",
    "href": "adjectives/gradable-adjectives.html#references",
    "title": "Gradable adjectives",
    "section": "References",
    "text": "References\n\n\n\n\nKennedy, Christopher. 2007. ‚ÄúVagueness and Grammar: The Semantics of Relative and Absolute Gradable Adjectives.‚Äù Linguistics and Philosophy 30 (1): 1‚Äì45. https://doi.org/10.1007/s10988-006-9008-0."
  },
  {
    "objectID": "pds-intro/pds-intro.html#motivation",
    "href": "pds-intro/pds-intro.html#motivation",
    "title": "PDS Introduction",
    "section": "Motivation",
    "text": "Motivation\nSemantic frameworks provide powerful tools for characterizing what we can and cannot mean in using linguistic expressions.\n\nImportant properties: compositional and modular.\n\nCompositional: the meanings of complex expressions systematically computed from the meanings of smaller expressions, how they are assembled.\nModular: we can analyze one linguistic phenomenon at a time (e.g., anaphora, vagueness)‚Ä¶ then use a systematic recipe for putting the analyses together (e.g., anaphora and vagueness)."
  },
  {
    "objectID": "pds-intro/pds-intro.html#prior-approach-1",
    "href": "pds-intro/pds-intro.html#prior-approach-1",
    "title": "PDS Introduction",
    "section": "Prior approach #1",
    "text": "Prior approach #1\nChallenge: semantic frameworks don‚Äôt generally provide an apparatus for characterizing uncertainty about what we can mean in using linguistic expressions.\n\nResult: they have great difficulty characterizing:\n\nthe actual inferences that a language-comprehender draws;\nthe statistical patterns exhibited by these inferences (e.g., in a human inference dataset)."
  },
  {
    "objectID": "pds-intro/pds-intro.html#inferential-uncertainty",
    "href": "pds-intro/pds-intro.html#inferential-uncertainty",
    "title": "PDS Introduction",
    "section": "Inferential uncertainty",
    "text": "Inferential uncertainty\n\nThe magician‚Äôs assistant won‚Äôt admit that they laughed during the trick.\nDid the magician‚Äôs assistant laugh?"
  },
  {
    "objectID": "pds-intro/pds-intro.html#inferential-uncertainty-1",
    "href": "pds-intro/pds-intro.html#inferential-uncertainty-1",
    "title": "PDS Introduction",
    "section": "Inferential uncertainty",
    "text": "Inferential uncertainty\n\nThe magician‚Äôs assistant won‚Äôt admit that they laughed during the trick.\nDid the magician‚Äôs assistant laugh?"
  },
  {
    "objectID": "pds-intro/pds-intro.html#inferential-uncertainty-2",
    "href": "pds-intro/pds-intro.html#inferential-uncertainty-2",
    "title": "PDS Introduction",
    "section": "Inferential uncertainty",
    "text": "Inferential uncertainty\n\nThe magician‚Äôs assistant won‚Äôt admit that they laughed during the trick.\nDid the magician‚Äôs assistant laugh?"
  },
  {
    "objectID": "pds-intro/pds-intro.html#inferential-uncertainty-3",
    "href": "pds-intro/pds-intro.html#inferential-uncertainty-3",
    "title": "PDS Introduction",
    "section": "Inferential uncertainty",
    "text": "Inferential uncertainty\n\nThe magician‚Äôs assistant won‚Äôt admit that they laughed during the trick.\nDid the magician‚Äôs assistant laugh?"
  },
  {
    "objectID": "pds-intro/pds-intro.html#inferential-uncertainty-4",
    "href": "pds-intro/pds-intro.html#inferential-uncertainty-4",
    "title": "PDS Introduction",
    "section": "Inferential uncertainty",
    "text": "Inferential uncertainty\n\nThe magician‚Äôs assistant won‚Äôt admit that they laughed during the trick.\nDid the magician‚Äôs assistant laugh?"
  },
  {
    "objectID": "pds-intro/pds-intro.html#inferential-uncertainty-5",
    "href": "pds-intro/pds-intro.html#inferential-uncertainty-5",
    "title": "PDS Introduction",
    "section": "Inferential uncertainty",
    "text": "Inferential uncertainty\n\nThe magician‚Äôs assistant won‚Äôt admit that they laughed during the trick.\nDid the magician‚Äôs assistant laugh?\n\n\nAverage response:"
  },
  {
    "objectID": "pds-intro/pds-intro.html#inferential-uncertainty-6",
    "href": "pds-intro/pds-intro.html#inferential-uncertainty-6",
    "title": "PDS Introduction",
    "section": "Inferential uncertainty",
    "text": "Inferential uncertainty\n\nThe magician‚Äôs assistant won‚Äôt admit that they laughed during the trick.\nDid the magician‚Äôs assistant laugh?\n\n\nAverage response:"
  },
  {
    "objectID": "pds-intro/pds-intro.html#prior-approach-2",
    "href": "pds-intro/pds-intro.html#prior-approach-2",
    "title": "PDS Introduction",
    "section": "Prior approach #2",
    "text": "Prior approach #2\nFrameworks for probabilistic semantics and pragmatics provide powerful tools for characterizing uncertainty about what we can mean in using linguistics expressions:\n\nsampling from and querying arbitrary probability distributions\nrelating the output to (e.g., human) data\nformal model comparison\n\n\nChallenge: no general structure-preserving method for mapping between semantic analyses and probabilistic analyses."
  },
  {
    "objectID": "pds-intro/pds-intro.html#probabilistic-dynamic-semantics",
    "href": "pds-intro/pds-intro.html#probabilistic-dynamic-semantics",
    "title": "PDS Introduction",
    "section": "‚ÄúProbabilistic Dynamic Semantics‚Äù",
    "text": "‚ÄúProbabilistic Dynamic Semantics‚Äù\nGoal: framework where probabilistic reasoning can be added to a semantic analysis without changing its structure.\n\nResult:\n\ncharacterize and distinguish difference sources of uncertainty\nuse existing semantic theories by plugging them into the framework\nformally compare different semantic theories with each other"
  },
  {
    "objectID": "pds-intro/pds-intro.html#a-general-goal",
    "href": "pds-intro/pds-intro.html#a-general-goal",
    "title": "PDS Introduction",
    "section": "A general goal",
    "text": "A general goal\n\nHave a methodology that is widely available to linguists working on meaning.\n\n\nLarge-scale inference datasets are becoming more and more central to linguistic methodology, and semantic theory should keep up!\n\nTools to render semantic theories as probabilistic models can help catalyze things."
  },
  {
    "objectID": "pds-intro/pds-intro.html#the-basic-idea",
    "href": "pds-intro/pds-intro.html#the-basic-idea",
    "title": "PDS Introduction",
    "section": "The basic idea",
    "text": "The basic idea\nWe should think of an experimental trial as a little discourse."
  },
  {
    "objectID": "pds-intro/pds-intro.html#the-basic-idea-1",
    "href": "pds-intro/pds-intro.html#the-basic-idea-1",
    "title": "PDS Introduction",
    "section": "The basic idea",
    "text": "The basic idea\n\nSentences: Start with a prior distribution over parameters some kind (e.g., encoding world knowledge, basic meanings). Update this prior with \\(‚ü¶\\textit{s1}‚üß\\), then \\(‚ü¶\\textit{s2}‚üß\\), etc.\nQuestion: Add \\(‚ü¶\\textit{q}‚üß\\) to the stack of questions under discussion (Ginzburg 1996; Farkas and Bruce 2010).\nAnswer: Retrieve \\(‚ü¶\\textit{q}‚üß\\) from the question stack; respond (i.e., query an inference distribution)."
  },
  {
    "objectID": "pds-intro/pds-intro.html#upshot",
    "href": "pds-intro/pds-intro.html#upshot",
    "title": "PDS Introduction",
    "section": "Upshot",
    "text": "Upshot\nOnce we have modeled the entire discourse in terms of some semantic analysis, we end up with a distribution over answers to the question prompt.\n\nA distribution we can learn about from data."
  },
  {
    "objectID": "pds-intro/pds-intro.html#two-kinds-of-uncertainty",
    "href": "pds-intro/pds-intro.html#two-kinds-of-uncertainty",
    "title": "PDS Introduction",
    "section": "Two kinds of uncertainty",
    "text": "Two kinds of uncertainty\n\nresolved (or type-level) uncertainty\n\nlexical, structural, or semantic (e.g., scopal) ambiguity\n\nunresolved (or token-level) uncertainty\n\npresent on individual occasions of language use"
  },
  {
    "objectID": "pds-intro/pds-intro.html#example-of-resolved-uncertainty",
    "href": "pds-intro/pds-intro.html#example-of-resolved-uncertainty",
    "title": "PDS Introduction",
    "section": "Example of resolved uncertainty",
    "text": "Example of resolved uncertainty\n\nJo ran a race.\n\n\n\nlocomotion sense of ran\nmanagement/organizational sense of ran\n\n\nAbout nature of speech act."
  },
  {
    "objectID": "pds-intro/pds-intro.html#example-of-unresolved-uncertainty",
    "href": "pds-intro/pds-intro.html#example-of-unresolved-uncertainty",
    "title": "PDS Introduction",
    "section": "Example of unresolved uncertainty",
    "text": "Example of unresolved uncertainty\n\nJo is tall.\n\n\n\nuncertainty about Jo‚Äôs height\n\n\nIn some sense, independent of speech act."
  },
  {
    "objectID": "pds-intro/pds-intro.html#uncertainty-in-pds",
    "href": "pds-intro/pds-intro.html#uncertainty-in-pds",
    "title": "PDS Introduction",
    "section": "Uncertainty in PDS",
    "text": "Uncertainty in PDS\n\nProbability distributions may be ‚Äústacked‚Äù; this stacking is reflected in the types of semantic values.\n\nE.g., \\(\\P e\\) vs.¬†\\(\\P (\\P e)\\).\n\nResolved uncertainty: about the state of some discourse.\n\n\\(\\P œÉ\\)\n\nUnresolved uncertainty: encoded in the common ground.\n\n\\(\\P Œπ\\) (\\(Œπ\\) the type of possible worlds)\nThe common ground is an aspect of the state! \\(\\P œÉ = \\P (‚Ä¶ \\P Œπ ‚Ä¶)\\)"
  },
  {
    "objectID": "pds-intro/pds-intro.html#discourse-states",
    "href": "pds-intro/pds-intro.html#discourse-states",
    "title": "PDS Introduction",
    "section": "Discourse states",
    "text": "Discourse states\n\nLists of parameters - can be arbitrarily complex.\n\nThe common ground (or context set; Stalnaker (1978))\nThe question under discussion (Roberts 2012; Ginzburg 1996)\nFarkas and Bruce (2010) stuff (e.g., projected sets)\n(Whatever you want)"
  },
  {
    "objectID": "pds-intro/pds-intro.html#common-grounds",
    "href": "pds-intro/pds-intro.html#common-grounds",
    "title": "PDS Introduction",
    "section": "Common grounds",
    "text": "Common grounds\n\nProbability distributions over indices of some type\n\nencode what is true ‚Äúin the world‚Äù \nmaybe certain linguistic parameters\n\n\n\nStates vs.¬†indices of the common ground\nWhat kind of thing goes where?\n\nUltimately, an empirical question."
  },
  {
    "objectID": "pds-intro/pds-intro.html#dynamic-semantics",
    "href": "pds-intro/pds-intro.html#dynamic-semantics",
    "title": "PDS Introduction",
    "section": "Dynamic semantics",
    "text": "Dynamic semantics\n\nSome linguist walked in. They gave a lecture.\n\n\nPopular idea\nMany frameworks for dynamic semantics model sentence meanings as maps from input states to sets of output states (e.g., Groenendijk and Stokhof 1991; Muskens 1996, i.a.).\n\n\n\n1st sentence: \\(Œªs.\\{ x{::}s^{\\prime} ‚à£ s^{\\prime} = s ‚àß \\ct{ling}(x) ‚àß \\ct{walk}(x)\\}\\)\n2nd sentence: \\(Œªs.\\{ s^{\\prime} ‚à£ s^{\\prime} = s ‚àß \\ct{lecture}(\\ct{sel}(s))\\}\\)"
  },
  {
    "objectID": "pds-intro/pds-intro.html#expressions-and-discourses",
    "href": "pds-intro/pds-intro.html#expressions-and-discourses",
    "title": "PDS Introduction",
    "section": "Expressions and discourses",
    "text": "Expressions and discourses\n\nmap discourse states onto probability distributions over new discourse states: \\(œÉ ‚Üí \\P (Œ± √ó œÉ^{\\prime})\\)\ncomplex linguistic acts may be sequenced\n\nan operation, bind, native to the probability monad\n\n\n\nFor instance\n\n\\(\\abbr{assert}(‚ü¶\\textit{Jo is tall}‚üß) : œÉ ‚Üí \\P (‚ãÑ √ó œÉ)\\)\n\\(\\abbr{ask}(‚ü¶\\textit{how tall?}‚üß) : œÉ ‚Üí \\P (‚ãÑ √ó \\Q Œπ r œÉ)\\)\n\\(\\abbr{assert}(‚ü¶\\textit{Jo is tall}‚üß) &gt;&gt; \\abbr{ask}(‚ü¶\\textit{how tall?}‚üß) : œÉ ‚Üí \\P (‚ãÑ √ó \\Q Œπ r œÉ)\\)"
  },
  {
    "objectID": "pds-intro/pds-intro.html#response-functionslinking-models",
    "href": "pds-intro/pds-intro.html#response-functionslinking-models",
    "title": "PDS Introduction",
    "section": "Response functions/linking models",
    "text": "Response functions/linking models\n\nTake a discourse, together with a prior distribution over states‚Ä¶\nGive back a distribution over responses to the last question asked (given some testing instrument).\n\n\n\\[\n\\begin{align*}\n\\ct{respond} &: \\P œÉ ‚Üí (œÉ ‚Üí \\P (‚ãÑ √ó \\Q Œπ Œ± œÉ^{\\prime})) ‚Üí \\P œÅ\n\\end{align*}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#examples-of-linking-models",
    "href": "pds-intro/pds-intro.html#examples-of-linking-models",
    "title": "PDS Introduction",
    "section": "Examples of linking models",
    "text": "Examples of linking models\n\\[\n\\begin{align*}\n\\ct{respond} &: \\P œÉ ‚Üí (œÉ ‚Üí \\P (‚ãÑ √ó \\Q Œπ Œ± œÉ^{\\prime})) ‚Üí \\P œÅ\n\\end{align*}\n\\]\n\nLikert scale (categorical distribution); e.g., \\(œÅ = \\{\\textit{yes}, \\textit{maybe}, \\textit{no}\\}\\)\nSlider scale (truncated normal distribution);  \\(œÅ = r\\)\nBinary forced choice (Bernoulli distribution);  \\(œÅ = t\\)"
  },
  {
    "objectID": "pds-intro/pds-intro.html#ccg",
    "href": "pds-intro/pds-intro.html#ccg",
    "title": "PDS Introduction",
    "section": "CCG",
    "text": "CCG\nAtomic types\n\\[\n\\begin{align*}\n\\mathcal{A} &\\Coloneqq np ‚à£ n ‚à£ s ‚à£\\,\\,...\n\\end{align*}\n\\] Noun phrases (\\(np\\)), nouns (\\(n\\)), and sentences (\\(s\\)).\n\nComplex types\n\\[\n\\begin{align*}\n\\mathcal{C}_{\\mathcal{A}} &\\Coloneqq \\mathcal{A} ‚à£ \\mathcal{C}_{\\mathcal{A}}/\\mathcal{C}_{\\mathcal{A}} ‚à£ \\mathcal{C}_{\\mathcal{A}}\\backslash\\mathcal{C}_{\\mathcal{A}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#haskell",
    "href": "pds-intro/pds-intro.html#haskell",
    "title": "PDS Introduction",
    "section": "Haskell",
    "text": "Haskell\n\ndata Cat = Base String  -- atomic categories\n         | Cat :/: Cat  -- the forward slash\n         | Cat :\\: Cat  -- the backward slash\n  deriving (Eq)"
  },
  {
    "objectID": "pds-intro/pds-intro.html#ccg-expressions",
    "href": "pds-intro/pds-intro.html#ccg-expressions",
    "title": "PDS Introduction",
    "section": "CCG expressions",
    "text": "CCG expressions\nAn expression:\n\\[\\Large\n\\begin{align*}\n  \\expr{\\textit{dog}}{Œªx, i.\\ct{dog}(i)(x)}{n}\n\\end{align*}\n\\]\n\n\\(\\textit{dog}\\) is a string.\n\\(Œªx, i.\\ct{dog}(i)(x)\\) is a meaning (reprented in the Œª-calculus).\n\nIts type is \\(e ‚Üí Œπ ‚Üí t\\).\n\n\\(n\\) is a CCG category."
  },
  {
    "objectID": "pds-intro/pds-intro.html#semantic-types-in-detail",
    "href": "pds-intro/pds-intro.html#semantic-types-in-detail",
    "title": "PDS Introduction",
    "section": "Semantic types (in detail)",
    "text": "Semantic types (in detail)\n\nAtomic types\n\\[\n\\begin{align*}\nA \\Coloneqq e ‚à£ t ‚à£\\,\\,...\n\\end{align*}\n\\]\n\n\nComplex types\n\\[\n\\begin{align*}\n   \\mathcal{T}_{A} \\Coloneqq A ‚à£ \\mathcal{T}_{A} ‚Üí \\mathcal{T}_{A} ‚à£ \\mathcal{T}_{A} √ó \\mathcal{T}_{A} ‚à£ ‚ãÑ\n\\end{align*}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#haskell-1",
    "href": "pds-intro/pds-intro.html#haskell-1",
    "title": "PDS Introduction",
    "section": "Haskell",
    "text": "Haskell\ndata Type = Atom String  -- atomic types\n          | Type :‚Üí Type -- arrows\n          | Unit         -- unit type\n          | Type :√ó Type -- products\n          | TyVar String -- type variables\n  deriving (Eq)\n\nNote the type variables!"
  },
  {
    "objectID": "pds-intro/pds-intro.html#typing-rules",
    "href": "pds-intro/pds-intro.html#typing-rules",
    "title": "PDS Introduction",
    "section": "Typing rules",
    "text": "Typing rules\n \\[ \\scriptsize\n\\begin{array}{c}\n\\begin{prooftree}\n\\AxiomC{}\n\\RightLabel{$\\mathtt{Ax}$}\\UnaryInfC{$Œì, x : Œ± ‚ä¢ x : Œ±$}\n\\end{prooftree}\n& \\begin{prooftree}\n\\AxiomC{$Œì, x : Œ± ‚ä¢ t : Œ≤$}\n\\RightLabel{${‚Üí}\\mathtt{I}$}\\UnaryInfC{$Œì ‚ä¢ Œªx.t : Œ± ‚Üí Œ≤$}\n\\end{prooftree}\n& \\begin{prooftree}\n\\AxiomC{$Œì ‚ä¢ t : Œ± ‚Üí Œ≤$}\n\\AxiomC{$Œì ‚ä¢ u : Œ±$}\n\\RightLabel{${‚Üí}\\mathtt{E}$}\\BinaryInfC{$Œì ‚ä¢ t(u) : Œ≤$}\n\\end{prooftree} \\\\[2mm]\n\\begin{prooftree}\n\\AxiomC{}\n\\RightLabel{$‚ãÑ\\mathtt{I}$}\\UnaryInfC{$Œì ‚ä¢ ‚ãÑ : ‚ãÑ$}\n\\end{prooftree}\n& \\begin{prooftree}\n\\AxiomC{$Œì ‚ä¢ t : Œ±$}\n\\AxiomC{$Œì ‚ä¢ u : Œ≤$}\n\\RightLabel{$√ó\\mathtt{I}$}\\BinaryInfC{$Œì ‚ä¢ ‚ü®t, u‚ü© : Œ± √ó Œ≤$}\n\\end{prooftree}\n& \\begin{prooftree}\n\\AxiomC{$Œì ‚ä¢ t : Œ±_1 √ó Œ±_2$}\n\\RightLabel{$√ó\\mathtt{E}_{j}$}\\UnaryInfC{$Œì ‚ä¢ œÄ_{j}(t) : Œ±_{j}$}\n\\end{prooftree}\n\\end{array}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#typing-rules-1",
    "href": "pds-intro/pds-intro.html#typing-rules-1",
    "title": "PDS Introduction",
    "section": "Typing rules",
    "text": "Typing rules\n\\(‚Üí\\mathtt{E}\\)\n \\[\n\\begin{prooftree}\n\\AxiomC{$Œì ‚ä¢ t : Œ± ‚Üí Œ≤$}\n\\AxiomC{$Œì ‚ä¢ u : Œ±$}\n\\RightLabel{${‚Üí}\\mathtt{E}$}\\BinaryInfC{$Œì ‚ä¢ t(u) : Œ≤$}\n\\end{prooftree}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#haskell-vanilla-terms",
    "href": "pds-intro/pds-intro.html#haskell-vanilla-terms",
    "title": "PDS Introduction",
    "section": "Haskell: vanilla terms",
    "text": "Haskell: vanilla terms\n-- | Untyped Œª-terms. Types are assigned separately (i.e., \"extrinsically\").\ndata Term = Var VarName           -- Variables.\n          | Con Constant          -- Constants.\n          | Lam VarName Term      -- Abstractions.\n          | App Term Term         -- Applications.\n          | TT                    -- The 0-tuple.\n          | Pair Term Term        -- Pairing.\n          | Pi1 Term              -- First projection.\n          | Pi2 Term              -- Second projection.\n\nConstants:\n-- | Constants are indexed by either strings or real numbers.\ntype Constant = Either String Double"
  },
  {
    "objectID": "pds-intro/pds-intro.html#deriving-meanings",
    "href": "pds-intro/pds-intro.html#deriving-meanings",
    "title": "PDS Introduction",
    "section": "Deriving meanings",
    "text": "Deriving meanings\n\nType homomorphism\n\n\\(\\small ‚ü¶np‚üß = e \\hspace{2cm} ‚ü¶n‚üß = e ‚Üí Œπ ‚Üí t \\hspace{2cm} ‚ü¶s‚üß = Œπ ‚Üí t\\) \n\\(\\small ‚ü¶b / a‚üß = ‚ü¶b \\backslash a‚üß = ‚ü¶a‚üß ‚Üí ‚ü¶b‚üß\\)\n\n\n\nExample: application\n\n\\[\\scriptsize\n\\frac{\\expr{\\textit{every}}{Œªp^{e ‚Üí Œπ ‚Üí t}, q^{e ‚Üí Œπ ‚Üí t}, i^{Œπ}.(‚àÄy.p(y)(i) ‚Üí q(y)(i))^{t}}{s}/(s\\backslash np)/ n \\hspace{1cm} \\expr{\\textit{linguist}}{Œªx^{e}, i^{Œπ}.(\\ct{ling}(i)(x))^{t}}{n}}{\n\\expr{\\textit{every linguist}}{Œªq^{e ‚Üí Œπ ‚Üí t}, i^{Œπ}.(‚àÄy.\\ct{ling}(i)(y) ‚Üí q(y)(i))^{t}}{s}/(s\\backslash np)\n}&gt;\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#adding-probabilistic-types",
    "href": "pds-intro/pds-intro.html#adding-probabilistic-types",
    "title": "PDS Introduction",
    "section": "Adding probabilistic types",
    "text": "Adding probabilistic types\n\nNew atomic types\n\\[\nA \\Coloneqq e ‚à£ t ‚à£ r ‚à£\\,\\,...\n\\]\n\n\nNew complex types\n\\[\n\\mathcal{T}_{A} \\Coloneqq A ‚à£ \\mathcal{T}_{A} ‚Üí \\mathcal{T}_{A} ‚à£ \\mathcal{T}_{A} √ó \\mathcal{T}_{A} ‚à£ ‚ãÑ ‚à£ \\P \\mathcal{T}_{A}\n\\]\n\n\\(\\P t\\): a Bernoulli distribution‚Äîprobabilistically True or False.\n\\(\\P r\\): e.g., a normal distribution."
  },
  {
    "objectID": "pds-intro/pds-intro.html#haskell-2",
    "href": "pds-intro/pds-intro.html#haskell-2",
    "title": "PDS Introduction",
    "section": "Haskell",
    "text": "Haskell\ndata Type = Atom String  -- atomic types\n          | Type :‚Üí Type -- arrows\n          | Unit         -- unit type\n          | Type :√ó Type -- products\n          | P Type       -- probabilistic types\n          | TyVar String -- type variables\n  deriving (Eq)"
  },
  {
    "objectID": "pds-intro/pds-intro.html#probabilistic-typing-rules",
    "href": "pds-intro/pds-intro.html#probabilistic-typing-rules",
    "title": "PDS Introduction",
    "section": "Probabilistic typing rules",
    "text": "Probabilistic typing rules\n \\[\n\\begin{array}{c}\n\\begin{prooftree}\n\\AxiomC{$Œì ‚ä¢ t : Œ±$}\n\\RightLabel{$\\mathtt{Return}$}\\UnaryInfC{$Œì ‚ä¢ \\pure{t} : \\P Œ±$}\n\\end{prooftree}\n& \\begin{prooftree}\n\\AxiomC{$Œì ‚ä¢ t : \\P Œ±$}\n\\AxiomC{$Œì, x : Œ± ‚ä¢ u : \\P Œ≤$}\n\\RightLabel{$\\mathtt{Bind}$}\\BinaryInfC{$Œì ‚ä¢ \\left(\\begin{array}{l} x ‚àº t \\\\ u\\end{array}\\right) : \\P Œ≤$}\n\\end{prooftree}\n\\end{array}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#haskell-probabilistic-programs",
    "href": "pds-intro/pds-intro.html#haskell-probabilistic-programs",
    "title": "PDS Introduction",
    "section": "Haskell: probabilistic programs",
    "text": "Haskell: probabilistic programs\n-- | Untyped Œª-terms. Types are assigned separately (i.e., \"extrinsically\").\ndata Term = Var VarName           -- Variables.\n          | Con Constant          -- Constants.\n          | Lam VarName Term      -- Abstractions.\n          | App Term Term         -- Applications.\n          | TT                    -- The 0-tuple.\n          | Pair Term Term        -- Pairing.\n          | Pi1 Term              -- First projection.\n          | Pi2 Term              -- Second projection.\n          | Return Term           -- Construct a degenerate distribution.\n          | Let VarName Term Term -- Sample from a distribution and continue.\n\nLet x t u ¬†¬†¬†=¬†¬†¬† \\(\\begin{array}[t]{l}\nx ‚àº t \\\\\nu\n\\end{array}\\)\nReturn t ¬†¬†¬†= ¬†¬†¬†\\(\\pure{t}\\)"
  },
  {
    "objectID": "pds-intro/pds-intro.html#making-observations",
    "href": "pds-intro/pds-intro.html#making-observations",
    "title": "PDS Introduction",
    "section": "Making observations",
    "text": "Making observations\n\\[\n\\begin{align*}\n\\ct{observe}\\ \\ &:\\ \\ t ‚Üí \\P ‚ãÑ \\\\\n\\end{align*}\n\\]\n\n\\[\n\\begin{array}[t]{l}\nx ‚àº \\ct{Normal}(0, 1) \\\\\n\\ct{observe}(-1 ‚â§ x ‚â§ 1) \\\\\n\\pure{x}\n\\end{array}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#making-observations-1",
    "href": "pds-intro/pds-intro.html#making-observations-1",
    "title": "PDS Introduction",
    "section": "Making observations",
    "text": "Making observations\n\\[\n\\begin{align*}\n\\ct{observe}\\ \\ &:\\ \\ t ‚Üí \\P ‚ãÑ \\\\\n\\end{align*}\n\\]\n\\[\n\\begin{array}[t]{l}\nx ‚àº \\ct{Normal}(0, 1) \\\\\n\\ct{observe}(-1 ‚â§ x ‚â§ 1) \\\\\n\\pure{x}\n\\end{array}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#haskell-typing-constants",
    "href": "pds-intro/pds-intro.html#haskell-typing-constants",
    "title": "PDS Introduction",
    "section": "Haskell: typing constants",
    "text": "Haskell: typing constants\n-- | Assign types to constants.\ntype Sig = Constant -&gt; Maybe Type\n\n\nAn example signature\ne, t, r :: Type\ne = At E\nt = At T\nr = At R\n\ntau :: Sig\ntau = \\case\n  Left  \"observe\"   -&gt; Just (t :‚Üí P Unit)\n  Left  \"Normal\"    -&gt; Just (r :√ó r ‚Üí P r)\n  Left  \"Bernoulli\" -&gt; Just (r :‚Üí P t)\n  Left  \"mother\"    -&gt; Just (e :‚Üí e)\n  Right _           -&gt; Just r"
  },
  {
    "objectID": "pds-intro/pds-intro.html#types-of-expressions",
    "href": "pds-intro/pds-intro.html#types-of-expressions",
    "title": "PDS Introduction",
    "section": "Types of expressions",
    "text": "Types of expressions\n\\[\\Large\nœÉ ‚Üí \\P (Œ± √ó œÉ^{\\prime})\n\\]\n\n\\(Œ±\\) could be:\n\n\\(e ‚Üí Œπ ‚Üí t \\hspace{2cm}\\) (a predicate meaning)\n\\(e \\hspace{7cm}\\) (a np meaning)\netc."
  },
  {
    "objectID": "pds-intro/pds-intro.html#an-interface-for-meaning-composition",
    "href": "pds-intro/pds-intro.html#an-interface-for-meaning-composition",
    "title": "PDS Introduction",
    "section": "An interface for meaning composition",
    "text": "An interface for meaning composition\n\\[\n\\begin{align*}\n‚Ñô^{œÉ}_{œÉ^{\\prime}} Œ±\\ \\ &=\\ \\ œÉ ‚Üí \\P (Œ± √ó œÉ^{\\prime})\n\\end{align*}\n\\]\n\n\\[\n\\begin{align*}\n\\return{v}\\ \\ &=\\ \\ Œªs.\\pure{‚ü®v, s‚ü©} : ‚Ñô^{œÉ}_{œÉ}\n\\end{align*}\n\\]\n\n\n\\[\n\\begin{align*}\n\\begin{array}{rl}\n\\Do & x ‚Üê m : ‚Ñô^{œÉ}_{œÉ^{\\prime}} \\\\\n& k(x) : ‚Ñô^{œÉ^{\\prime}}_{œÉ^{\\prime\\prime}}\n\\end{array}\\ \\\n&=\\ \\ Œªs.\\left(\\begin{array}{l}\n‚ü®x, s^{\\prime}‚ü© ‚àº m(s) \\\\\nk(x)(s^{\\prime})\n\\end{array}\\right) : ‚Ñô^{œÉ}_{œÉ^{\\prime\\prime}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#types-of-expressions-1",
    "href": "pds-intro/pds-intro.html#types-of-expressions-1",
    "title": "PDS Introduction",
    "section": "Types of expressions",
    "text": "Types of expressions\n\nJo ran a race.\n\n\n\\(‚ü¶\\textit{run}‚üß : ‚Ñô^{œÉ}_{œÉ} (e ‚Üí Œπ ‚Üí t)\\)\n\n\n\\[\n\\expr{\\textit{run}}{Œªs.\\pure{‚ü®Œªx, i.\\ct{if\\_then\\_else}(œÑ_{\\textit{run}}(s))(\\ct{run}_{loc.}(i)(x), \\ct{run}_{org.}(i)(x)), s‚ü©}}{s \\backslash np}\n\\]\n\n\nDepends on the state.\nDoesn‚Äôt update it.\nNot very probabilistic."
  },
  {
    "objectID": "pds-intro/pds-intro.html#pds-rules",
    "href": "pds-intro/pds-intro.html#pds-rules",
    "title": "PDS Introduction",
    "section": "PDS Rules",
    "text": "PDS Rules\nExample: leftward application\n\\[\n\\begin{prooftree}\n\\AxiomC{$\\expr{s_{1}}{M_{1}}{b}$}\n\\AxiomC{$\\expr{s_{2}}{M_{2}}{c\\backslash b}$}\n\\RightLabel{$&lt;$}\\BinaryInfC{$\\expr{s_{1}\\,s_{2}}{\n\\begin{array}{rl}\n\\Do & m_{1} ‚Üê M_{1} \\\\\n& m_{2} ‚Üê M_{2}; \\\\\n& \\return{m_{2}(m_{1})}\n\\end{array}\n}{c}$}\n\\end{prooftree}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#a-sentence-meaning",
    "href": "pds-intro/pds-intro.html#a-sentence-meaning",
    "title": "PDS Introduction",
    "section": "A sentence meaning",
    "text": "A sentence meaning\n\nA linguist saw a philosopher.\n\n\n\\[\\tiny\\hspace{-6cm}\n\\begin{prooftree}\n\\AxiomC{$\\expr{\\textit{a ling.}}{Œªs.\\pure{‚ü®Œªi, k.‚àÉx.\\ct{ling}(i)(x) ‚àß k(i)(x), s‚ü©}}{s/(s\\backslash np)}$}\n\\AxiomC{$\\expr{\\textit{saw}}{Œªs.\\pure{‚ü®Œªi, y, x.\\ct{see}(i)(y)(x), s‚ü©}}{s\\backslash np/ np}$}\n\\AxiomC{$\\expr{\\textit{a phil.}}{Œªs.\\pure{‚ü®Œªi, k, x.‚àÉy.\\ct{phil}(i)(y) ‚àß k(i)(y)(x), s‚ü©}}{s\\backslash np/(s\\backslash np/np)}$}\n\\RightLabel{$&lt;$}\\BinaryInfC{$\\expr{\\textit{saw a phil.}}{Œªs.\\pure{‚ü®Œªx, i.‚àÉy.\\ct{phil}(i)(y) ‚àß \\ct{see}(i)(y)(x), s‚ü©}}{s \\backslash np}$}\n\\RightLabel{$&lt;$}\\BinaryInfC{$\\expr{\\textit{a linguist saw a philosopher}}{Œªs.\\pure{‚ü®Œªi.‚àÉx, y.\\ct{ling}(i)(x) ‚àß \\ct{phil}(i)(y) ‚àß \\ct{see}(i)(y)(x), s‚ü©}}{s}$}\n\\end{prooftree}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#a-sentence-meaning-1",
    "href": "pds-intro/pds-intro.html#a-sentence-meaning-1",
    "title": "PDS Introduction",
    "section": "A sentence meaning",
    "text": "A sentence meaning\n\n\\[\n\\expr{\\textit{a linguist saw a philosopher}}{Œªs.\\pure{‚ü®Œªi.‚àÉx, y.\\ct{ling}(i)(x) ‚àß \\ct{phil}(i)(y) ‚àß \\ct{see}(i)(y)(x), s‚ü©}}{s}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#intensionality",
    "href": "pds-intro/pds-intro.html#intensionality",
    "title": "PDS Introduction",
    "section": "Intensionality",
    "text": "Intensionality\nIntensional constants:\n\\[\n\\begin{align*}\n\\ct{see} &: Œπ ‚Üí e ‚Üí e ‚Üí t \\\\\n\\ct{ling} &: Œπ ‚Üí e ‚Üí t\n\\end{align*}\n\\]\n\nWe require other constants:\n\\[\n\\begin{align*}\n\\updct{see} &: (e ‚Üí e ‚Üí t) ‚Üí Œπ ‚Üí Œπ \\\\\n\\updct{ling} &: (e ‚Üí t) ‚Üí Œπ ‚Üí Œπ\n\\end{align*}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#how-intensional-constants-interact",
    "href": "pds-intro/pds-intro.html#how-intensional-constants-interact",
    "title": "PDS Introduction",
    "section": "How intensional constants interact",
    "text": "How intensional constants interact\n\n\\[\n\\ct{see}(\\updct{see}(p)(i)) = p \\\\\n\\]\n\n\n\\[\n\\ct{see}(\\updct{ling}(p)(i)) = \\ct{see}(i)\n\\]\n\n\n\\[\n\\ct{ling}(\\updct{ling}(p)(i)) = p \\\\\n\\]\n\n\n\\[\n\\ct{ling}(\\updct{see}(p)(i)) = \\ct{ling}(i)\n\\]\n\nCan be seen as a theory of states and locations.\n\nIndices are ‚Äústates‚Äù.\n(Pairs of) constants correspond to ‚Äúlocations‚Äù."
  },
  {
    "objectID": "pds-intro/pds-intro.html#the-common-ground",
    "href": "pds-intro/pds-intro.html#the-common-ground",
    "title": "PDS Introduction",
    "section": "The common ground",
    "text": "The common ground\nDefinition\nA common ground is a probabilistic program of type \\(\\P Œπ\\).\n\n\\(Œπ\\), a variable over types.\n\n\nA ‚Äústarting‚Äù index\n\\[\\ct{@} : Œπ\\]\n\nConstants that update indices can add information, to be later retrieved by intensional constants."
  },
  {
    "objectID": "pds-intro/pds-intro.html#states",
    "href": "pds-intro/pds-intro.html#states",
    "title": "PDS Introduction",
    "section": "States",
    "text": "States\nSome state-sensitive constants:\n\n\\[\n\\ct{CG} : œÉ ‚Üí \\P Œπ \\\\\n\\]\n\n\n\\[\n\\updct{CG} : \\P Œπ ‚Üí œÉ ‚Üí œÉ \\\\\n\\]\n\n\n\\[\n\\ct{QUD} : \\Q Œπ Œ± œÉ ‚Üí Œ± ‚Üí Œπ ‚Üí t\n\\]\n\n\n\\[\n\\updct{QUD} : (Œ± ‚Üí Œπ ‚Üí t) ‚Üí œÉ ‚Üí \\Q Œπ Œ± œÉ\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#haskell-the-q-constructor",
    "href": "pds-intro/pds-intro.html#haskell-the-q-constructor",
    "title": "PDS Introduction",
    "section": "Haskell: the \\(\\Q\\) constructor",
    "text": "Haskell: the \\(\\Q\\) constructor\n\n-- | Arrows, products, and probabilistic types, as well as (a) abstract types\n-- representing the addition of a new Q, and (b) type variables for encoding\n-- polymorphism.\ndata Type = At Atom\n          | Type :‚Üí Type\n          | Unit\n          | Type :√ó Type\n          | P Type\n          | Q Type Type Type\n          | TyVar String\n  deriving (Eq)"
  },
  {
    "objectID": "pds-intro/pds-intro.html#how-stateful-constants-interact",
    "href": "pds-intro/pds-intro.html#how-stateful-constants-interact",
    "title": "PDS Introduction",
    "section": "How stateful constants interact",
    "text": "How stateful constants interact\n\\[\n\\ct{CG}(\\updct{CG}(cg)(s)) = cg\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#manipulating-stateful-programs",
    "href": "pds-intro/pds-intro.html#manipulating-stateful-programs",
    "title": "PDS Introduction",
    "section": "Manipulating stateful programs",
    "text": "Manipulating stateful programs\n\\(\\ct{get}\\) and \\(\\ct{put}\\)\n\\[\n\\begin{align*}\n\\abbr{get} &: ‚Ñô^{œÉ}_{œÉ} œÉ\n\\end{align*}\n\\]\n\nGets the current state.\n\n\n\\[\n\\begin{align*}\n\\abbr{put} &: œÉ^{\\prime} ‚Üí ‚Ñô^{œÉ}_{œÉ^{\\prime}} ‚ãÑ \\\\\n\\end{align*}\n\\]\n\nOverwrites the current state with a new one."
  },
  {
    "objectID": "pds-intro/pds-intro.html#haskell-3",
    "href": "pds-intro/pds-intro.html#haskell-3",
    "title": "PDS Introduction",
    "section": "Haskell",
    "text": "Haskell\n\ngetPP :: Term\ngetPP = lam' s (Return (s & s))\n\nputPP :: Term -&gt; Term\nputPP s = Lam fr (Return (TT & s))\n  where fr:esh = fresh [s]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#asking-a-question",
    "href": "pds-intro/pds-intro.html#asking-a-question",
    "title": "PDS Introduction",
    "section": "Asking a question",
    "text": "Asking a question\n\\[\n\\begin{align*}\n\\abbr{ask} &: ‚Ñô^{œÉ}_{œÉ^{\\prime}} (Œ± ‚Üí Œπ ‚Üí t) ‚Üí ‚Ñô^{œÉ}_{\\Q Œπ Œ± œÉ^{\\prime}} ‚ãÑ\n\\end{align*}\n\\]\n\n\\[\n\\begin{align*}\n\\abbr{ask}(‚ü¶\\textit{how tall?}‚üß) &= \\begin{array}[t]{rl}\n\\Do & q^{r ‚Üí Œπ ‚Üí t} ‚Üê ‚ü¶\\textit{how tall?}‚üß^{‚Ñô^{œÉ}_{œÉ}(r ‚Üí Œπ ‚Üí t)} \\\\\n& s ‚Üê \\abbr{get} \\\\\n& \\abbr{put}(\\updct{QUD}(q)(s))\n\\end{array}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#responding-to-a-question",
    "href": "pds-intro/pds-intro.html#responding-to-a-question",
    "title": "PDS Introduction",
    "section": "Responding to a question",
    "text": "Responding to a question\n\n\\[\n\\begin{align*}\n\\abbr{respond}^{f_Œ¶ : r ‚Üí \\P œÅ} &: \\P œÉ ‚Üí ‚Ñô^{œÉ}_{\\Q Œπ r œÉ^{\\prime}} ‚ãÑ ‚Üí \\P œÅ\n\\end{align*}\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#responding-to-a-question-1",
    "href": "pds-intro/pds-intro.html#responding-to-a-question-1",
    "title": "PDS Introduction",
    "section": "Responding to a question",
    "text": "Responding to a question\n\\[\n\\begin{align*}\n&\\abbr{respond}^{f_Œ¶ : r ‚Üí \\P œÅ}(prior)(discourse) \\\\[5mm]\n&= \\begin{array}[t]{l}\ns ‚àº prior \\\\\n‚ü®‚ãÑ, s^{\\prime}‚ü© ‚àº discourse(s)\\\\\ni ‚àº \\ct{CG}(s^{\\prime}) \\\\\nf(\\ct{max}(Œªd.\\ct{QUD}(s^{\\prime})(d)(i)), Œ¶)\n\\end{array}\n\\end{align*}\n\\]\n\nExample \\(f_{Œ¶}\\)\n\n\\(f(x, Œ¶) = \\ct{Normal}(x, 1)\\)"
  },
  {
    "objectID": "pds-intro/pds-intro.html#a-probabilistic-model",
    "href": "pds-intro/pds-intro.html#a-probabilistic-model",
    "title": "PDS Introduction",
    "section": "A probabilistic model",
    "text": "A probabilistic model\n\\[\n\\begin{align*}\n\\abbr{respond}^{f_Œ¶ : r ‚Üí \\P œÅ} &: \\P œÉ ‚Üí ‚Ñô^{œÉ}_{\\Q Œπ r œÉ^{\\prime}} ‚ãÑ ‚Üí \\P œÅ\n\\end{align*}\n\\]\n\\[\n\\abbr{respond}^{Œªx.\\ct{Normal}(x, 1)}(prior)\\left(\n\\begin{array}{rl}\n\\Do & \\abbr{assert}(‚ü¶\\textit{Jo is tall}‚üß) \\\\\n& \\abbr{ask}(‚ü¶\\textit{how tall?}‚üß)\n\\end{array}\n\\right)\n\\]"
  },
  {
    "objectID": "pds-intro/pds-intro.html#how-do-we-actually-compute-this-stuff",
    "href": "pds-intro/pds-intro.html#how-do-we-actually-compute-this-stuff",
    "title": "PDS Introduction",
    "section": "How do we actually compute this stuff?",
    "text": "How do we actually compute this stuff?\n\n\nDelta-rules.\n-- | The type of Delta-rules.\ntype DeltaRule = Term -&gt; Maybe Term"
  },
  {
    "objectID": "pds-intro/pds-intro.html#summing-up",
    "href": "pds-intro/pds-intro.html#summing-up",
    "title": "PDS Introduction",
    "section": "Summing up",
    "text": "Summing up\n\nWe have a computational framework for:\n\nencoding grammar fragments\nrepresenting speech acts (assertions, questions)\nrepresenting theories linking inferences to behavior (as response functions)\ncomputing with the resulting probabilistic programs (via delta-rules)"
  },
  {
    "objectID": "pds-intro/pds-intro.html#references",
    "href": "pds-intro/pds-intro.html#references",
    "title": "PDS Introduction",
    "section": "",
    "text": "References\n\n\n\n\nFarkas, Donka F., and Kim B. Bruce. 2010. ‚ÄúOn Reacting to Assertions and Polar Questions.‚Äù Journal of Semantics 27 (1): 81‚Äì118. https://doi.org/10.1093/jos/ffp010.\n\n\nGinzburg, Jonathan. 1996. ‚ÄúDynamics and the Semantics of Dialogue.‚Äù In Logic, Language, and Computation, edited by Jerry Seligman and Dag Westerst√•hl, 1:221‚Äì37. Stanford: CSLI Publications.\n\n\nGroenendijk, Jeroen A., and Martin B. J. Stokhof. 1991. ‚ÄúDynamic Predicate Logic.‚Äù Linguistics and Philosophy 14 (1): 39‚Äì100. https://doi.org/10.1007/BF00628304.\n\n\nMuskens, Reinhard. 1996. ‚ÄúCombining Montague Semantics and Discourse Representation.‚Äù Linguistics and Philosophy 19 (2): 143‚Äì86. https://doi.org/10.1007/BF00635836.\n\n\nRoberts, Craige. 2012. ‚ÄúInformation Structure: Towards an Integrated Formal Theory of Pragmatics.‚Äù Semantics and Pragmatics 5 (December): 6:1‚Äì69. https://doi.org/10.3765/sp.5.6.\n\n\nStalnaker, Robert. 1978. ‚ÄúAssertion.‚Äù In Pragmatics, edited by Peter Cole, 9:315‚Äì32. New York: Academic Press."
  }
]