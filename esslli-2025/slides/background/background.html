<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-11cbd3234f06388db87dcfbb7d560d62.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <title>Probabilistic dynamic semantics</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #ebdbb2; background-color: #282828; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #ebdbb2; } /* Normal */
    code span.al { color: #282828; background-color: #cc241d; font-weight: bold; } /* Alert */
    code span.an { color: #98971a; } /* Annotation */
    code span.at { color: #d79921; } /* Attribute */
    code span.bn { color: #f67400; } /* BaseN */
    code span.bu { color: #d65d0e; } /* BuiltIn */
    code span.cf { color: #cc241d; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #b16286; } /* Char */
    code span.cn { color: #b16286; font-weight: bold; } /* Constant */
    code span.co { color: #928374; } /* Comment */
    code span.cv { color: #928374; } /* CommentVar */
    code span.do { color: #98971a; } /* Documentation */
    code span.dt { color: #d79921; } /* DataType */
    code span.dv { color: #f67400; } /* DecVal */
    code span.er { color: #cc241d; text-decoration: underline; } /* Error */
    code span.ex { color: #689d6a; font-weight: bold; } /* Extension */
    code span.fl { color: #f67400; } /* Float */
    code span.fu { color: #689d6a; } /* Function */
    code span.im { color: #689d6a; } /* Import */
    code span.in { color: #282828; background-color: #83a598; } /* Information */
    code span.kw { color: #ebdbb2; font-weight: bold; } /* Keyword */
    code span.op { color: #ebdbb2; } /* Operator */
    code span.ot { color: #689d6a; } /* Other */
    code span.pp { color: #d65d0e; } /* Preprocessor */
    code span.re { color: #928374; background-color: #1d2021; } /* RegionMarker */
    code span.sc { color: #b16286; } /* SpecialChar */
    code span.ss { color: #98971a; } /* SpecialString */
    code span.st { color: #98971a; } /* String */
    code span.va { color: #458588; } /* Variable */
    code span.vs { color: #98971a; } /* VerbatimString */
    code span.wa { color: #282828; background-color: #fabd2f; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-85a37fd1af7426b1e045386c9b01ffe1.css">
  <link rel="stylesheet" href="../styles.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Probabilistic dynamic semantics</h1>

<div class="quarto-title-authors">
</div>

</section>
<section class="slide level2">

<div class="hidden">
<p><span class="math display">\[
\newcommand{\expr}[3]{\begin{array}{c}
#1 \\
\bbox[lightblue,5px]{#2}
\end{array} ⊢ #3}
\newcommand{\ct}[1]{\bbox[font-size: 0.8em]{\mathsf{#1}}}
\newcommand{\updct}[1]{\ct{upd\_#1}}
\newcommand{\abbr}[1]{\bbox[transform: scale(0.95)]{\mathtt{#1}}}
\newcommand{\pure}[1]{\bbox[border: 1px solid orange]{\bbox[border: 4px solid transparent]{#1}}}
\newcommand{\return}[1]{\bbox[border: 1px solid black]{\bbox[border: 4px solid transparent]{#1}}}
\def\P{\mathtt{P}}
\def\Q{\mathtt{Q}}
\def\True{\ct{T}}
\def\False{\ct{F}}
\def\ite{\ct{if\_then\_else}}
\def\Do{\abbr{do}}
\]</span></p>
</div>
</section>
<section id="background" class="slide level2">
<h2>Background</h2>
<p>Who might benefit from this course?</p>
<ul>
<li class="fragment">People who have a semantics background and want to see how experimental methodologies for studying meaning might serve their goals.</li>
<li class="fragment">People who have a computational cog sci or experimental background and want to see how their approach might connect up with semantic theory.</li>
</ul>
</section>
<section id="some-papers-this-course-is-based-on" class="slide level2">
<h2>Some papers this course is based on</h2>

<img data-src="images/papers.png" class="r-stretch"></section>
<section id="a-couple-resources" class="slide level2">
<h2>A couple resources</h2>
<ul>
<li class="fragment">Course notes: https://juliangrove.github.io/nasslli-2025/</li>
<li class="fragment">Some code: https://juliangrove.github.io/pds/</li>
</ul>
</section>
<section id="the-bridge-from-theory-to-data" class="slide level2">
<h2>The bridge from theory to data</h2>
<p>Semantic theory has achieved remarkable success in characterizing compositional structure of meaning</p>
<div class="fragment">
<p><strong>The opportunity</strong>: Connect elegant formal theories to messy, gradient patterns from large-scale experiments</p>
</div>
<div class="fragment">
<p><strong>The goal</strong>: Maintain theoretical insights while extending to account for empirical richness</p>
</div>
<div class="fragment">
<p><span class="gb-orange">Probabilistic Dynamic Semantics as systematic bridge</span></p>
</div>
<aside class="notes">
<p>Welcome everyone. Today we’re starting our journey into Probabilistic Dynamic Semantics by first understanding the motivation - why do we need a new framework?</p>
<p>Semantic theory has given us elegant tools for understanding how meaning works compositionally. Think of how we can understand “Every linguist saw a student” by understanding the parts and how they combine. This has been incredibly successful.</p>
<p>But now we face an exciting opportunity. With large-scale experimental methods, we’re collecting thousands of judgments from hundreds of speakers. And what we find is messy - gradience everywhere, even where theory predicts categorical distinctions.</p>
<p>Our goal with PDS is not to replace traditional semantics but to extend it. We want to maintain those hard-won theoretical insights while being able to account for and predict the patterns we see in experimental data.</p>
<p>PDS will be our systematic bridge - taking compositional analyses from traditional Montagovian semantics and mapping them to probabilistic models we can test quantitatively.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="semantic-theorys-success" class="slide level2">
<h2>Semantic theory’s success</h2>
<p>Elegant formal systems capturing how complex meanings arise from systematic combination</p>
<div class="fragment">
<ul>
<li class="fragment">Decades of careful theoretical work</li>
<li class="fragment">Explains acceptability and inference judgments</li>
<li class="fragment">Compositional analyses via Montagovian methods</li>
</ul>
</div>
<div class="fragment">
<h3 id="the-challenge">The challenge</h3>
<p>How to test theoretical predictions at unprecedented scale while maintaining formal rigor?</p>
</div>
<aside class="notes">
<p>Let’s appreciate what semantic theory has accomplished. Over decades, semanticists have developed formal systems that elegantly capture compositionality - how complex meanings arise from simpler parts.</p>
<p>These theories explain two fundamental types of judgments speakers make: whether sentences are acceptable in context, and what inferences follow from what we say. The Montagovian tradition gave us tools to be precise about these intuitions.</p>
<p>But here’s our challenge: How do we test these theories when we move from a handful of carefully constructed examples to thousands of judgments across hundreds of predicates? How do we maintain formal rigor while engaging with messy empirical reality?</p>
<p>This is what motivates PDS - we need a framework that respects the insights of formal semantics while enabling quantitative evaluation against large-scale data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="dynamic-semantics" class="slide level2">
<h2>Dynamic semantics</h2>
<ol class="example" type="1">
<li class="fragment">Some linguist walked in. They gave a lecture.</li>
</ol>
<div class="fragment">
<h3 id="popular-idea">Popular idea</h3>
<p>Many frameworks for dynamic semantics model sentence meanings as maps from input states to sets of output states <span class="citation" data-cites="groenendijk_dynamic_1991 muskens_combining_1996">(e.g., <a href="#/references" role="doc-biblioref" onclick="">Groenendijk and Stokhof 1991</a>; <a href="#/references" role="doc-biblioref" onclick="">Muskens 1996</a>, i.a.)</span>.</p>
</div>
<div class="fragment">
<ul>
<li class="fragment">1st sentence: <span class="math inline">\(λs.\{ x{::}s^{\prime} ∣ s^{\prime} = s ∧ \ct{ling}(x) ∧ \ct{walk}(x)\}\)</span></li>
<li class="fragment">2nd sentence: <span class="math inline">\(λs.\{ s^{\prime} ∣ s^{\prime} = s ∧ \ct{lecture}(\ct{sel}(s))\}\)</span></li>
</ul>
</div>
</section>
<section id="formal-pragmatics" class="slide level2">
<h2>Formal pragmatics</h2>
<p>Much work has built on these ideas by enriching the notion of a state to countenance broader aspects of discourse structure.</p>
<div class="fragment">
<p>Besides entities for anaphora:</p>
<ul>
<li class="fragment">Common grounds <span class="citation" data-cites="stalnaker_assertion_1978">(<a href="#/references" role="doc-biblioref" onclick="">Stalnaker 1978</a>, et seq.)</span>.</li>
<li class="fragment">Questions under discussion <span class="citation" data-cites="roberts_information_2012 ginzburg_dynamics_1996">(QUDs, <a href="#/references" role="doc-biblioref" onclick="">Roberts 2012</a>; <a href="#/references" role="doc-biblioref" onclick="">Ginzburg 1996</a>)</span>.</li>
<li class="fragment">Contexts combining these and other structures <span class="citation" data-cites="farkas_reacting_2010">(<a href="#/references" role="doc-biblioref" onclick="">Farkas and Bruce 2010</a>)</span>.</li>
</ul>
</div>
<div class="fragment">
<div style="text-align: center;">
<p>Enriched states: central to PDS.</p>
</div>
</div>
</section>
<section id="standard-methodology" class="slide level2">
<h2>Standard methodology</h2>
<p><strong>Acceptability judgments</strong>: assess whether strings are well-formed in context <span class="citation" data-cites="chomsky_syntactic_1957 schütze_gramaticality_2016">(<a href="#/references" role="doc-biblioref" onclick="">Chomsky 1957</a>; see <a href="#/references" role="doc-biblioref" onclick="">Schütze 2016</a>)</span></p>
<p><span id="exm-comitative-good"></span> <span id="exm-coordination-bad"></span></p>
<div class="fragment">
<p><strong>Example:</strong> island effects <span class="citation" data-cites="ross_constraints_1967 sprouse_island_2021">(<a href="#/references" role="doc-biblioref" onclick="">Ross 1967</a>; see <a href="#/references" role="doc-biblioref" onclick="">Sprouse and Villata 2021</a>)</span></p>
<ol start="2" class="example" type="1">
<li class="fragment">What would you like with your coffee? ✓</li>
<li class="fragment">#What would you like and your coffee? ✗</li>
</ol>
</div>
<aside class="notes">
<p>Traditional semantic methodology centers on two types of judgments. First, acceptability judgments - is a string well-formed in a given context?</p>
<p>Consider these examples. In a context where a host asks what a guest wants with coffee, the first is clearly acceptable - using “with” to indicate accompaniment. But the second, trying to coordinate “what” and “your coffee”, is unacceptable. This follows from Ross’s coordinate structure constraint.</p>
<p>These judgments have been the bread and butter of linguistic theory. They’re usually quite clear - speakers have strong intuitions about what’s acceptable and what isn’t. This clarity has allowed theorists to build elegant accounts of linguistic constraints.</p>
<p>But as we’ll see, when we scale up to hundreds of predicates and thousands of judgments, things get more complex.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="standard-methodology-1" class="slide level2">
<h2>Standard methodology</h2>
<p><strong>Inference judgments</strong>: assess relationships between strings <span class="citation" data-cites="davis_semantics_2004">(see <a href="#/references" role="doc-biblioref" onclick="">Davis and Gillon 2004</a> and references therein)</span></p>
<div class="fragment">
<p><span id="exm-love-antecedent"></span></p>
<p><strong>Example:</strong> factivity <span class="citation" data-cites="white_lexically_2019">(see <a href="#/references" role="doc-biblioref" onclick="">White 2019</a> and references therein)</span></p>
<ol start="4" class="example" type="1">
<li class="fragment">Jo {loves, doesn’t love} that Mo left. ⟹ Mo left.</li>
</ol>
</div>
<aside class="notes">
<p>The second pillar of semantic methodology is inference judgments - what follows from what speakers say?</p>
<p>Here’s a classic example. When speakers hear “Jo loved that Mo left,” they typically infer that Mo actually left. This inference is particularly interesting because it survives under negation and questioning - “Jo didn’t love that Mo left” and “Did Jo love that Mo left?” both still suggest Mo left.</p>
<p>This pattern led theorists to identify a class of predicates - factives - that trigger presuppositions about their complements being true. We’ll return to factivity as one of our main case studies, because it presents a fascinating puzzle when we look at experimental data.</p>
<p>These inference judgments are central to semantic theory, but they also reveal complexities we need to address.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="observational-adequacy" class="slide level2">
<h2>Observational adequacy</h2>
<p><strong>Core desideratum:</strong> predict acceptability and inference patterns for any string <span class="citation" data-cites="chomsky_current_1964">(<a href="#/references" role="doc-biblioref" onclick="">Chomsky 1964</a>)</span></p>
<ul>
<li class="fragment">Requires mapping vocabulary to abstractions predicting judgments parsimoniously</li>
<li class="fragment">Abstractions: discrete or continuous, simple or richly structured</li>
</ul>
<aside class="notes">
<p>A core goal of semantic theory is observational adequacy - for any string, we should predict whether speakers find it acceptable and what inferences they draw.</p>
<p>Achieving this requires mapping vocabulary to abstractions. These abstractions might be discrete categories or continuous dimensions, simple features or rich structures. The key is parsimony - explaining the most data with the fewest theoretical commitments.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="observational-adequacy-1" class="slide level2">
<h2>Observational adequacy</h2>
<div class="fragment">
<p><strong>Example:</strong></p>
<ul>
<li class="fragment"><em>love</em>, <em>hate</em>, <em>be surprised</em>, <em>know</em> share properties</li>
<li class="fragment">Inferences survive under negation/questioning <span class="citation" data-cites="kiparsky_fact_1970 karttunen_observations_1971">(<a href="#/references" role="doc-biblioref" onclick="">Kiparsky and Kiparsky 1970</a>; cf. <a href="#/references" role="doc-biblioref" onclick="">Karttunen 1971</a>)</span></li>
<li class="fragment">Leads to positing shared properties, such as <em>factivity</em></li>
</ul>
</div>
<aside class="notes">
<p>Through careful analysis, semanticists have identified powerful generalizations. Take predicates like “love,” “hate,” “be surprised,” and “know.” They all give rise to inferences about their complements that survive under negation and questioning. This pattern led Kiparsky and Kiparsky to posit a shared property - factivity.</p>
<p>This is theoretical success at its best: identifying deep regularities across superficially different expressions. But as we’ll see when we look at experimental data, these generalizations become more complex at scale.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="descriptive-adequacy" class="slide level2">
<h2>Descriptive adequacy</h2>
<p><strong>Core desideratum:</strong> capturing data “in terms of significant generalizations that express underlying regularities” <span class="citation" data-cites="chomsky_current_1964">(<a href="#/references" role="doc-biblioref" onclick="">Chomsky 1964, 63</a>)</span></p>
<div class="fragment">
<p>Two approaches:</p>
<ol type="1">
<li class="fragment"><strong>Analysis-driven</strong>: Start with observationally adequate analyses, extract constraints <span class="citation" data-cites="chomsky_conditions_1973">(<a href="#/references" role="doc-biblioref" onclick="">Chomsky 1973</a>)</span></li>
<li class="fragment"><strong>Hypothesis-driven</strong>: Begin with constrained formalisms, test empirical coverage <span class="citation" data-cites="stabler_derivational_1997">(<a href="#/references" role="doc-biblioref" onclick="">Stabler 1997</a>)</span></li>
</ol>
</div>
<div class="fragment">
<p>PDS adopts hypothesis-driven approach for semantics</p>
</div>
<aside class="notes">
<p>Beyond observational adequacy lies a deeper goal: descriptive adequacy. We don’t just want to predict the data - we want to capture it in terms of significant generalizations that reveal underlying regularities in language.</p>
<p>The history of generative syntax shows two approaches to achieving this. The analysis-driven approach starts with observationally adequate analyses in expressive formalisms, then extracts constraints. Think of how Chomsky started with transformational grammars and extracted island constraints.</p>
<p>The hypothesis-driven approach works differently - begin with constrained formalisms like CCG or minimalist grammars, then test their empirical coverage. This approach makes phenomena boundaries clearer through representational constraints.</p>
<p>PDS takes the hypothesis-driven approach for semantics. We start with constrained semantic representations and test what phenomena they can capture. This becomes especially important when we want models that both accord with theory AND can be evaluated quantitatively against behavioral data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="our-approach" class="slide level2">
<h2>Our approach</h2>
<p>Probabilistic Dynamic Semantics aims to:</p>
<ul>
<li class="fragment">Provide a relatively unconstrained formalism</li>
<li class="fragment">State testable hypotheses about distribution of judgments<br>
</li>
<li class="fragment">Test hypotheses using behavioral data</li>
</ul>
<div class="fragment">
<h3 id="key-innovation">Key innovation</h3>
<p>Delineate phenomena through representational constraints while enabling quantitative evaluation</p>
</div>
<aside class="notes">
<p>So what’s our approach with PDS? We aim to provide a formalism that’s relatively unconstrained - flexible enough to encode different theoretical positions - while still being principled and compositional.</p>
<p>The key is that we can state testable hypotheses about the distribution of judgments, not just individual data points. When hundreds of people judge whether “Jo knows that Mo left” implies Mo left, we get a distribution of responses. PDS lets us predict these distributions from our semantic theory.</p>
<p>Our key innovation is maintaining the delineation of phenomena through representational constraints - the hallmark of the hypothesis-driven approach - while enabling quantitative evaluation against behavioral data.</p>
<p>As we’ll see in the implementation, this means building probability distributions into the semantics compositionally, so that when meanings combine, the uncertainty combines too. This lets us maintain theoretical commitments while engaging with empirical complexity.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="standard-methods" class="slide level2">
<h2>Standard methods</h2>
<h3 id="profound-insights-achieved">Profound insights achieved:</h3>
<ul>
<li class="fragment">Semantic composition</li>
<li class="fragment">Scope phenomena<br>
</li>
<li class="fragment">Discourse dynamics</li>
</ul>
<aside class="notes">
<p>Let’s acknowledge what traditional methods have achieved - profound insights into how meaning works. We understand semantic composition, how scope ambiguities arise, how discourse context affects interpretation, and the delicate dance between semantics and pragmatics.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="standard-methods-1" class="slide level2">
<h2>Standard methods</h2>
<div class="fragment">
<h3 id="natural-boundaries">Natural boundaries:</h3>
<ul>
<li class="fragment">How well do generalizations from 5-10 predicates extend to thousands?</li>
<li class="fragment">What factors beyond semantic knowledge influence judgments?</li>
<li class="fragment">How does abstract knowledge produce concrete behavioral responses?</li>
</ul>
</div>
<aside class="notes">
<p>But every methodology has natural boundaries. Traditional methods excel at deep analysis of carefully chosen examples, but face constraints when we scale up.</p>
<p>First, how well do generalizations from examining 5-10 predicates extend to the thousands in the lexicon? When we test 300 attitude predicates, do we find the neat categories theory predicts?</p>
<p>Second, what factors beyond pure semantic knowledge influence judgments? When people rate inferences, they’re using world knowledge, contextual reasoning, and response strategies alongside semantic competence.</p>
<p>Third, how exactly does abstract semantic knowledge produce concrete behavioral responses? What cognitive processes intervene between computing a meaning and moving a slider on a scale?</p>
<p>These aren’t failures of traditional methods - they’re opportunities for extension. And that’s where experimental semantics comes in.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="experimental-semantics-and-pragmatics" class="slide level2">
<h2>Experimental semantics and pragmatics</h2>
<p>Methodologies like inference judgment tasks study larger lexical areas</p>
<h3 id="existing-challenges">Existing challenges:</h3>
<ul>
<li class="fragment">Unclear how modeling constructs relate to semantic theory</li>
<li class="fragment">What lexical properties are implicated?</li>
<li class="fragment">How do they interact with language-external factors?</li>
</ul>
<div class="fragment">
<p><strong>The gradience question</strong>: What theoretical constructs underlie distributions of judgments?</p>
</div>
<aside class="notes">
<p>Experimental semantics and pragmatics have made great strides in scaling up semantic investigation. Using inference judgment tasks and other methodologies, we can study entire lexical domains rather than handfuls of examples.</p>
<p>But challenges remain. Often it’s unclear how the constructs in our statistical models relate to semantic theory. When we find that some predicates cluster together in their inference patterns, what lexical semantic properties are we actually discovering? And how do these properties interact with language-external factors like world knowledge?</p>
<p>The central challenge is the gradience question: when we get distributions of judgments rather than categorical patterns, what theoretical constructs underlie these distributions? Traditional semantic theory often assumes categorical distinctions, but the data shows continuity.</p>
<p>This is where PDS comes in - providing a framework where gradience can emerge from principled semantic sources while maintaining theoretical clarity about what those sources are.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-experimental-turn" class="slide level2">
<h2>The experimental turn</h2>
<p>Experimental semantics brings behavioral experimentation to meaning questions</p>
<h3 id="scaling-semantic-investigation">Scaling semantic investigation</h3>
<ul>
<li class="fragment">Traditional: handful of predicates</li>
<li class="fragment">Experimental: entire lexical domains</li>
</ul>
<aside class="notes">
<p>The experimental turn in semantics represents a methodological revolution. We’re bringing the tools of behavioral experimentation - controlled stimuli, statistical analysis, large participant samples - to questions about meaning.</p>
<p>The scale difference is dramatic. Where traditional methods might examine 5-10 predicates in detail, experimental approaches can investigate entire lexical domains. This isn’t just more of the same - at scale, new patterns emerge that are invisible in small samples.</p>
<p>Let me give you a concrete example with the MegaAttitude project, which we’ll look at next. This project studies hundreds of clause-embedding predicates - virtually the entire lexical class. At this scale, we can see gradience and clustering patterns that would be invisible looking at just “know,” “believe,” and “think.”</p>
<p>This scaling reveals both the power of traditional generalizations and their limits. Some patterns hold beautifully; others reveal unexpected complexity.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="megaattitude-project-example" class="slide level2">
<h2>MegaAttitude project example</h2>
<p>Large-scale investigation of clause-embedding predicates <span class="citation" data-cites="white_computational_2016 white_role_2018 white_lexicosyntactic_2018 white_frequency_2020 an_lexical_2020 moon_source_2020 kane_intensional_2022">(<a href="#/references" role="doc-biblioref" onclick="">White and Rawlins 2016</a>, <a href="#/references" role="doc-biblioref" onclick="">2018</a>, <a href="#/references" role="doc-biblioref" onclick="">2020</a>; <a href="#/references" role="doc-biblioref" onclick="">White et al. 2018</a>; <a href="#/references" role="doc-biblioref" onclick="">An and White 2020</a>; <a href="#/references" role="doc-biblioref" onclick="">Moon and White 2020</a>; <a href="#/references" role="doc-biblioref" onclick="">Kane, Gantt, and White 2022</a>)</span></p>
<ul>
<li class="fragment">Hundreds of predicates</li>
<li class="fragment">Multiple contexts and inference types</li>
<li class="fragment">Reveals subtle patterns difficult to see traditionally</li>
</ul>
<aside class="notes">
<p>The MegaAttitude project exemplifies the power of experimental semantics at scale. This massive undertaking studies hundreds of clause-embedding predicates across multiple inference types and contexts.</p>
<p>The scale is unprecedented - instead of the usual suspects like “know” and “believe,” we have data on predicates like “sense,” “intimate,” “let on,” and hundreds more. Each predicate is tested in multiple syntactic frames, with different inference types, across many experimental participants.</p>
<p>What emerges is fascinating. We see continuous gradients where theory predicted discrete categories. We find predicates clustering in unexpected ways. “Admit” patterns differently than “confess” despite their apparent similarity. Some predicates show bimodal response distributions suggesting genuine ambiguity, while others show unimodal gradience.</p>
<p>This rich empirical landscape is exactly what PDS is designed to model. We can ask: what theoretical properties produce these patterns? How do discrete semantic features combine with continuous uncertainty to yield the distributions we observe?</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="teasing-apart-contributing-factors" class="slide level2">
<h2>Teasing apart contributing factors</h2>
<p>Multiple factors influence inference judgments:</p>
<ul>
<li class="fragment"><strong>Semantic knowledge</strong>: core meanings of expressions</li>
<li class="fragment"><strong>World knowledge</strong>: prior beliefs about plausibility <span class="citation" data-cites="degen_prior_2021">(<a href="#/references" role="doc-biblioref" onclick="">Degen and Tonhauser 2021</a> among others)</span></li>
<li class="fragment"><strong>Contextual factors</strong>: discourse context and QUD <span class="citation" data-cites="simons_best_2017">(<a href="#/references" role="doc-biblioref" onclick="">Simons et al. 2017</a> among others)</span></li>
</ul>
<aside class="notes">
<p>One major advantage of experimental methods is the ability to tease apart different factors that influence inference judgments. When someone judges whether “Jo knows that Mo left” implies Mo left, multiple cognitive systems are at play.</p>
<p>First, semantic knowledge - the core meaning of “know” and how it composes with its complement. This is what traditional semantics focuses on.</p>
<p>Second, world knowledge. If Mo leaving is highly plausible given the context, people are more likely to infer it’s true. Degen and Tonhauser showed this beautifully by manipulating prior probabilities while holding predicates constant. Even canonical factives like “know” show gradient behavior when world knowledge varies.</p>
<p>Third, contextual factors like the discourse context and question under discussion. What’s at-issue can affect which inferences project.</p>
<p>[Continue to next slide for remaining factors]</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="teasing-apart-contributing-factors-1" class="slide level2">
<h2>Teasing apart contributing factors</h2>
<ul>
<li class="fragment"><strong>Individual differences</strong>: variation in interpretation</li>
<li class="fragment"><strong>Response strategies</strong>: how participants use rating scales</li>
</ul>
<div class="fragment">
<p>These are windows into cognitive processes, not confounds!</p>
</div>
<aside class="notes">
<p>[Continuing from previous slide]</p>
<p>Fourth, individual differences. Different speakers may have slightly different lexical entries or interpretation strategies. What looks like gradience at the population level might reflect discrete differences between individuals.</p>
<p>Fifth, response strategies - how participants map their internal judgments onto the response scale. A 7 on a slider might mean different things to different people.</p>
<p>Crucially, these aren’t confounds to be eliminated - they’re windows into the cognitive processes underlying semantic interpretation. PDS provides a framework where each factor can be modeled explicitly and their interactions studied systematically.</p>
<p>This multiplicity of factors is why we need sophisticated frameworks. Simple models that ignore this complexity will miss crucial patterns in the data. But we also need principled ways to separate semantic from non-semantic factors, which is what PDS provides through its modular architecture.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="making-linking-hypotheses-explicit" class="slide level2">
<h2>Making linking hypotheses explicit</h2>
<p>Experimental approaches force explicit theorizing about <span class="citation" data-cites="jasbi_linking_2019 waldon_modeling_2020 phillips_theories_2021">(<a href="#/references" role="doc-biblioref" onclick="">Jasbi, Waldon, and Degen 2019</a>; <a href="#/references" role="doc-biblioref" onclick="">Waldon and Degen 2020</a>; <a href="#/references" role="doc-biblioref" onclick="">Phillips et al. 2021</a>)</span></p>
<ul>
<li class="fragment">Link between semantic representations and behavioral responses</li>
<li class="fragment">What cognitive processes produce judgments?</li>
<li class="fragment">How do abstract representations map onto scale responses?</li>
</ul>
<div class="fragment">
<p><strong>Key insight</strong>: Different linking hypotheses → different predictions about response patterns</p>
<p>We can’t ignore how representations map onto task responses</p>
</div>
<aside class="notes">
<p>One of the most important contributions of experimental semantics is forcing us to be explicit about linking hypotheses - how abstract semantic representations connect to concrete behavioral responses.</p>
<p>Traditional semantics can remain agnostic about this link. We say “know” triggers a presupposition, but how does that abstract property produce a slider response? What cognitive processes intervene?</p>
<p>Different linking hypotheses make different predictions. Maybe people directly report their certainty about inferences. Or maybe they perform pragmatic reasoning first. Maybe they’re influenced by task demands or response biases.</p>
<p>The key insight is that we can’t ignore this linking problem if we want to test theories against data. Even if our real interest is in semantic representations, we need theories of how those representations produce behavior.</p>
<p>PDS makes linking hypotheses explicit through response functions that map semantic computations to response distributions. As we’ll see in the implementation, different response functions embody different assumptions about this crucial link.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="understanding-gradience" class="slide level2">
<h2>Understanding gradience</h2>
<p><strong>Striking finding</strong>: Pervasive gradient judgments</p>
<ul>
<li class="fragment">Even where theory assumes categorical distinctions</li>
<li class="fragment">Examples: gradable adjectives (expected) vs.&nbsp;factivity (puzzling)</li>
</ul>
<div class="fragment">
<p>Understanding gradience crucial for connecting semantics to behavioral data</p>
</div>
<aside class="notes">
<p>The most striking finding from experimental semantics is the pervasiveness of gradient judgments. This isn’t just in domains where we expect it, like gradable adjectives where “tall” has no sharp boundary. We find gradience even where traditional theory assumes categorical distinctions.</p>
<p>Factivity is the perfect example. Theory says predicates either trigger presuppositions or they don’t - “know” is factive, “think” isn’t. But experiments reveal continuous variation. Mean projection ratings vary smoothly from “pretend” at the bottom to “be annoyed” at the top, with no clear categorical break.</p>
<p>Multiple studies have found these gradient patterns. Whether using projection tasks, prosodic manipulations, or other paradigms, the gradience persists. This isn’t measurement noise - it’s a systematic phenomenon demanding theoretical explanation.</p>
<p>Understanding this gradience is crucial for connecting semantic theory to behavioral data. We need frameworks that can generate gradient predictions while maintaining theoretical clarity about what produces the gradience. That’s a core goal of PDS.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="examples-of-unexpected-gradience" class="slide level2">
<h2>Examples of unexpected gradience</h2>
<p>Large-scale datasets reveal gradience where theory might predict sharper distinctions <span class="citation" data-cites="white_role_2018">(<a href="#/references" role="doc-biblioref" onclick="">White and Rawlins 2018</a>)</span></p>
<div style="text-align: center;">
<p><img data-src="plots/veridicality_factivity.png" width="700"></p>
</div>
<aside class="notes">
<p>Let’s look at some concrete examples of unexpected gradience from the MegaAttitude datasets. These large-scale studies reveal patterns that aren’t readily apparent when examining just a few predicates.</p>
<p>This plot shows veridicality judgments from White and colleagues’ MegaVeridicality dataset. Each point represents a predicate’s mean inference strength under positive and negative embeddings. If factivity were a discrete property, we’d expect to see clear clusters - factive predicates in one region, non-factives in another.</p>
<p>Instead, we see continuous variation across both dimensions. There’s no natural boundary where we could draw a line between factive and non-factive predicates. This gradience presents a real challenge for theories that assume categorical distinctions.</p>
<p>But the challenge goes deeper when we start looking at relationships between different properties…</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="deriving-measures-from-gradience" class="slide level2">
<h2>Deriving measures from gradience</h2>
<p>To study relationships between properties, we need to derive measures</p>
<div style="text-align: center;">
<p><img data-src="plots/derived_factivity_measure.png" width="900"></p>
</div>
<aside class="notes">
<p>When we have gradient data, we need principled ways to derive measures of theoretical properties. For factivity, one approach is to take the maximum of veridicality ratings under positive and negative embedding.</p>
<p>The logic is that factive predicates should trigger inferences about their complements being true regardless of matrix polarity. So “know” should score high on both dimensions, while “think” might score high only under positive embedding.</p>
<p>This derived measure gives us a single factivity score for each predicate. But now we face a new challenge: how do we interpret relationships between such continuous measures? The theoretical predictions were developed for categorical properties, not gradients.</p>
<p>Let me show you why this matters with a concrete example…</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="relating-gradient-measures" class="slide level2">
<h2>Relating gradient measures</h2>
<p><strong>Example:</strong> Factivity vs.&nbsp;Neg-raising</p>
<p><span id="exm-negraising"></span> <span id="exm-negraising-inference"></span></p>
<div class="fragment">
<p><strong>Neg-raising</strong>: Predicates licensing inferences like:</p>
<ol start="5" class="example" type="1">
<li class="fragment">Jo doesn’t think that Mo left.</li>
<li class="fragment">Jo thinks that Mo didn’t leave.</li>
</ol>
</div>
<div class="fragment">
<p><strong>Expectation</strong>: Factives are not neg-raisers</p>
</div>
<aside class="notes">
<p>Let’s examine how gradient measures relate to each other. Consider neg-raising - a phenomenon where negation seems to “lower” into the complement clause.</p>
<p>A predicate is neg-raising if “X doesn’t think that P” can be interpreted as “X thinks that not-P.” Classic neg-raisers include “think,” “believe,” and “want.” The inference isn’t logically valid but is strongly preferred by speakers.</p>
<p>Traditional theory suggests factives shouldn’t be neg-raisers. The reasoning is that factives presuppose their complements are true, which conflicts with the neg-raised reading where the complement is false.</p>
<p>So we have a clear theoretical prediction about the relationship between two properties. Let’s see what happens when both properties are measured as gradients…</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="testing-predictions-with-gradient-data" class="slide level2">
<h2>Testing predictions with gradient data</h2>
<p>MegaNegRaising dataset <span class="citation" data-cites="an_lexical_2020">(<a href="#/references" role="doc-biblioref" onclick="">An and White 2020</a>)</span></p>
<div style="text-align: center;">
<p><img data-src="plots/negraising_factivity.png" width="700"></p>
</div>
<aside class="notes">
<p>Here we compare neg-raising scores from An and White’s MegaNegRaising dataset with our derived factivity measure. Each point is a predicate.</p>
<p>If the traditional generalization holds categorically, we’d expect points in the upper left (non-neg-raisers that are factive) and lower right (neg-raisers that are non-factive), but few in the upper right (both neg-raising and factive).</p>
<p>We do see this general trend - there’s a negative correlation. But it’s far from categorical. Some predicates score moderately on both dimensions. How do we interpret this? Is the generalization wrong, or does gradience emerge from other sources while the underlying relationship remains categorical?</p>
<p>This challenge - interpreting relationships between continuous measures - becomes even more complex when we look at other theoretical predictions…</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="beliefs-v.-desires" class="slide level2">
<h2>Beliefs v. Desires</h2>
<p><strong>Proposed Generalization:</strong> Predicates that trigger both belief and desire inferences background the belief component <span class="citation" data-cites="anand_factivity_2014">(<a href="#/references" role="doc-biblioref" onclick="">Anand and Hacquard 2014</a>)</span></p>
<aside class="notes">
<p>Let’s look at another theoretical relationship - between belief and desire inferences. Anand and Hacquard proposed that when predicates trigger inferences about both beliefs and desires, the belief component gets backgrounded.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="belief-inferences" class="slide level2">
<h2>Belief Inferences</h2>
<p>Measure of belief inferences from MegaIntensionality dataset <span class="citation" data-cites="kane_intensional_2022">(<a href="#/references" role="doc-biblioref" onclick="">Kane, Gantt, and White 2022</a>)</span></p>
<div style="text-align: center;">
<p><img data-src="plots/belief.png" width="600"></p>
</div>
<aside class="notes">
<p>This makes an interesting prediction: predicates shouldn’t trigger strong inferences about both beliefs and desires simultaneously. If they trigger desire inferences, belief inferences should be weak.</p>
<p>Kane and colleagues’ MegaIntensionality dataset lets us test this. They collected judgments about whether various attitude predicates trigger inferences about the attitude holder’s beliefs and desires.</p>
<p>Here we see the belief inference measure - how strongly each predicate suggests the attitude holder believes the complement. There’s clear variation across predicates, from “know” and “discover” at the top to “hope” and “want” at the bottom.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="desire-inferences" class="slide level2">
<h2>Desire Inferences</h2>
<p>Measure of desire inferences from MegaIntensionality dataset <span class="citation" data-cites="kane_intensional_2022">(<a href="#/references" role="doc-biblioref" onclick="">Kane, Gantt, and White 2022</a>)</span></p>
<div style="text-align: center;">
<p><img data-src="plots/desire.png" width="600"></p>
</div>
<aside class="notes">
<p>Now here’s the desire inference measure from the same dataset. Notice how the pattern is almost complementary to the belief measure.</p>
<p>Predicates like “want,” “hope,” and “wish” trigger strong desire inferences, while “know,” “discover,” and “realize” trigger weak ones. This already suggests the predicted trade-off might hold.</p>
<p>But to really test Anand and Hacquard’s hypothesis, we need to look at the relationship between these two measures directly…</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="relating-belief-and-desire-inferences" class="slide level2">
<h2>Relating belief and desire inferences</h2>
<div style="text-align: center;">
<p><img data-src="plots/desire_belief.png" width="750"></p>
</div>
<aside class="notes">
<p>This plot shows the relationship between desire and belief inference scores. The pattern is striking - there’s a strong negative correlation. Predicates cluster along a diagonal, with very few triggering strong inferences about both beliefs and desires.</p>
<p>This supports Anand and Hacquard’s theoretical claim remarkably well. But notice how different this relationship looks from the factivity/neg-raising plot we saw earlier. There, the relationship was noisy with many intermediate cases. Here, it’s much cleaner.</p>
<p>This raises a crucial point: different theoretical relationships manifest differently in gradient data. Some show clear trade-offs, others show noisy correlations. We need frameworks that can theorize about these continuous relationships, not just categorical ones.</p>
<p>This is exactly what PDS provides - a way to model different sources of gradience and predict what kinds of relationships we should see in the data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="key-takeaways-about-gradience" class="slide level2">
<h2>Key takeaways about gradience</h2>
<ol type="1">
<li class="fragment"><strong>Gradience is pervasive</strong> - even where theory assumes categories</li>
<li class="fragment"><strong>Relationships vary</strong> - some clean trade-offs, others noisy correlations<br>
</li>
<li class="fragment"><strong>Need new theoretical tools</strong> - to understand continuous relationships</li>
</ol>
<div class="fragment">
<p>This motivates our taxonomy of uncertainty types…</p>
</div>
<aside class="notes">
<p>These examples illustrate three key points about gradience in semantic judgments.</p>
<p>First, gradience is pervasive. We find it not just in obviously gradient domains like adjectives, but even where theory has assumed categorical distinctions like factivity.</p>
<p>Second, relationships between gradient measures vary dramatically. The belief/desire trade-off is clean and systematic. The factivity/neg-raising relationship is noisy and probabilistic. These different patterns suggest different underlying mechanisms.</p>
<p>Third, we need new theoretical tools. Traditional semantic theory was developed for categorical distinctions. When everything is gradient, how do we state and test theoretical claims? How do we distinguish noise from systematic variation?</p>
<p>This is why we need a principled taxonomy of uncertainty types. Different sources of gradience make different predictions about data patterns. Let’s turn to that taxonomy now…</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="a-taxonomy-of-uncertainty" class="slide level2">
<h2>A taxonomy of uncertainty</h2>
<p>Two fundamental types producing gradience:</p>
<div style="font-size: 90%;">
<pre><code>Sources of Gradience
├── Resolved (Type-Level) Uncertainty
│   ├── Ambiguity
│   │   ├── Lexical (e.g., "run" = locomote vs. manage)
│   │   ├── Syntactic (e.g., attachment ambiguities)
│   │   └── Semantic (e.g., scope ambiguities)
│   └── Discourse Status
│       └── QUD (Question Under Discussion)
└── Unresolved (Token-Level) Uncertainty
    ├── Vagueness (e.g., height threshold for "tall")
    ├── World knowledge (e.g., likelihood of facts)
    └── Task effects
        ├── Response strategies
        └── Response error</code></pre>
</div>
<aside class="notes">
<p>To understand gradience, we need a taxonomy of uncertainty. I’ll argue there are two fundamental types, and this distinction is crucial for building adequate models.</p>
<p>Resolved or type-level uncertainty involves discrete choices. There are multiple possible interpretations, and uncertainty about which one applies. This includes lexical ambiguity (run as locomotion vs.&nbsp;management), syntactic ambiguity (attachment sites), semantic ambiguity (scope), and discourse ambiguity (what’s the QUD).</p>
<p>Unresolved or token-level uncertainty persists even after all ambiguities are resolved. This includes vagueness (where exactly is the threshold for “tall”?), world knowledge (how likely is it that Mo actually left?), and task effects (how do I use this slider scale?).</p>
<p>This distinction matters theoretically. Resolved uncertainty suggests discrete representations with probabilistic selection - mixture models. Unresolved uncertainty suggests gradient representations or continuous parameters.</p>
<p>Different phenomena may involve different types, and PDS can model both. The framework’s power is in letting data adjudicate which type of uncertainty explains which patterns.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="resolved-uncertainty" class="slide level2">
<h2>Resolved uncertainty</h2>
<p>Multiple discrete possibilities—speakers choose among interpretations</p>
<p><strong>Example</strong>: “My uncle is running the race”</p>
<ul>
<li class="fragment"><em>run</em> = locomotion or management?</li>
<li class="fragment">“How likely is it that my uncle has good managerial skills?”
<ul>
<li class="fragment">Locomotion interpretation → ~0.2</li>
<li class="fragment">Management interpretation → ~0.8</li>
<li class="fragment">Population average → ~0.5 (mixture of discrete interpretations)</li>
</ul></li>
</ul>
<div class="fragment">
<p>Uncertainty “resolved” because once interpretation fixed, inference follows determinately</p>
</div>
<aside class="notes">
<p>Let’s look at resolved uncertainty in detail. The key characteristic is that there are multiple discrete possibilities, and speakers must choose among them.</p>
<p>Take “My uncle is running the race.” The verb “run” is ambiguous between locomotion and management senses. If asked about the uncle’s managerial skills, people who interpret “run” as locomotion might respond around 0.2 - runners don’t necessarily have management skills. Those interpreting it as management might respond around 0.8 - race organizers likely have such skills.</p>
<p>The population average might be 0.5, but this is a mixture of discrete interpretations, not genuine gradience. No individual thinks the uncle is simultaneously sort-of-running and sort-of-managing.</p>
<p>This uncertainty is “resolved” because once we fix the interpretation, the inference follows determinately. If running means locomotion, the managerial inference is weak. If it means managing, the inference is strong. The gradience emerges from averaging across resolutions, not from uncertainty within any single interpretation.</p>
<p>This pattern - bimodal or multimodal response distributions - is a signature of resolved uncertainty.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="unresolved-uncertainty" class="slide level2">
<h2>Unresolved uncertainty</h2>
<p>Persists even after fixing all ambiguities</p>
<p><strong>Example</strong>: “My uncle is tall”</p>
<ul>
<li class="fragment">No ambiguity about <em>tall</em>’s meaning</li>
<li class="fragment">Speakers uncertain about height threshold</li>
<li class="fragment">Classic vagueness—inherently gradient application <span class="citation" data-cites="fine_vagueness_1975 graff_shifting_2000 kennedy_vagueness_2007 van_rooij_vagueness_2011 sorensen_vagueness_2023">(<a href="#/references" role="doc-biblioref" onclick="">Fine 1975</a>; <a href="#/references" role="doc-biblioref" onclick="">Graff 2000</a>; <a href="#/references" role="doc-biblioref" onclick="">Christopher Kennedy 2007</a>; <a href="#/references" role="doc-biblioref" onclick="">Rooij 2011</a>; <a href="#/references" role="doc-biblioref" onclick="">Sorensen 2023</a>)</span></li>
</ul>
<div class="fragment">
<p>World knowledge: even knowing someone runs races, uncertainty about speed, endurance, etc. remains</p>
</div>
<aside class="notes">
<p>Unresolved uncertainty contrasts sharply with resolved uncertainty - it persists even after we’ve fixed all ambiguities.</p>
<p>Consider “My uncle is tall.” There’s no ambiguity about what “tall” means - it’s about height exceeding some threshold. But speakers remain uncertain about exactly where that threshold is. Is 5’11” tall? 6’0”? 6’1”? This is classic vagueness - the predicate’s application conditions are inherently gradient.</p>
<p>The philosophical literature on vagueness is vast because it’s genuinely puzzling. Unlike ambiguity, we can’t just pick an interpretation and be done. The uncertainty is baked into the meaning itself.</p>
<p>World knowledge creates additional layers of unresolved uncertainty. Even knowing someone runs races (locomotion sense, ambiguity resolved), we remain uncertain about their speed, endurance, likelihood of finishing. These uncertainties appear within individual trials, not just across participants.</p>
<p>This type of uncertainty typically produces unimodal, continuous response distributions - very different from the multimodal patterns of resolved uncertainty.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="why-this-distinction-matters" class="slide level2">
<h2>Why this distinction matters</h2>
<p>Type of uncertainty has profound implications:</p>
<ul>
<li class="fragment"><strong>Resolved uncertainty</strong> → discrete representations with probabilistic selection</li>
<li class="fragment"><strong>Unresolved uncertainty</strong> → gradient representations or probabilistic reasoning within fixed meanings</li>
</ul>
<div class="fragment">
<p>Different phenomena may involve different types:</p>
<ul>
<li class="fragment">Vagueness: unresolved (inherently uncertain application)</li>
<li class="fragment">Factivity: puzzling—resolved ambiguity or unresolved projection?</li>
</ul>
</div>
<aside class="notes">
<p>This distinction between resolved and unresolved uncertainty isn’t just taxonomic - it has profound implications for semantic theory and how we model behavioral data.</p>
<p>If gradience comes from resolved uncertainty, we need discrete semantic representations with probabilistic selection mechanisms. Think mixture models where people probabilistically choose among interpretations. The semantics stays discrete; the probability is in the selection.</p>
<p>If gradience comes from unresolved uncertainty, we need gradient representations or probabilistic reasoning within fixed meanings. The uncertainty is in the semantics itself, not just in choosing between options.</p>
<p>Different phenomena may involve different types. Vagueness seems clearly to involve unresolved uncertainty - there’s no discrete set of “tall” meanings to choose from. But factivity is puzzling. Is the gradience from resolved uncertainty (ambiguous predicates, variable discourse conditions) or unresolved uncertainty (gradient projection mechanisms)?</p>
<p>PDS lets us implement both possibilities and test them against data. As we’ll see in our case studies, the empirical patterns can distinguish these theoretical alternatives.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="case-studies-testing-theory-at-scale" class="slide level2">
<h2>Case studies: Testing theory at scale</h2>
<p>Two case studies exemplifying different framework aspects:</p>
<div class="fragment">
<h3 id="case-study-1-vagueness-and-gradable-adjectives">Case Study 1: Vagueness and gradable adjectives</h3>
<ul>
<li class="fragment">Ideal starting point—everyone agrees on gradient uncertainty</li>
<li class="fragment"><em>tall</em>, <em>expensive</em>, <em>old</em> lack sharp boundaries</li>
</ul>
</div>
<div class="fragment">
<h3 id="case-study-2-factivity-and-projection">Case Study 2: Factivity and projection</h3>
<ul>
<li class="fragment">Traditional theory treats as discrete</li>
<li class="fragment">Experimental data reveals pervasive gradience</li>
<li class="fragment">A theoretical puzzle</li>
</ul>
</div>
<aside class="notes">
<p>We’ll explore two case studies that exemplify different aspects of the PDS framework and show how it bridges theory and data.</p>
<p>Our first case study examines vagueness in gradable adjectives. This is an ideal starting point because everyone agrees these involve gradient uncertainty. Words like “tall,” “expensive,” and “old” lack sharp boundaries - there’s no precise height where someone becomes tall. This makes them perfect for demonstrating how PDS incorporates gradience into compositional semantics.</p>
<p>Our second case study tackles factivity and projection - a much more controversial domain. Traditional theory treats factivity as discrete: predicates either trigger presuppositions or they don’t. But experimental data reveals pervasive gradience that’s difficult to explain. This presents a theoretical puzzle that PDS can help resolve.</p>
<p>These case studies aren’t just examples - they represent two poles of semantic phenomena. Adjectives show uncontroversial gradience from vagueness. Factivity shows surprising gradience where theory expected discreteness. Together, they demonstrate PDS’s range and power.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="case-study-1-vagueness" class="slide level2">
<h2>Case Study 1: Vagueness</h2>
<p>Vague predicates lack sharp boundaries <span class="citation" data-cites="lakoff_hedges_1973 sadock_truth_1977 lasersohn_pragmatic_1999 krifka_approximate_2007 solt_vagueness_2015">(<a href="#/references" role="doc-biblioref" onclick="">Lakoff 1973</a>; <a href="#/references" role="doc-biblioref" onclick="">Sadock 1977</a>; <a href="#/references" role="doc-biblioref" onclick="">Lasersohn 1999</a>; <a href="#/references" role="doc-biblioref" onclick="">Krifka 2007</a>; <a href="#/references" role="doc-biblioref" onclick="">Solt 2015</a>)</span></p>
<p>Everyone agrees they involve gradient uncertainty</p>
<div class="fragment">
<p>Makes vagueness ideal for demonstrating PDS framework</p>
</div>
<aside class="notes">
<p>Vague predicates have been recognized as problematic for classical semantics since antiquity. If “tall” means exceeding some height, what height? There seems to be no principled answer.</p>
<p>The modern literature on vagueness is vast and sophisticated. Linguists and philosophers have developed numerous approaches - supervaluation, fuzzy logic, epistemic theories, degree-based theories. What they share is recognizing that vagueness involves gradient uncertainty.</p>
<p>This consensus makes vagueness ideal for demonstrating the PDS framework. We’re not trying to convince anyone that adjectives involve gradience - that’s accepted. Instead, we can focus on how PDS incorporates this gradience into compositional semantics while maintaining formal rigor.</p>
<p>We’ll see how degree-based theories of adjectives - already probabilistic in spirit - map naturally into PDS. The framework adds explicit probability distributions where traditional theories left things implicit.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="case-study-1-vagueness-1" class="slide level2">
<h2>Case Study 1: Vagueness</h2>
<p>Degree-based theories long recognized gradience <span class="citation" data-cites="klein_semantics_1980 bierwisch_semantics_1989 kamp_two_1975 kennedy_projecting_1999 kennedy_scale_2005 kennedy_vagueness_2007 barker_dynamics_2002">(<a href="#/references" role="doc-biblioref" onclick="">Klein 1980</a>; <a href="#/references" role="doc-biblioref" onclick="">Bierwisch 1989</a>; <a href="#/references" role="doc-biblioref" onclick="">Kamp 1975</a>; <a href="#/references" role="doc-biblioref" onclick="">Chris Kennedy 1999</a>; <a href="#/references" role="doc-biblioref" onclick="">Christopher Kennedy and McNally 2005</a>; <a href="#/references" role="doc-biblioref" onclick="">Christopher Kennedy 2007</a>; <a href="#/references" role="doc-biblioref" onclick="">Barker 2002</a>)</span></p>
<p><strong>Analysis</strong>: <em>tall</em> is true of <span class="math inline">\(x\)</span> if height(<span class="math inline">\(x\)</span>) <span class="math inline">\(≥ d_{\text{tall}}\)</span>(context) - Threshold varies with context - Gradient judgments even within fixed context</p>
</section>
<section id="case-study-1-vagueness-2" class="slide level2">
<h2>Case Study 1: Vagueness</h2>
<p>PDS can:</p>
<ul>
<li class="fragment">Maintain compositional degree-based analysis</li>
<li class="fragment">Add probability distributions over thresholds</li>
<li class="fragment">Model context shifts</li>
<li class="fragment">Link distributions to slider responses</li>
</ul>
<div class="fragment">
<p>Makes vagueness ideal for demonstrating framework</p>
</div>
</section>
<section id="case-study-1-vagueness-3" class="slide level2">
<h2>Case Study 1: Vagueness</h2>
<p>Recent work reveals additional complexity:</p>
<ul>
<li class="fragment"><strong>Relative adjectives</strong> (<em>tall</em>, <em>wide</em>): maximum gradience</li>
<li class="fragment"><strong>Absolute adjectives</strong> (<em>clean</em>, <em>dry</em>): different threshold distributions</li>
<li class="fragment"><strong>Minimum vs.&nbsp;maximum standard</strong>: asymmetric patterns</li>
</ul>
<aside class="notes">
<p>Recent experimental work on adjectives reveals complexity beyond what traditional theories predicted. Different types of adjectives show distinct patterns of gradience.</p>
<p>Relative adjectives like “tall” and “wide” show maximum gradience in their positive form. There’s no inherent standard - tallness is always relative to a comparison class. This produces smooth, continuous judgment curves.</p>
<p>Absolute adjectives like “clean” and “dry” work differently. They have endpoint-oriented standards - “clean” means no dirt, “dry” means no moisture. But experimentally, they still show gradience, just with different threshold distributions clustered near the endpoints.</p>
<p>There are also asymmetries between minimum and maximum standard adjectives. “Dirty” (minimum standard - any dirt suffices) patterns differently from “clean” (maximum standard - no dirt allowed). These patterns both support and refine formal theories.</p>
<p>[Continue to next slide for computational integration]</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="case-study-1-integration-with-models" class="slide level2">
<h2>Case Study 1: Integration with models</h2>
<div class="smaller">
<p><span class="citation" data-cites="lassiter_context_2013 qing_gradable_2014 kao_nonliteral_2014 lassiter_adjectival_2017 bumford_rationalizing_2021">(<a href="#/references" role="doc-biblioref" onclick="">Lassiter and Goodman 2013</a>, <a href="#/references" role="doc-biblioref" onclick="">2017</a>; <a href="#/references" role="doc-biblioref" onclick="">Qing and Franke 2014</a>; <a href="#/references" role="doc-biblioref" onclick="">Kao et al. 2014</a>; <a href="#/references" role="doc-biblioref" onclick="">Bumford and Rett 2021</a>)</span></p>
</div>
<p>PDS synthesizes and compares these approaches</p>
<p>Both support and refine formal theories</p>
</section>
<section id="how" class="slide level2">
<h2>How?</h2>
<div class="fragment">
<p>More or less: <br><br></p>
<p><span class="math inline">\(⟦\textit{how likely is it that X is tall}⟧ ⇒\)</span></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource stan number-lines code-with-copy"><code class="sourceCode stan"><span id="cb2-1"><a></a><span class="kw">parameters</span> {</span>
<span id="cb2-2"><a></a>  <span class="dt">real</span> v;</span>
<span id="cb2-3"><a></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="fl">0.0</span>, <span class="kw">upper</span>=<span class="fl">1.0</span>&gt; w;</span>
<span id="cb2-4"><a></a>}</span>
<span id="cb2-5"><a></a></span>
<span id="cb2-6"><a></a><span class="kw">model</span> {</span>
<span id="cb2-7"><a></a>  v ~ normal(<span class="fl">0.0</span>, <span class="fl">1.0</span>);</span>
<span id="cb2-8"><a></a></span>
<span id="cb2-9"><a></a>  <span class="kw">target +=</span> normal_lpdf(y | <span class="fl">1.0</span> - normal_cdf(v, <span class="fl">0.0</span>, <span class="fl">1.0</span>), w);</span>
<span id="cb2-10"><a></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<aside class="notes">
<p>Recent years have seen partial integration of adjective theories into computational models. These models make quantitative predictions about judgment patterns, but often in framework-specific ways.</p>
<p>Lassiter and colleagues model threshold uncertainty using Bayesian pragmatics. Qing and Franke use similar ideas for gradable adjectives. Kao et al.&nbsp;extend this to non-literal uses. Each approach captures important insights but uses different formal frameworks.</p>
<p>PDS provides a unifying framework where these different approaches can be compared. By implementing different theories within PDS, we can test their predictions against the same data using the same evaluation metrics.</p>
<p>This synthesis is powerful. We can ask: Does threshold uncertainty alone explain the patterns, or do we need pragmatic reasoning too? How do different linking hypotheses affect model fit? PDS enables principled theory comparison while maintaining compositionality.</p>
<p>The experimental patterns both support formal theories (yes, there are thresholds) and refine them (the threshold distributions are more complex than expected).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="case-study-2-factivity" class="slide level2">
<h2>Case Study 2: Factivity</h2>
<p>Traditional theory: predicates either trigger presuppositions or don’t <span class="citation" data-cites="kiparsky_fact_1970 karttunen_observations_1971">(<a href="#/references" role="doc-biblioref" onclick="">Kiparsky and Kiparsky 1970</a>; <a href="#/references" role="doc-biblioref" onclick="">Karttunen 1971</a>)</span></p>
<p><span id="exm-love-positive"></span> <span id="exm-love-negative"></span> <span id="exm-love-question"></span> <strong>Example</strong>: <em>love</em> appears factive</p>
<ol start="7" class="example" type="1">
<li class="fragment">Jo loves that Mo left. ⟹ Mo left.</li>
<li class="fragment">Jo doesn’t love that Mo left. ⟹ Mo left.</li>
<li class="fragment">Does Jo love that Mo left? ⟹ Mo left.</li>
</ol>
<aside class="notes">
<p>Now let’s turn to factivity - a phenomenon that presents a real puzzle for connecting theory to data.</p>
<p>Traditional theory treats factivity as a discrete property. Predicates either trigger presuppositions about their complements or they don’t. “Know” is factive - it presupposes its complement is true. “Think” is non-factive - it doesn’t.</p>
<p>The classic diagnostic uses the “family of sentences” test. A predicate is factive if the inference about its complement survives under negation and questioning. “Love” appears factive because all three sentences suggest Mo left - the positive assertion, the negation, and the question.</p>
<p>This categorical view made sense given traditional methodology. Looking at clear cases like “know” vs.&nbsp;“think,” the distinction seems sharp. But what happens when we test hundreds of predicates with hundreds of participants?</p>
<p>Spoiler: we find pervasive gradience that’s difficult to explain under the discrete view.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="case-study-2-factivity---definition" class="slide level2">
<h2>Case Study 2: Factivity - Definition</h2>
<p>A predicate is <em>factive</em> if it triggers inferences about its complement that project through entailment-canceling operators</p>
<p>Inference about complement truth survives:</p>
<ul>
<li class="fragment">Negation</li>
<li class="fragment">Questions</li>
<li class="fragment">Other embeddings</li>
</ul>
<aside class="notes">
<p>Let’s be precise about what factivity means. A predicate is factive if it triggers inferences about its complement that project through entailment-canceling operators.</p>
<p>What are entailment-canceling operators? Negation is the classic example - “Jo laughed” entails Jo exists, but “Jo didn’t laugh” doesn’t. Questions are another - “Did Jo laugh?” doesn’t entail Jo laughed.</p>
<p>But with factive predicates, complement inferences survive these operators. “Jo knows that Mo left” suggests Mo left. So does “Jo doesn’t know that Mo left” and “Does Jo know that Mo left?” The inference projects through the operators.</p>
<p>This projection behavior is what makes factivity special and theoretically interesting. It seems to mark a natural class of predicates with shared semantic properties. But as we’ll see, the empirical reality is more complex than this neat categorical picture suggests.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="case-study-2-factivity-1" class="slide level2">
<h2>Case Study 2: Factivity</h2>
<p><span class="citation" data-cites="white_role_2018">White and Rawlins (<a href="#/references" role="doc-biblioref" onclick="">2018</a>)</span> and <span class="citation" data-cites="degen_are_2022">Degen and Tonhauser (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span> found continuous variation:</p>
<ul>
<li class="fragment">No clear line between factive and non-factive predicates</li>
<li class="fragment">Mean projection ratings vary continuously</li>
<li class="fragment"><em>pretend</em> (lowest) to <em>be annoyed</em> (highest)</li>
</ul>
<aside class="notes">
<p>Here’s where things get interesting. When White and colleagues tested hundreds of attitude predicates, they found continuous variation in projection strength - not the discrete categories theory predicted.</p>
<p>Look at this plot. Each point is a predicate’s mean projection rating. If factivity were discrete, we’d expect clustering - a factive group with high projection, a non-factive group with low projection. Instead, we see a smooth gradient from “pretend” at the bottom to “be annoyed” at the top.</p>
<p>Where would you draw the line between factive and non-factive? At 0.5? Why not 0.6 or 0.4? The data doesn’t suggest any natural boundary.</p>
<p>This is a serious puzzle. Either our theoretical understanding is wrong - maybe factivity isn’t discrete - or something about the relationship between discrete semantic properties and gradient behavioral responses needs explaining. PDS lets us explore both possibilities formally.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="case-study-2-factivity-2" class="slide level2">
<h2>Case Study 2: Factivity</h2>
<div style="text-align: center;">
<p><embed data-src="plots/projection_no_fact_means.pdf" width="650"></p>
</div>
<aside class="notes">
<p>Here’s where things get interesting. When White and colleagues tested hundreds of attitude predicates, they found continuous variation in projection strength - not the discrete categories theory predicted.</p>
<p>Look at this plot. Each point is a predicate’s mean projection rating. If factivity were discrete, we’d expect clustering - a factive group with high projection, a non-factive group with low projection. Instead, we see a smooth gradient from “pretend” at the bottom to “be annoyed” at the top.</p>
<p>Where would you draw the line between factive and non-factive? At 0.5? Why not 0.6 or 0.4? The data doesn’t suggest any natural boundary.</p>
<p>This is a serious puzzle. Either our theoretical understanding is wrong - maybe factivity isn’t discrete - or something about the relationship between discrete semantic properties and gradient behavioral responses needs explaining. PDS lets us explore both possibilities formally.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="factivity-fundamental-discreteness" class="slide level2">
<h2>Factivity: Fundamental Discreteness</h2>
<p><strong>Hypothesis 1</strong>: Factivity is discrete; gradience arises from:</p>
<ul>
<li class="fragment">Multiple predicate senses (factive and non-factive variants)</li>
<li class="fragment">Structural ambiguity affecting projection <span class="citation" data-cites="varlokosta_issues_1994 giannakidou_polarity_1998 giannakidou_affective_1999 giannakidou_dependency_2009 roussou_selecting_2010 farudi_antisymmetric_2007 abrusan_predicting_2011 kastner_factivity_2015 ozyildiz_attitude_2017">(<a href="#/references" role="doc-biblioref" onclick="">Varlokosta 1994</a>; <a href="#/references" role="doc-biblioref" onclick="">Giannakidou 1998</a>, <a href="#/references" role="doc-biblioref" onclick="">1999</a>, <a href="#/references" role="doc-biblioref" onclick="">2009</a>; <a href="#/references" role="doc-biblioref" onclick="">Roussou 2010</a>; <a href="#/references" role="doc-biblioref" onclick="">Farudi 2007</a>; <a href="#/references" role="doc-biblioref" onclick="">Abrusán 2011</a>; <a href="#/references" role="doc-biblioref" onclick="">Kastner 2015</a>; <a href="#/references" role="doc-biblioref" onclick="">Ozyildiz 2017</a>)</span></li>
<li class="fragment">Contextual variation in whether complements are at-issue <span class="citation" data-cites="simons_best_2017 roberts_preconditions_2024 qing_rational_2016">(<a href="#/references" role="doc-biblioref" onclick="">Simons et al. 2017</a>; <a href="#/references" role="doc-biblioref" onclick="">Roberts and Simons 2024</a>; <a href="#/references" role="doc-biblioref" onclick="">Qing, Goodman, and Lassiter 2016</a>)</span></li>
</ul>
<aside class="notes">
<p>The Fundamental Discreteness hypothesis maintains that factivity is discrete, but gradience emerges from other sources. This preserves traditional theoretical insights while explaining the puzzling data.</p>
<p>First, predicates might have multiple senses - factive and non-factive variants. “Realize” might have a factive sense (coming to know) and a non-factive sense (understanding). Population-level gradience would reflect mixture across senses.</p>
<p>Second, structural ambiguity might affect projection. Different syntactic analyses of the same string might have different projection properties. The complement might be parsed as truly embedded under the attitude verb, or as a parenthetical, affecting whether presuppositions project.</p>
<p>Third, contextual factors like at-issueness might vary. When complements address the question under discussion, their content might not project as strongly. This would create gradience even with discrete underlying mechanisms.</p>
<p>Under this view, factivity remains a discrete semantic property, but various factors create gradient behavioral patterns.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="factivity-fundamental-gradience" class="slide level2">
<h2>Factivity: Fundamental Gradience</h2>
<p><strong>Hypothesis 2</strong>: No discrete property exists</p>
<ul>
<li class="fragment">Gradient degrees of complement truth support</li>
<li class="fragment">Continuous variation reflects semantic reality <span class="citation" data-cites="tonhauser_how_2018">(<a href="#/references" role="doc-biblioref" onclick="">Tonhauser, Beaver, and Degen 2018</a>)</span></li>
</ul>
<aside class="notes">
<p>The Fundamental Gradience hypothesis takes a more radical position: there’s no discrete factivity property at all. Instead, predicates provide gradient degrees of support for their complement’s truth.</p>
<p>Under this view, the continuous variation we observe in experiments directly reflects semantic reality. “Know” strongly supports its complement being true, “think” weakly supports it, and other predicates fall at various points between. There’s no categorical distinction, just a continuum.</p>
<p>This aligns with Tonhauser and colleagues’ Gradient Projection Principle - that projection strength varies continuously based on multiple factors including lexical semantics, world knowledge, and discourse structure.</p>
<p>This hypothesis is theoretically simpler in some ways - no need to explain away the gradience. But it’s more radical, abandoning a distinction that seemed well-motivated by traditional data. It also raises questions about how gradient projection would work compositionally.</p>
<p>PDS lets us implement both hypotheses formally and test their predictions.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="factivity-testing-both-hypotheses" class="slide level2">
<h2>Factivity: Testing both hypotheses</h2>
<ul>
<li class="fragment">PDS enables testing both hypotheses against fine-grained response distributions</li>
<li class="fragment">Not just means, but entire patterns including multimodality</li>
</ul>
<aside class="notes">
<p>The power of PDS is that we can implement both hypotheses formally and test their predictions against fine-grained data patterns - not just average ratings but entire response distributions.</p>
<p>If factivity is discrete with gradience from mixture, we’d expect multimodal response distributions. Some participants interpret predicates as factive (high ratings), others as non-factive (low ratings), producing bimodality.</p>
<p>If factivity is fundamentally gradient, we’d expect unimodal distributions centered at different points for different predicates. The variation would be smooth and continuous.</p>
<p>PDS can generate predictions for both patterns. We can implement discrete factivity with probabilistic selection between variants, or gradient factivity with continuous parameters. The data can then adjudicate between theories.</p>
<p>As we’ll see in the implementation section, the discrete hypothesis actually fits the data better - suggesting that despite surface gradience, factivity involves categorical distinctions. This vindicates the traditional view while explaining the puzzling experimental patterns.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="factivity-testing-paradigms" class="slide level2">
<h2>Factivity: Testing paradigms</h2>
<p>Various experimental paradigms:</p>
<ul>
<li class="fragment">Prosodic manipulations: <span class="citation" data-cites="tonhauser_prosodic_2016">Tonhauser (<a href="#/references" role="doc-biblioref" onclick="">2016</a>)</span>; <span class="citation" data-cites="djarv_prosodic_2017">Djärv and Bacovcin (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span>; <span class="citation" data-cites="jeong_prosodically-conditioned_2021">Jeong (<a href="#/references" role="doc-biblioref" onclick="">2021</a>)</span></li>
<li class="fragment">Prior beliefs: <span class="citation" data-cites="degen_prior_2021">Degen and Tonhauser (<a href="#/references" role="doc-biblioref" onclick="">2021</a>)</span></li>
<li class="fragment">Large-scale judgments: <span class="citation" data-cites="white_role_2018">White and Rawlins (<a href="#/references" role="doc-biblioref" onclick="">2018</a>)</span>; <span class="citation" data-cites="white_lexicosyntactic_2018">White et al. (<a href="#/references" role="doc-biblioref" onclick="">2018</a>)</span>; <span class="citation" data-cites="degen_are_2022">Degen and Tonhauser (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span>; <span class="citation" data-cites="kane_intensional_2022">Kane, Gantt, and White (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span></li>
</ul>
<aside class="notes">
<p>The factivity puzzle has motivated diverse experimental paradigms, each revealing different aspects of the phenomenon.</p>
<p>Prosodic manipulation studies show that focus and accent placement affect projection strength. This supports the view that information structure matters - focused content is less likely to project.</p>
<p>Large-scale judgment studies like MegaVeridicality test hundreds of predicates systematically. These reveal the full gradient landscape rather than just endpoints.</p>
<p>Prior belief manipulations tease apart semantic from world knowledge contributions. Even canonical factives show reduced projection when complements are implausible.</p>
<p>Each paradigm provides a different window into the phenomenon. PDS models can incorporate insights from all of them - prosodic effects through discourse parameters, cognitive effects through response noise, prior beliefs through world knowledge distributions.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="theoretical-challenges-from-gradience" class="slide level2">
<h2>Theoretical challenges from gradience</h2>
<p>The gradience poses challenges <span class="citation" data-cites="simons_observations_2007 simons_what_2010 simons_best_2017 tonhauser_how_2018">(<a href="#/references" role="doc-biblioref" onclick="">Simons 2007</a>; <a href="#/references" role="doc-biblioref" onclick="">Simons et al. 2010</a>, <a href="#/references" role="doc-biblioref" onclick="">2017</a>; <a href="#/references" role="doc-biblioref" onclick="">Tonhauser, Beaver, and Degen 2018</a>)</span></p>
<div class="fragment">
<h3 id="key-questions">Key questions:</h3>
<ul>
<li class="fragment">Is the gradience fundamental or derived?</li>
<li class="fragment">What mechanisms produce continuous variation?</li>
<li class="fragment">How do semantic and pragmatic factors interact?</li>
</ul>
</div>
<aside class="notes">
<p>The pervasive gradience in projection poses serious theoretical challenges that have motivated substantial recent work.</p>
<p>The fundamental question is whether gradience is a core property of projection or derived from other sources. If fundamental, we need gradient semantic representations - a major revision to standard theories. If derived, we need to identify the sources and show how they produce the observed patterns.</p>
<p>We also need mechanisms that produce continuous variation. Discrete categories usually produce discrete behaviors. How do we get smooth gradients? Through probabilistic selection? Continuous parameters? Multiple interacting factors?</p>
<p>Finally, how do semantic and pragmatic factors interact? Is projection computed by semantic mechanisms then modulated by pragmatics? Or is it pragmatic from the start?</p>
<p>PDS provides tools to address these questions formally. We can implement different theoretical positions, derive their predictions, and test them against data. This moves debates from intuition to empirical testing.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-pds-provides" class="slide level2">
<h2>What PDS provides</h2>
<p>A possible discourse:</p>
<ul>
<li class="fragment">Prior knowledge: <em>Zoe is a math major</em></li>
<li class="fragment">Some predicate: <em>X <u>discovered</u> that Zoe calculated the tip</em></li>
<li class="fragment">Question: <em>How likely is it that Zoe calculated the tip?</em></li>
</ul>
<div class="fragment">
<p><span class="math display">\[⇒\]</span></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource stan number-lines code-with-copy"><code class="sourceCode stan"><span id="cb3-1"><a></a><span class="kw">model</span> {</span>
<span id="cb3-2"><a></a>  v ~ logit_normal(<span class="fl">0.0</span>, <span class="fl">1.0</span>);</span>
<span id="cb3-3"><a></a>  w ~ logit_normal(<span class="fl">0.0</span>, <span class="fl">1.0</span>);</span>
<span id="cb3-4"><a></a></span>
<span id="cb3-5"><a></a>  <span class="kw">target +=</span> log_mix(v, truncated_normal_lpdf(y | <span class="fl">1.0</span>, u, <span class="fl">0.0</span>, <span class="fl">1.0</span>), truncated_normal_lpdf(y | w, u, <span class="fl">0.0</span>, <span class="fl">1.0</span>));</span>
<span id="cb3-6"><a></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="the-need-for-new-frameworks" class="slide level2">
<h2>The need for new frameworks</h2>
<p>Case studies illustrate four key requirements:</p>
<p><strong>1. Maintain Compositionality</strong></p>
<ul>
<li class="fragment">Derive meanings compositionally</li>
<li class="fragment">Preserve formal semantic insights</li>
<li class="fragment">Can’t abandon compositionality for gradience</li>
</ul>
<div class="fragment">
<p><strong>2. Model Uncertainty Explicitly</strong></p>
<ul>
<li class="fragment">Represent resolved and unresolved uncertainty</li>
<li class="fragment">Show interaction during interpretation</li>
</ul>
</div>
<aside class="notes">
<p>Our case studies illustrate what we need from a framework connecting formal semantics to experimental data.</p>
<p>First, we must maintain compositionality. The meanings of complex expressions must be derived systematically from their parts. We can’t abandon this core principle just because judgments are gradient. Decades of semantic research have shown the power of compositional analysis - we need to preserve these insights.</p>
<p>Second, we need to model uncertainty explicitly. The framework must represent both types we’ve discussed - resolved uncertainty from discrete choices and unresolved uncertainty from inherent gradience. Crucially, it must show how these interact during interpretation.</p>
<p>[Continue to next slide for requirements 3 and 4]</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="framework-requirements" class="slide level2">
<h2>Framework requirements</h2>
<p><strong>3. Make Linking Hypotheses Precise</strong></p>
<ul>
<li class="fragment">Explicit theories: representation → behavior</li>
<li class="fragment">What processes between meaning and slider?</li>
</ul>
<div class="fragment">
<p><strong>4. Enable Quantitative Evaluation</strong></p>
<ul>
<li class="fragment">Testable predictions about distributions</li>
<li class="fragment">Compare theories using standard metrics</li>
</ul>
</div>
<aside class="notes">
<p>[Continuing from previous slide]</p>
<p>Third, we must make linking hypotheses precise. We need explicit theories of how semantic representations produce behavioral responses. What cognitive processes intervene between computing a meaning and moving a slider? Different assumptions lead to different predictions.</p>
<p>Fourth, the framework must enable quantitative evaluation. Theories should make testable predictions about response distributions, not just qualitative patterns. Different theories must be comparable using standard statistical metrics so we can determine which best explains the data.</p>
<p>These requirements are demanding. We need a framework that’s both theoretically principled and empirically adequate. Existing approaches often excel at one or the other. PDS aims to satisfy all four requirements simultaneously.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="moving-forward" class="slide level2">
<h2>Moving forward</h2>
<p>Existing computational approaches (e.g., RSA) bridge formal semantics with probabilistic reasoning <span class="citation" data-cites="frank_predicting_2012 goodman_knowledge_2013">(<a href="#/references" role="doc-biblioref" onclick="">Frank and Goodman 2012</a>; <a href="#/references" role="doc-biblioref" onclick="">Goodman and Stuhlmüller 2013</a>)</span></p>
<div class="fragment">
<h3 id="challenges-with-existing-approaches">Challenges with existing approaches:</h3>
<ul>
<li class="fragment">Difficulty maintaining modularity</li>
<li class="fragment">Often blur semantics/pragmatics distinction</li>
<li class="fragment">Connection to traditional theory somewhat opaque</li>
</ul>
</div>
<aside class="notes">
<p>Before introducing PDS in detail, let’s acknowledge existing computational approaches, particularly Rational Speech Act (RSA) models. These have made important strides in bridging formal semantics with probabilistic reasoning.</p>
<p>RSA models formalize Gricean reasoning - speakers choose utterances to convey information, listeners interpret utterances by reasoning about speaker intentions. This provides a principled way to derive pragmatic inferences from literal meanings.</p>
<p>But RSA faces challenges when scaling to the phenomena we’ve discussed. It’s difficult to maintain modularity - semantic and pragmatic components become intertwined. The distinction between what’s semantic and what’s pragmatic often blurs.</p>
<p>Most importantly, the connection to traditional compositional semantics is somewhat opaque. RSA typically operates on complete utterance meanings rather than building them compositionally. This makes it hard to leverage existing semantic insights.</p>
<p>[Continue to next slide for PDS motivation]</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="motivating-pds" class="slide level2">
<h2>Motivating PDS</h2>
<p>This motivates Probabilistic Dynamic Semantics:</p>
<ul>
<li class="fragment">Preserves semantic insights (the “semantics” part)</li>
<li class="fragment">Adds probabilistic tools for gradient data (the “probabilistic” part)</li>
<li class="fragment">Models experimental tasks as complex discourses (the “dynamic” part)</li>
<li class="fragment">Maintains theoretical commitments while enabling tests</li>
</ul>
<div class="fragment">
<p>PDS provides the framework we need to bridge theory and data</p>
</div>
<aside class="notes">
<p>This brings us to Probabilistic Dynamic Semantics. PDS is designed to address the challenges we’ve identified while building on the successes of both traditional and computational approaches.</p>
<p>PDS preserves the insights of formal semantics - compositionality, precise truth conditions, systematic meaning derivation. But it adds probabilistic tools needed to model gradient behavioral data. Uncertainty is built into the semantic representations themselves, not added post-hoc.</p>
<p>Crucially, PDS maintains theoretical commitments while enabling quantitative tests. You can implement your favorite semantic theory within PDS and test its predictions. Different theories become comparable through their empirical adequacy.</p>
<p>The framework provides exactly what we need - a principled bridge from compositional semantic theory to gradient behavioral data. It’s not a replacement for traditional semantics but an extension that enables new types of investigation.</p>
<p>In the next session, we’ll see how PDS works in detail, starting with its relationship to RSA models.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interim-summary" class="slide level2">
<h2>Interim summary</h2>
<ul>
<li class="fragment">Traditional semantics achieved remarkable success</li>
<li class="fragment">Experimental methods open new opportunities</li>
<li class="fragment">Gradience poses theoretical challenges</li>
<li class="fragment">Need frameworks bridging theory and data</li>
</ul>
<div class="fragment">
<p><strong>Next</strong>: How Rational Speech Act models attempt this bridge, and what PDS can add</p>
</div>
<aside class="notes">
<p>Let’s summarize where we are. Traditional semantics has achieved remarkable success in characterizing compositionality and inference patterns. These insights are too valuable to abandon.</p>
<p>But experimental methods open new opportunities. We can test theories at unprecedented scale, investigate entire lexical domains, and discover patterns invisible to traditional methods. The data is rich but challenging.</p>
<p>The pervasive gradience in this data poses theoretical challenges. We need to understand what produces gradience - discrete choices, continuous parameters, or both? And we need frameworks that can model these sources while maintaining theoretical clarity.</p>
<p>This motivates frameworks that bridge theory and data. We need approaches that preserve semantic insights while engaging with empirical complexity. They must be both principled and practical.</p>
<p>Next, we’ll examine how Rational Speech Act models attempt this bridge. We’ll see their successes and limitations, which will set the stage for understanding how PDS provides a complementary approach that maintains compositionality while adding probabilistic structure.</p>
<p>Thank you! Questions before we move to RSA?</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="a-theoretically-oriented-approach">A theoretically-oriented approach</h3>
<div style="text-align: center;">
<p><span class="huge">R</span> ational <span class="huge">S</span> peech <span class="huge">A</span> ct</p>
<p>Models</p>
</div>
<p><br> <span class="citation" data-cites="frank_predicting_2012 goodman_pragmatic_2016">(<a href="#/references" role="doc-biblioref" onclick="">Frank and Goodman 2012</a>; <a href="#/references" role="doc-biblioref" onclick="">Goodman and Frank 2016</a>, among many others)</span>. <br><br> <span class="fragment">See <span class="citation" data-cites="degen_rational_2023">Degen (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span> for a recent overview of RSA and associated literature.</span></p>
</section>
<section id="two-kinds-of-models" class="slide level2">
<h2>Two kinds of models</h2>
<ul>
<li class="fragment">Listener models</li>
<li class="fragment">Speaker models</li>
</ul>
</section>
<section id="listener-models" class="slide level2">
<h2>Listener models</h2>
<h3 id="l_0"><span class="math inline">\(L_{0}\)</span></h3>
<p><span class="math display">\[
\begin{aligned}
P_{L_0}(w | u) &amp;∝ \begin{cases}
P_{L_0}(w) &amp; ⟦u⟧^w = \mathtt{T} \\
0 &amp; ⟦u⟧^w = \mathtt{F}
\end{cases}
\end{aligned}
\]</span></p>
<ul>
<li class="fragment"><span class="math inline">\(u\)</span> is an utterance</li>
<li class="fragment"><span class="math inline">\(w\)</span> is a meaning (e.g., state of the world to be communicated)</li>
<li class="fragment"><span class="math inline">\(L_{0}\)</span> is just filtering the prior distribution over <span class="math inline">\(w\)</span></li>
</ul>
</section>
<section id="listener-models-1" class="slide level2">
<h2>Listener models</h2>
<h3 id="l_i-i-0"><span class="math inline">\(L_{i} (i &gt; 0)\)</span></h3>
<p><span class="math display">\[
\begin{aligned}
P_{L_i}(w | u) &amp;= \frac{P_{L_i}(u | w) * P_{L_i}(w)}{∑_{w^\prime}P_{L_i}(u | w^\prime) *
P_{L_i}(w^\prime)}
\end{aligned}
\]</span></p>
<div class="fragment">
<p>Bayes’ Theorem</p>
<ul>
<li class="fragment">Posterior probability (given some observation) is proportional to prior probability, multiplied by the likelihood of the observation.</li>
<li class="fragment">Derivable from probability axioms.</li>
</ul>
</div>
</section>
<section id="listener-models-2" class="slide level2">
<h2>Listener models</h2>
<h3 id="l_i-i-0-1"><span class="math inline">\(L_{i} (i &gt; 0)\)</span></h3>
<p><span class="math display">\[
\begin{aligned}
P_{L_i}(w | u) &amp;= \frac{P_{L_i}(u | w) * P_{L_i}(w)}{∑_{w^\prime}P_{L_i}(u | w^\prime) *
P_{L_i}(w^\prime)}
\end{aligned}
\]</span></p>
<ul>
<li class="fragment"><span class="math inline">\(P_{L_{i}}(w) = P(w) \hspace{7cm}\)</span> (prior over meanings)</li>
<li class="fragment"><span class="math inline">\(P_{L_{i}}(u ∣ w)  = P_{S_{i}}(u ∣ w) \hspace{2cm}\)</span> (utterance probability <span class="math inline">\(∣ w\)</span>)</li>
</ul>
</section>
<section id="listener-models-3" class="slide level2">
<h2>Listener models</h2>
<h3 id="l_i-i-0-2"><span class="math inline">\(L_{i} (i &gt; 0)\)</span></h3>
<p><span class="math display">\[
\begin{aligned}
P_{L_i}(w | u) &amp;= \frac{P_{S_{i}}(u | w) * P(w)}{∑_{w^\prime}P_{S_{i}}(u | w^\prime) *
P(w^\prime)}
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(P_{L_{i}}(w) = P(w) \hspace{7cm}\)</span> (prior over meanings)</li>
<li><span class="math inline">\(P_{L_{i}}(u ∣ w)  = P_{S_{i}}(u ∣ w) \hspace{2cm}\)</span> (utterance probability <span class="math inline">\(∣ w\)</span>)</li>
</ul>
</section>
<section id="listener-models-4" class="slide level2">
<h2>Listener models</h2>
<h3 id="l_i-i-0-3"><span class="math inline">\(L_{i} (i &gt; 0)\)</span></h3>
<p><span class="math display">\[
\begin{aligned}
P_{L_i}(w | u) &amp;= \frac{P_{S_{i}}(u | w) * P(w)}{∑_{w^\prime}P_{S_{i}}(u | w^\prime) *
P(w^\prime)}
\end{aligned}
\]</span></p>
<p>Intuition</p>
<ul>
<li class="fragment">The most probable meaning is the one the speaker would’ve most likely chosen that utterance for to get you to infer it.</li>
<li class="fragment"><span class="citation" data-cites="grice_logic_1975">Grice (<a href="#/references" role="doc-biblioref" onclick="">1975</a>)</span></li>
</ul>
</section>
<section class="slide level2">

<h3 id="cookies-example">Cookies example</h3>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/7cookies.jpg" width="500"></p>
<figcaption>Cookies (7 of them)</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li class="fragment"><span class="math inline">\(u = \textit{Jo ate five cookies}\)</span></li>
<li class="fragment"><span class="math inline">\(n_{\textit{cookies}} ≥ 5\)</span> <small>(literal meaning)</small></li>
</ul>
<div class="fragment">
<p>Three components:</p>
<ul>
<li class="fragment">A <span class="gb-orange">literal listener</span> <span class="math inline">\(L_{0}\)</span>: <span class="math inline">\(u ↦ P(w ∣ u)\)</span></li>
<li class="fragment">A <span class="gb-orange">pragmatic speaker</span> <span class="math inline">\(S_{1}\)</span>: <span class="math inline">\(w ↦ P(u ∣ w)\)</span></li>
<li class="fragment">A <span class="gb-orange">pragmatic listener</span> <span class="math inline">\(L_{1}\)</span>: <span class="math inline">\(u ↦ P(w ∣ u)\)</span></li>
</ul>
</div>
</div></div>
</section>
<section class="slide level2">

<h3 id="the-literal-listener-l_0">The literal listener <span class="math inline">\(L_{0}\)</span></h3>
<div style="text-align: center;">
<p><br> <span class="math display">\[
P_{L_{0}}(w ∣ u) ∝ 𝟙(w ≥ n) × P (w)
\]</span></p>
<div class="fragment">
<div id="incremental-row-table">
<table class="caption-top">
<thead>
<tr class="header">
<th><span class="math inline">\(w =\)</span></th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(u = \textit{Jo ate 5 cookies}\)</span></td>
<td>1/3</td>
<td>1/3</td>
<td>1/3</td>
</tr>
<tr class="even">
<td><span class="math inline">\(u = \textit{Jo ate 6 cookies}\)</span></td>
<td>0</td>
<td>1/2</td>
<td>1/2</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(u = \textit{Jo ate 7 cookies}\)</span></td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
</div>
<script>
document.addEventListener('DOMContentLoaded', function () {
  document.querySelectorAll('#incremental-row-table table tbody tr').forEach(tr => {
    tr.classList.add('fragment');
  });
});
</script>
</div>
</section>
<section class="slide level2">

<h3 id="the-pragmatic-speaker-s_0">The pragmatic speaker <span class="math inline">\(S_{0}\)</span></h3>
<div style="text-align: center;">
<p><br> <span class="math display">\[
P_{S_{1}}(u ∣ w) ∝ \frac{P_{L_{0}}(w ∣ u)^{α}}{e^{α × C(u)}}
\]</span></p>
<div class="fragment">
<p>Assume: <span class="math inline">\(α = 4\)</span>, and <span class="math inline">\(C(u)\)</span> is constant.</p>
</div>
<div class="fragment">
<div id="incremental-col-table">
<table class="caption-top">
<thead>
<tr class="header">
<th><span class="math inline">\(w =\)</span></th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(u = \textit{Jo ate 5 cookies}\)</span></td>
<td>1</td>
<td>0.16</td>
<td>0.01</td>
</tr>
<tr class="even">
<td><span class="math inline">\(u = \textit{Jo ate 6 cookies}\)</span></td>
<td>0</td>
<td>0.84</td>
<td>0.06</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(u = \textit{Jo ate 7 cookies}\)</span></td>
<td>0</td>
<td>0</td>
<td>0.93</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<script>
document.addEventListener('DOMContentLoaded', function () {
  const container = document.querySelector('#incremental-col-table');
  if (!container) return;

  const table = container.querySelector('table');
  if (!table) return;

  const rows = table.querySelectorAll('tr');
  const numCols = rows[0].children.length;

  for (let col = 1; col < numCols; col++) {
    const fragIndex = col;
    rows.forEach(row => {
      const cells = row.children;
      if (cells.length > col) {
        const cell = cells[col];
        cell.classList.add('fragment');
        cell.setAttribute('data-fragment-index', fragIndex);
      }
    });
  }
});
</script>
</section>
<section class="slide level2">

<h3 id="the-pragmatic-listener-l_1">The pragmatic listener <span class="math inline">\(L_{1}\)</span></h3>
<div style="text-align: center;">
<p><br> <span class="math display">\[
P_{L_{1}}(w ∣ u) ∝ P_{S_{1}}(u ∣ w) × P (w)
\]</span></p>
<div class="fragment">
<div id="incremental-row-table">
<table class="caption-top">
<thead>
<tr class="header">
<th><span class="math inline">\(w =\)</span></th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(u = \textit{Jo ate 5 cookies}\)</span></td>
<td>0.85</td>
<td>0.14</td>
<td>0.01</td>
</tr>
<tr class="even">
<td><span class="math inline">\(u = \textit{Jo ate 6 cookies}\)</span></td>
<td>0</td>
<td>0.93</td>
<td>0.07</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(u = \textit{Jo ate 7 cookies}\)</span></td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section class="slide level2">

<h3 id="rsa">RSA</h3>
<ul>
<li class="fragment"><strong>Modularity</strong>: aims to distinguish semantic and pragmatic causes of inference behavior:
<ul>
<li class="fragment">literal listener vs.&nbsp;pragmatic listener/speaker</li>
</ul></li>
</ul>
<div class="fragment">
<h3 id="challenges">Challenges</h3>
<ul>
<li class="fragment">Not super clear what role compositionality can play:
<ul>
<li class="fragment">The theory of L0 must come from outside.</li>
<li class="fragment">Models typically operate at the sentence level…
<ul>
<li class="fragment">how do pragmatic effects of individual expressions determine the global pragmatic effect of an utterance?</li>
</ul></li>
</ul></li>
</ul>
</div>
</section>
<section class="slide level2">

<h3 id="setting-the-stage">Setting the stage</h3>
<p>PDS has three important properties:</p>
<ul>
<li class="fragment"><span class="gb-orange">compositionality</span>: models of linguistic datasets are derived compositionally from semantic grammar fragments.</li>
<li class="fragment"><span class="gb-orange">modularity</span>: factors affecting inference judgments may be theorized about independently and combined.</li>
<li class="fragment"><span class="gb-orange">abstraction</span>: models of meaning and inference should be statable <em>abstractly</em>, without reference to implementation.</li>
</ul>
</section>
<section class="slide level2">

<h3 id="compositionality-for-models">Compositionality for models</h3>
<p>What could it mean for models of linguistic datasets to be <em>compositional</em>?</p>
<div class="fragment">
<p>Basic strategy:</p>
<ul>
<li class="fragment">build the distributional assumptions associated with a mixed-effects model <em>into the semantics</em>…
<ul>
<li class="fragment">when basic meanings compose, so do these assumptions.</li>
</ul></li>
</ul>
</div>
</section>
<section class="slide level2">

<h3 id="distributions-in-the-semantics">Distributions in the semantics</h3>
<p>What do they represent?</p>
<ul>
<li class="fragment"><strong>Uncertainty.</strong> <span class="smaller">I.e., about what inferences are licensed.</span></li>
</ul>
<div class="fragment">
<p><span class="smaller">Traditional view of meaning:</span> programs that compute values.</p>
<ul>
<li class="fragment"><span class="math inline">\(⟦\textit{jo laughs}⟧ = ⟦\textit{laughs}⟧ ▹ ⟦\textit{jo}⟧ = laughs(j)\)</span></li>
</ul>
</div>
<div class="fragment">
<p><span class="smaller">Probabilistic view:</span> instead, compute probability distributions.</p>
<ul>
<li class="fragment"><span class="math inline">\(⟦\textit{jo laughs}⟧ = ⟦\textit{laughs}⟧ ▹ ⟦\textit{jo}⟧ =\)</span> <span class="math inline">\({\small\begin{array}[t]{l}
j ∼ JoDistr \\
laugh ∼ LaughDistr \\
Return (laugh(j))
\end{array}}\)</span></li>
</ul>
</div>
</section>
<section class="slide level2">

<h3 id="modularity">Modularity</h3>
<p>Factors affecting inference judgments can be theorized about independently and combined:</p>
<ul>
<li class="fragment">lexical and compositional semantics</li>
<li class="fragment">world knowledge</li>
<li class="fragment">response behavior: how does someone use a testing instrument (e.g., slider scale)?</li>
</ul>
<div class="fragment">
<p>An upshot: PDS can have different uses.</p>
<ul>
<li class="fragment">E.g., swap out a model of response behavior for a model of likely utterances (perhaps, <span class="math inline">\(S_{1}\)</span>).</li>
</ul>
</div>
</section>
<section class="slide level2">

<h3 id="abstraction">Abstraction</h3>
<p>We should be able to state models of inference judgment data abstractly:</p>
<ul>
<li class="fragment"><em>describing</em> probability distributions,</li>
<li class="fragment">not worrying how they are computed.</li>
</ul>
<div class="fragment">
<p>Consequence: separation between theory and model.</p>
<ul>
<li class="fragment">Allows flexibility about implementation.</li>
<li class="fragment">Allows the theory to be simpler.</li>
<li class="fragment">Allows seamless integration between formal semantics and probabilistic semantics. (More tomorrow!)</li>
</ul>
</div>
</section>
<section id="haskell" class="slide level2">
<h2>Haskell</h2>
<ul>
<li class="fragment"><a href="https://learnyouahaskell.com/">Learn you a Haskell</a>: https://learnyouahaskell.com/</li>
</ul>
<div class="fragment">
<p>Things we’ll rely on:</p>
<ul>
<li class="fragment"><p>Algebraic data types</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource haskell number-lines code-with-copy"><code class="sourceCode haskell"><span id="cb4-1"><a></a><span class="kw">data</span> <span class="dt">SomeData</span> <span class="ot">=</span> <span class="dt">FirstThing</span> <span class="op">|</span> <span class="dt">SecondThing</span> <span class="dt">String</span> <span class="op">|</span> <span class="dt">ThirdThing</span> <span class="dt">Double</span> <span class="kw">deriving</span> (<span class="dt">Eq</span>, <span class="dt">Show</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li class="fragment"><p>Maybe and Either</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource haskell number-lines code-with-copy"><code class="sourceCode haskell"><span id="cb5-1"><a></a><span class="kw">data</span> <span class="dt">Maybe</span> a <span class="ot">=</span> <span class="dt">Just</span> a <span class="op">|</span> <span class="dt">Nothing</span> <span class="kw">deriving</span> <span class="op">...</span></span>
<span id="cb5-2"><a></a></span>
<span id="cb5-3"><a></a><span class="kw">data</span> <span class="dt">Either</span> a b <span class="ot">=</span> <span class="dt">Left</span> a <span class="op">|</span> <span class="dt">Right</span> b <span class="kw">deriving</span> <span class="op">...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li class="fragment"><p>Lots of other stuff: we’ll explain as we go.</p></li>
</ul>
</div>
</section>
<section class="slide level2 smaller scrollable" id="references">

<h3 id="references">References</h3>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-abrusan_predicting_2011" class="csl-entry" role="listitem">
Abrusán, Márta. 2011. <span>“Predicting the Presuppositions of Soft Triggers.”</span> <em>Linguistics and Philosophy</em> 34 (6): 491–535.
</div>
<div id="ref-an_lexical_2020" class="csl-entry" role="listitem">
An, Hannah, and Aaron White. 2020. <span>“The Lexical and Grammatical Sources of Neg-Raising Inferences.”</span> <em>Proceedings of the Society for Computation in Linguistics</em> 3 (1): 220–33. https://doi.org/<a href="https://doi.org/10.7275/yts0-q989">https://doi.org/10.7275/yts0-q989</a>.
</div>
<div id="ref-anand_factivity_2014" class="csl-entry" role="listitem">
Anand, Pranav, and Valentine Hacquard. 2014. <span>“Factivity, <span>Belief</span> and <span>Discourse</span>.”</span> In <em>The <span>Art</span> and <span>Craft</span> of <span>Semantics</span>: <span>A</span> <span>Festschrift</span> for <span>Irene</span> <span>Heim</span></em>, edited by Luka Crni\v{c} and Uli Sauerland, 1:69–90. <span>MITWPL</span> 70. MITWPL. <a href="https://semanticsarchive.net/Archive/jZiNmM4N/">https://semanticsarchive.net/Archive/jZiNmM4N/</a>.
</div>
<div id="ref-barker_dynamics_2002" class="csl-entry" role="listitem">
Barker, Chris. 2002. <span>“The <span>Dynamics</span> of <span>Vagueness</span>.”</span> <em>Linguistics and Philosophy</em> 25 (1): 1–36. <a href="https://doi.org/10.1023/A:1014346114955">https://doi.org/10.1023/A:1014346114955</a>.
</div>
<div id="ref-bierwisch_semantics_1989" class="csl-entry" role="listitem">
Bierwisch, Manfred. 1989. <span>“The Semantics of Gradation.”</span> In <em>Dimensional Adjectives</em>, edited by Manfred Bierwisch and Ewald Lang, 71–261. Berlin: Springer-Verlag.
</div>
<div id="ref-bumford_rationalizing_2021" class="csl-entry" role="listitem">
Bumford, Dylan, and Jessica Rett. 2021. <span>“Rationalizing Evaluativity.”</span> <em>Proceedings of Sinn Und Bedeutung</em> 25 (September): 187–204. <a href="https://doi.org/10.18148/sub/2021.v25i0.931">https://doi.org/10.18148/sub/2021.v25i0.931</a>.
</div>
<div id="ref-chomsky_syntactic_1957" class="csl-entry" role="listitem">
Chomsky, Noam. 1957. <em>Syntactic <span>Structures</span></em>. The Hague/Paris: Mouton &amp; Co.
</div>
<div id="ref-chomsky_current_1964" class="csl-entry" role="listitem">
———. 1964. <span>“Current Issues in Linguistic Theory.”</span> In <em>The Structure of Language</em>, edited by J. Fodor and J. Katz, 50–118. New York: Prentice Hall.
</div>
<div id="ref-chomsky_conditions_1973" class="csl-entry" role="listitem">
———. 1973. <span>“Conditions on Transformations.”</span> In <em>A <span>Festschrift</span> for <span>Morris</span> <span>Halle</span></em>, edited by S. Anderson and P. Kiparsky, 232–86. New York: Holt, Rinehart, &amp; Winston.
</div>
<div id="ref-davis_semantics_2004" class="csl-entry" role="listitem">
Davis, Steven, and Brendan S Gillon. 2004. <em>Semantics: A Reader</em>. New York: Oxford University Press.
</div>
<div id="ref-degen_rational_2023" class="csl-entry" role="listitem">
Degen, Judith. 2023. <span>“The <span>Rational</span> <span>Speech</span> <span>Act</span> <span>Framework</span>.”</span> <em>Annual Review of Linguistics</em> 9 (Volume 9, 2023): 519–40. <a href="https://doi.org/10.1146/annurev-linguistics-031220-010811">https://doi.org/10.1146/annurev-linguistics-031220-010811</a>.
</div>
<div id="ref-degen_prior_2021" class="csl-entry" role="listitem">
Degen, Judith, and Judith Tonhauser. 2021. <span>“Prior <span>Beliefs</span> <span>Modulate</span> <span>Projection</span>.”</span> <em>Open Mind</em> 5 (September): 59–70. <a href="https://doi.org/10.1162/opmi_a_00042">https://doi.org/10.1162/opmi_a_00042</a>.
</div>
<div id="ref-degen_are_2022" class="csl-entry" role="listitem">
———. 2022. <span>“Are There Factive Predicates? <span>An</span> Empirical Investigation.”</span> <em>Language</em> 98 (3): 552–91. <a href="https://doi.org/10.1353/lan.0.0271">https://doi.org/10.1353/lan.0.0271</a>.
</div>
<div id="ref-djarv_prosodic_2017" class="csl-entry" role="listitem">
Djärv, Kajsa, and Hezekiah Akiva Bacovcin. 2017. <span>“Prosodic <span>Effects</span> on <span>Factive</span> <span>Presupposition</span> <span>Projection</span>.”</span> <em>Semantics and Linguistic Theory</em> 27 (0): 116–33. <a href="https://doi.org/10.3765/salt.v27i0.4134">https://doi.org/10.3765/salt.v27i0.4134</a>.
</div>
<div id="ref-farkas_reacting_2010" class="csl-entry" role="listitem">
Farkas, Donka F., and Kim B. Bruce. 2010. <span>“On <span>Reacting</span> to <span>Assertions</span> and <span>Polar</span> <span>Questions</span>.”</span> <em>Journal of Semantics</em> 27 (1): 81–118. <a href="https://doi.org/10.1093/jos/ffp010">https://doi.org/10.1093/jos/ffp010</a>.
</div>
<div id="ref-farudi_antisymmetric_2007" class="csl-entry" role="listitem">
Farudi, Annahita. 2007. <span>“An Antisymmetric Approach to <span>Persian</span> Clausal Complements.”</span> <em>Ms., University of Massachusetts, Amherst</em>.
</div>
<div id="ref-fine_vagueness_1975" class="csl-entry" role="listitem">
Fine, Kit. 1975. <span>“Vagueness, <span>Truth</span> and <span>Logic</span>.”</span> <em>Synthese</em> 30 (3/4): 265–300. <a href="https://www.jstor.org/stable/20115033">https://www.jstor.org/stable/20115033</a>.
</div>
<div id="ref-frank_predicting_2012" class="csl-entry" role="listitem">
Frank, Michael C., and Noah D. Goodman. 2012. <span>“Predicting <span>Pragmatic</span> <span>Reasoning</span> in <span>Language</span> <span>Games</span>.”</span> <em>Science</em> 336 (6084): 998–98. <a href="https://doi.org/10.1126/science.1218633">https://doi.org/10.1126/science.1218633</a>.
</div>
<div id="ref-giannakidou_polarity_1998" class="csl-entry" role="listitem">
Giannakidou, Anastasia. 1998. <em>Polarity Sensitivity as (Non) Veridical Dependency</em>. Vol. 23. John Benjamins Publishing.
</div>
<div id="ref-giannakidou_affective_1999" class="csl-entry" role="listitem">
———. 1999. <span>“Affective Dependencies.”</span> <em>Linguistics and Philosophy</em> 22 (4): 367–421.
</div>
<div id="ref-giannakidou_dependency_2009" class="csl-entry" role="listitem">
———. 2009. <span>“The Dependency of the Subjunctive Revisited: <span>Temporal</span> Semantics and Polarity.”</span> <em>Lingua</em> 119 (12): 1883–1908.
</div>
<div id="ref-ginzburg_dynamics_1996" class="csl-entry" role="listitem">
Ginzburg, Jonathan. 1996. <span>“Dynamics and the Semantics of Dialogue.”</span> In <em>Logic, <span>Language</span>, and <span>Computation</span></em>, edited by Jerry Seligman and Dag Westerståhl, 1:221–37. Stanford: CSLI Publications.
</div>
<div id="ref-goodman_pragmatic_2016" class="csl-entry" role="listitem">
Goodman, Noah D., and Michael C. Frank. 2016. <span>“Pragmatic <span>Language</span> <span>Interpretation</span> as <span>Probabilistic</span> <span>Inference</span>.”</span> <em>Trends in Cognitive Sciences</em> 20 (11): 818–29. <a href="https://doi.org/10.1016/j.tics.2016.08.005">https://doi.org/10.1016/j.tics.2016.08.005</a>.
</div>
<div id="ref-goodman_knowledge_2013" class="csl-entry" role="listitem">
Goodman, Noah D., and Andreas Stuhlmüller. 2013. <span>“Knowledge and Implicature: Modeling Language Understanding as Social Cognition.”</span> <em>Topics in Cognitive Science</em> 5 (1): 173–84. <a href="https://doi.org/10.1111/tops.12007">https://doi.org/10.1111/tops.12007</a>.
</div>
<div id="ref-graff_shifting_2000" class="csl-entry" role="listitem">
Graff, Delia. 2000. <span>“Shifting <span>Sands</span>: <span>An</span> <span>Interest</span>-<span>Relative</span> <span>Theory</span> of <span>Vagueness</span>.”</span> <em>Philosophical Topics</em> 28 (1): 45–81. <a href="https://www.jstor.org/stable/43154331">https://www.jstor.org/stable/43154331</a>.
</div>
<div id="ref-grice_logic_1975" class="csl-entry" role="listitem">
Grice, H. Paul. 1975. <span>“Logic and <span>Conversation</span>.”</span> In <em>Syntax and <span>Semantics</span></em>, edited by Peter Cole and Jerry L. Morgan, 3, Speech Acts:41–58. New York: Academic Press.
</div>
<div id="ref-groenendijk_dynamic_1991" class="csl-entry" role="listitem">
Groenendijk, Jeroen A., and Martin B. J. Stokhof. 1991. <span>“Dynamic <span>Predicate</span> <span>Logic</span>.”</span> <em>Linguistics and Philosophy</em> 14 (1): 39–100. <a href="https://doi.org/10.1007/BF00628304">https://doi.org/10.1007/BF00628304</a>.
</div>
<div id="ref-jasbi_linking_2019" class="csl-entry" role="listitem">
Jasbi, Masoud, Brandon Waldon, and Judith Degen. 2019. <span>“Linking <span>Hypothesis</span> and <span>Number</span> of <span>Response</span> <span>Options</span> <span>Modulate</span> <span>Inferred</span> <span>Scalar</span> <span>Implicature</span> <span>Rate</span>.”</span> <em>Frontiers in Psychology</em> 10 (February). <a href="https://doi.org/10.3389/fpsyg.2019.00189">https://doi.org/10.3389/fpsyg.2019.00189</a>.
</div>
<div id="ref-jeong_prosodically-conditioned_2021" class="csl-entry" role="listitem">
Jeong, Sunwoo. 2021. <span>“Prosodically-Conditioned Factive Inferences in <span>Korean</span>: <span>An</span> Experimental Study.”</span> <em>Semantics and Linguistic Theory</em> 30 (0): 1–21. <a href="https://doi.org/10.3765/salt.v30i0.4798">https://doi.org/10.3765/salt.v30i0.4798</a>.
</div>
<div id="ref-kamp_two_1975" class="csl-entry" role="listitem">
Kamp, J. A. W. 1975. <span>“Two Theories about Adjectives.”</span> In <em>Formal <span>Semantics</span> of <span>Natural</span> <span>Language</span></em>, edited by Edward L. Keenan, 123–55. Cambridge: Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511897696.011">https://doi.org/10.1017/CBO9780511897696.011</a>.
</div>
<div id="ref-kane_intensional_2022" class="csl-entry" role="listitem">
Kane, Benjamin, Will Gantt, and Aaron Steven White. 2022. <span>“Intensional <span>Gaps</span>: <span>Relating</span> Veridicality, Factivity, Doxasticity, Bouleticity, and Neg-Raising.”</span> <em>Semantics and Linguistic Theory</em> 31 (0): 570–605. <a href="https://doi.org/10.3765/salt.v31i0.5137">https://doi.org/10.3765/salt.v31i0.5137</a>.
</div>
<div id="ref-kao_nonliteral_2014" class="csl-entry" role="listitem">
Kao, Justine T., Jean Y. Wu, Leon Bergen, and Noah D. Goodman. 2014. <span>“Nonliteral Understanding of Number Words.”</span> <em>Proceedings of the National Academy of Sciences</em> 111 (33): 12002–7. <a href="https://doi.org/10.1073/pnas.1407479111">https://doi.org/10.1073/pnas.1407479111</a>.
</div>
<div id="ref-karttunen_observations_1971" class="csl-entry" role="listitem">
Karttunen, Lauri. 1971. <span>“Some Observations on Factivity.”</span> <em>Paper in Linguistics</em> 4 (1): 55–69. <a href="https://doi.org/10.1080/08351817109370248">https://doi.org/10.1080/08351817109370248</a>.
</div>
<div id="ref-kastner_factivity_2015" class="csl-entry" role="listitem">
Kastner, Itamar. 2015. <span>“Factivity Mirrors Interpretation: <span>The</span> Selectional Requirements of Presuppositional Verbs.”</span> <em>Lingua</em> 164: 156–88.
</div>
<div id="ref-kennedy_projecting_1999" class="csl-entry" role="listitem">
Kennedy, Chris. 1999. <em>Projecting the Adjective: The Syntax and Semantics of Gradability and Comparison</em>. New York: Garland.
</div>
<div id="ref-kennedy_vagueness_2007" class="csl-entry" role="listitem">
Kennedy, Christopher. 2007. <span>“Vagueness and Grammar: The Semantics of Relative and Absolute Gradable Adjectives.”</span> <em>Linguistics and Philosophy</em> 30 (1): 1–45. <a href="https://doi.org/10.1007/s10988-006-9008-0">https://doi.org/10.1007/s10988-006-9008-0</a>.
</div>
<div id="ref-kennedy_scale_2005" class="csl-entry" role="listitem">
Kennedy, Christopher, and Louise McNally. 2005. <span>“Scale <span>Structure</span>, <span>Degree</span> <span>Modification</span>, and the <span>Semantics</span> of <span>Gradable</span> <span>Predicates</span>.”</span> <em>Language</em> 81 (2): 345–81. <a href="https://doi.org/10.1353/lan.2005.0071">https://doi.org/10.1353/lan.2005.0071</a>.
</div>
<div id="ref-kiparsky_fact_1970" class="csl-entry" role="listitem">
Kiparsky, Paul, and Carol Kiparsky. 1970. <span>“<span>FACT</span>.”</span> In <em>Progress in <span>Linguistics</span></em>, 143–73. De Gruyter Mouton. <a href="https://doi.org/10.1515/9783111350219.143">https://doi.org/10.1515/9783111350219.143</a>.
</div>
<div id="ref-klein_semantics_1980" class="csl-entry" role="listitem">
Klein, Ewan. 1980. <span>“A Semantics for Positive and Comparative Adjectives.”</span> <em>Linguistics and Philosophy</em> 4 (1): 1–45. <a href="https://doi.org/10.1007/BF00351812">https://doi.org/10.1007/BF00351812</a>.
</div>
<div id="ref-krifka_approximate_2007" class="csl-entry" role="listitem">
Krifka, Manfred. 2007. <span>“Approximate Interpretation of Number Words: <span>A</span> Case for Strategic Communication.”</span> In <em>Cognitive <span>Foundations</span> of <span>Interpretation</span></em>, edited by Gerlof Bouma, Irene Krämer, and Joost Zwartz, 111–26. Amsterdam: Koninklijke Nederlandse Akademie van Wetenschapen. <a href="https://doi.org/10.18452/9508">https://doi.org/10.18452/9508</a>.
</div>
<div id="ref-lakoff_hedges_1973" class="csl-entry" role="listitem">
Lakoff, George. 1973. <span>“Hedges: <span>A</span> Study in Meaning Criteria and the Logic of Fuzzy Concepts.”</span> <em>Journal of Philosophical Logic</em> 2 (4): 458–508. <a href="https://doi.org/10.1007/BF00262952">https://doi.org/10.1007/BF00262952</a>.
</div>
<div id="ref-lasersohn_pragmatic_1999" class="csl-entry" role="listitem">
Lasersohn, Peter. 1999. <span>“Pragmatic <span>Halos</span>.”</span> <em>Language</em> 75 (3): 522–51. <a href="https://doi.org/10.2307/417059">https://doi.org/10.2307/417059</a>.
</div>
<div id="ref-lassiter_context_2013" class="csl-entry" role="listitem">
Lassiter, Daniel, and Noah D. Goodman. 2013. <span>“Context, Scale Structure, and Statistics in the Interpretation of Positive-Form Adjectives.”</span> <em>Semantics and Linguistic Theory</em> 23 (0): 587–610. <a href="https://doi.org/10.3765/salt.v23i0.2658">https://doi.org/10.3765/salt.v23i0.2658</a>.
</div>
<div id="ref-lassiter_adjectival_2017" class="csl-entry" role="listitem">
———. 2017. <span>“Adjectival Vagueness in a <span>Bayesian</span> Model of Interpretation.”</span> <em>Synthese</em> 194 (10): 3801–36. <a href="https://doi.org/10.1007/s11229-015-0786-1">https://doi.org/10.1007/s11229-015-0786-1</a>.
</div>
<div id="ref-moon_source_2020" class="csl-entry" role="listitem">
Moon, Ellise, and Aaron White. 2020. <span>“The Source of Nonfinite Temporal Interpretation.”</span> In <em>Proceedings of the 50th <span>Annual</span> <span>Meeting</span> of the <span>North</span> <span>East</span> <span>Linguistic</span> <span>Society</span></em>, edited by Mariam Asatryan, Yixiao Song, and Ayana Whitmal, 3:11–24. Amherst: GLSA Publications.
</div>
<div id="ref-muskens_combining_1996" class="csl-entry" role="listitem">
Muskens, Reinhard. 1996. <span>“Combining <span>Montague</span> Semantics and Discourse Representation.”</span> <em>Linguistics and Philosophy</em> 19 (2): 143–86. <a href="https://doi.org/10.1007/BF00635836">https://doi.org/10.1007/BF00635836</a>.
</div>
<div id="ref-ozyildiz_attitude_2017" class="csl-entry" role="listitem">
Ozyildiz, Deniz. 2017. <span>“Attitude Reports with and Without True Belief.”</span> In <em>Semantics and <span>Linguistic</span> <span>Theory</span></em>, edited by Dan Burgdorf, Jacob Collard, Sireemas Maspong, and Brynhildur Stefánsdóttir, 27:397–417. Linguistic Society of America.
</div>
<div id="ref-phillips_theories_2021" class="csl-entry" role="listitem">
Phillips, Colin, Phoebe Gaston, Nick Huang, and Hanna Muller. 2021. <span>“Theories <span>All</span> the <span>Way</span> <span>Down</span>: <span>Remarks</span> on <span>‘<span>Theoretical</span>’</span> and <span>‘<span>Experimental</span>’</span> <span>Linguistics</span>.”</span> In <em>The <span>Cambridge</span> <span>Handbook</span> of <span>Experimental</span> <span>Syntax</span></em>, edited by Grant Goodall, 587–616. Cambridge <span>Handbooks</span> in <span>Language</span> and <span>Linguistics</span>. Cambridge: Cambridge University Press. <a href="https://doi.org/10.1017/9781108569620.023">https://doi.org/10.1017/9781108569620.023</a>.
</div>
<div id="ref-qing_gradable_2014" class="csl-entry" role="listitem">
Qing, Ciyang, and Michael Franke. 2014. <span>“Gradable Adjectives, Vagueness, and Optimal Language Use: <span>A</span> Speaker-Oriented Model.”</span> <em>Semantics and Linguistic Theory</em>, August, 23–41. <a href="https://doi.org/10.3765/salt.v24i0.2412">https://doi.org/10.3765/salt.v24i0.2412</a>.
</div>
<div id="ref-qing_rational_2016" class="csl-entry" role="listitem">
Qing, Ciyang, Noah D. Goodman, and Daniel Lassiter. 2016. <span>“A Rational Speech-Act Model of Projective Content.”</span> In <em>Proceedings of the 38th <span>Annual</span> <span>Meeting</span> of the <span>Cognitive</span> <span>Science</span> <span>Society</span>: <span>Recognising</span> and Representing Events</em>, 1110–15. The Cognitive Science Society. <a href="https://www.research.ed.ac.uk/en/publications/a-rational-speech-act-model-of-projective-content">https://www.research.ed.ac.uk/en/publications/a-rational-speech-act-model-of-projective-content</a>.
</div>
<div id="ref-roberts_information_2012" class="csl-entry" role="listitem">
Roberts, Craige. 2012. <span>“Information <span>Structure</span>: <span>Towards</span> an Integrated Formal Theory of Pragmatics.”</span> <em>Semantics and Pragmatics</em> 5 (December): 6:1–69. <a href="https://doi.org/10.3765/sp.5.6">https://doi.org/10.3765/sp.5.6</a>.
</div>
<div id="ref-roberts_preconditions_2024" class="csl-entry" role="listitem">
Roberts, Craige, and Mandy Simons. 2024. <span>“Preconditions and Projection: <span>Explaining</span> Non-Anaphoric Presupposition.”</span> <em>Linguistics and Philosophy</em> 47 (4): 703–48. <a href="https://doi.org/10.1007/s10988-024-09413-9">https://doi.org/10.1007/s10988-024-09413-9</a>.
</div>
<div id="ref-van_rooij_vagueness_2011" class="csl-entry" role="listitem">
Rooij, Robert van. 2011. <span>“Vagueness and <span>Linguistics</span>.”</span> In <em>Vagueness: <span>A</span> <span>Guide</span></em>, edited by Giuseppina Ronzitti, 123–70. Dordrecht: Springer Netherlands. <a href="https://doi.org/10.1007/978-94-007-0375-9_6">https://doi.org/10.1007/978-94-007-0375-9_6</a>.
</div>
<div id="ref-ross_constraints_1967" class="csl-entry" role="listitem">
Ross, John Robert. 1967. <span>“Constraints on Variables in Syntax.”</span> PhD thesis, Massachusetts Institute of Technology.
</div>
<div id="ref-roussou_selecting_2010" class="csl-entry" role="listitem">
Roussou, Anna. 2010. <span>“Selecting Complementizers.”</span> <em>Lingua</em> 120 (3): 582–603.
</div>
<div id="ref-sadock_truth_1977" class="csl-entry" role="listitem">
Sadock, Jerrold M. 1977. <span>“Truth and <span>Approximations</span>.”</span> <em>Annual Meeting of the Berkeley Linguistics Society</em>, September, 430–39. <a href="https://doi.org/10.3765/bls.v3i0.2268">https://doi.org/10.3765/bls.v3i0.2268</a>.
</div>
<div id="ref-schütze_gramaticality_2016" class="csl-entry" role="listitem">
Schütze, Carson T. 2016. <em>The Empirical Base of Linguistics</em>. Classics in Linguistics 2. Berlin: Language Science Press. <a href="https://doi.org/10.17169/langsci.b89.100">https://doi.org/10.17169/langsci.b89.100</a>.
</div>
<div id="ref-simons_observations_2007" class="csl-entry" role="listitem">
Simons, Mandy. 2007. <span>“Observations on Embedding Verbs, Evidentiality, and Presupposition.”</span> <em>Lingua</em> 117 (6): 1034–56. <a href="https://doi.org/10.1016/j.lingua.2006.05.006">https://doi.org/10.1016/j.lingua.2006.05.006</a>.
</div>
<div id="ref-simons_best_2017" class="csl-entry" role="listitem">
Simons, Mandy, David Beaver, Craige Roberts, and Judith Tonhauser. 2017. <span>“The <span>Best</span> <span>Question</span>: <span>Explaining</span> the <span>Projection</span> <span>Behavior</span> of <span>Factives</span>.”</span> <em>Discourse Processes</em> 54 (3): 187–206.
</div>
<div id="ref-simons_what_2010" class="csl-entry" role="listitem">
Simons, Mandy, Judith Tonhauser, David Beaver, and Craige Roberts. 2010. <span>“What Projects and Why.”</span> In <em>Semantics and <span>Linguistic</span> <span>Theory</span></em>, edited by Nan Li and David Lutz, 20:309–27. University of British Columbia; Simon Fraser University: Linguistic Society of America. <a href="https://doi.org/10.3765/salt.v20i0.2584">https://doi.org/10.3765/salt.v20i0.2584</a>.
</div>
<div id="ref-solt_vagueness_2015" class="csl-entry" role="listitem">
Solt, Stephanie. 2015. <span>“Vagueness and <span>Imprecision</span>: <span>Empirical</span> <span>Foundations</span>.”</span> <em>Annual Review of Linguistics</em> 1 (Volume 1, 2015): 107–27. <a href="https://doi.org/10.1146/annurev-linguist-030514-125150">https://doi.org/10.1146/annurev-linguist-030514-125150</a>.
</div>
<div id="ref-sorensen_vagueness_2023" class="csl-entry" role="listitem">
Sorensen, Roy. 2023. <span>“Vagueness.”</span> In <em>The <span>Stanford</span> <span>Encyclopedia</span> of <span>Philosophy</span></em>, edited by Edward N. Zalta and Uri Nodelman, Winter 2023. Metaphysics Research Lab, Stanford University. <a href="https://plato.stanford.edu/archives/win2023/entries/vagueness/">https://plato.stanford.edu/archives/win2023/entries/vagueness/</a>.
</div>
<div id="ref-sprouse_island_2021" class="csl-entry" role="listitem">
Sprouse, Jon, and Sandra Villata. 2021. <span>“Island Effects.”</span> In <em>The Cambridge Handbook of Experimental Syntax</em>, edited by Grant Goodall, 227–57. Cambridge Handbooks in Language and Linguistics. Cambridge University Press. <a href="https://doi.org/10.1017/9781108569620.010">https://doi.org/10.1017/9781108569620.010</a>.
</div>
<div id="ref-stabler_derivational_1997" class="csl-entry" role="listitem">
Stabler, Edward. 1997. <span>“Derivational Minimalism.”</span> In <em>Logical <span>Aspects</span> of <span>Computational</span> <span>Linguistics</span></em>, edited by Christian Retoré, 68–95. Lecture <span>Notes</span> in <span>Computer</span> <span>Science</span>. Berlin, Heidelberg: Springer. <a href="https://doi.org/10.1007/BFb0052152">https://doi.org/10.1007/BFb0052152</a>.
</div>
<div id="ref-stalnaker_assertion_1978" class="csl-entry" role="listitem">
Stalnaker, Robert. 1978. <span>“Assertion.”</span> In <em>Pragmatics</em>, edited by Peter Cole, 9:315–32. New York: Academic Press.
</div>
<div id="ref-tonhauser_prosodic_2016" class="csl-entry" role="listitem">
Tonhauser, Judith. 2016. <span>“Prosodic Cues to Presupposition Projection.”</span> <em>Semantics and Linguistic Theory</em> 26 (0): 934–60. <a href="https://doi.org/10.3765/salt.v26i0.3788">https://doi.org/10.3765/salt.v26i0.3788</a>.
</div>
<div id="ref-tonhauser_how_2018" class="csl-entry" role="listitem">
Tonhauser, Judith, David I. Beaver, and Judith Degen. 2018. <span>“How <span>Projective</span> Is <span>Projective</span> <span>Content</span>? <span>Gradience</span> in <span>Projectivity</span> and <span>At</span>-Issueness.”</span> <em>Journal of Semantics</em> 35 (3): 495–542. <a href="https://doi.org/10.1093/jos/ffy007">https://doi.org/10.1093/jos/ffy007</a>.
</div>
<div id="ref-varlokosta_issues_1994" class="csl-entry" role="listitem">
Varlokosta, Spyridoula. 1994. <span>“Issues in <span>Modern</span> <span>Greek</span> <span>Sentential</span> <span>Complementation</span>.”</span> PhD thesis, University of Maryland, College Park.
</div>
<div id="ref-waldon_modeling_2020" class="csl-entry" role="listitem">
Waldon, Brandon, and Judith Degen. 2020. <span>“Modeling <span>Behavior</span> in <span>Truth</span> <span>Value</span> <span>Judgment</span> <span>Task</span> <span>Experiments</span>.”</span> In <em>Proceedings of the <span>Society</span> for <span>Computation</span> in <span>Linguistics</span> 2020</em>, edited by Allyson Ettinger, Gaja Jarosz, and Joe Pater, 238–47. New York, New York: Association for Computational Linguistics. <a href="https://aclanthology.org/2020.scil-1.29/">https://aclanthology.org/2020.scil-1.29/</a>.
</div>
<div id="ref-white_lexically_2019" class="csl-entry" role="listitem">
White, Aaron Steven. 2019. <span>“Lexically Triggered Veridicality Inferences.”</span> In <em>Handbook of <span>Pragmatics</span></em>, 22:115–48. John Benjamins Publishing Company. <a href="https://doi.org/10.1075/hop.22.lex4">https://doi.org/10.1075/hop.22.lex4</a>.
</div>
<div id="ref-white_computational_2016" class="csl-entry" role="listitem">
White, Aaron Steven, and Kyle Rawlins. 2016. <span>“A Computational Model of <span>S</span>-Selection.”</span> <em>Semantics and Linguistic Theory</em> 26 (0): 641–63. <a href="https://doi.org/10.3765/salt.v26i0.3819">https://doi.org/10.3765/salt.v26i0.3819</a>.
</div>
<div id="ref-white_role_2018" class="csl-entry" role="listitem">
———. 2018. <span>“The Role of Veridicality and Factivity in Clause Selection.”</span> In <em><span>NELS</span> 48: <span>Proceedings</span> of the <span>Forty</span>-<span>Eighth</span> <span>Annual</span> <span>Meeting</span> of the <span>North</span> <span>East</span> <span>Linguistic</span> <span>Society</span></em>, edited by Sherry Hucklebridge and Max Nelson, 48:221–34. University of Iceland: GLSA (Graduate Linguistics Student Association), Department of Linguistics, University of Massachusetts.
</div>
<div id="ref-white_frequency_2020" class="csl-entry" role="listitem">
———. 2020. <span>“Frequency, Acceptability, and Selection: <span>A</span> Case Study of Clause-Embedding.”</span> <em>Glossa: A Journal of General Linguistics</em> 5 (1). <a href="https://doi.org/10.5334/gjgl.1001">https://doi.org/10.5334/gjgl.1001</a>.
</div>
<div id="ref-white_lexicosyntactic_2018" class="csl-entry" role="listitem">
White, Aaron Steven, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme. 2018. <span>“Lexicosyntactic <span>Inference</span> in <span>Neural</span> <span>Models</span>.”</span> In <em>Proceedings of the 2018 <span>Conference</span> on <span>Empirical</span> <span>Methods</span> in <span>Natural</span> <span>Language</span> <span>Processing</span></em>, 4717–24. Brussels, Belgium: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D18-1501">https://doi.org/10.18653/v1/D18-1501</a>.
</div>
</div>
</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p>Probabilistic dynamic semantics</p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>