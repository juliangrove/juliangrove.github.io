[
  {
    "objectID": "qud/index.html",
    "href": "qud/index.html",
    "title": "Notes",
    "section": "",
    "text": "Note\n\n\n\nThese notes will become available on June 27."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site contains materials for a course on Probabilistic dynamic semantics given by Julian Grove and Aaron Steven White at ESSLLI 2025, held at the Ruhr Universtät Bochum from August 4–8, 2025.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#about-the-instructors",
    "href": "about.html#about-the-instructors",
    "title": "About",
    "section": "About the instructors",
    "text": "About the instructors\nJulian Grove is a postdoctoral researcher in Linguistics at the University of Rochester and a member of the Formal and Computational Semantics lab (FACTS.lab). He is interested in combining computational tools from Bayesian data analysis and programming language theory and bringing them to semantics. He will be an Assistant Professor of Semantics and Computational Linguistics at the University of Florida starting in August, 2025.\nAaron Steven White is an Associate Professor of Linguistics and Computer Science at the University of Rochester, where he directs the Center for Language Sciences and the Formal and Computational Semantics lab (FACTS.lab). His research investigates the relationship between linguistic expressions and conceptual categories that undergird the human ability to convey information about possible past, present, and future configurations of things in the world.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#about-the-site",
    "href": "about.html#about-the-site",
    "title": "About",
    "section": "About the site",
    "text": "About the site\nThe site itself is built using Quarto. The source files for this site are available on github at juliangrove/pds-2025.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#acknowledgments",
    "href": "about.html#acknowledgments",
    "title": "About",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThe materials for Module 3 were developed partly in collaboration with Helena Aparicio.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#license",
    "href": "about.html#license",
    "title": "About",
    "section": "License ",
    "text": "License \nProbabilistic dynamic semantics by Julian Grove and Aaron Steven White is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. Based on work at https://github.com/juliangrove/pds-2025.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "background/understanding-gradience.html",
    "href": "background/understanding-gradience.html",
    "title": "Understanding gradience",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\updct}[1]{\\ct{upd\\_#1}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\newcommand{\\pure}[1]{\\bbox[border: 1px solid orange]{\\bbox[border: 4px solid transparent]{#1}}}\n\\newcommand{\\return}[1]{\\bbox[border: 1px solid black]{\\bbox[border: 4px solid transparent]{#1}}}\n\\def\\P{\\mathtt{P}}\n\\def\\Q{\\mathtt{Q}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\def\\ite{\\ct{if\\_then\\_else}}\n\\def\\Do{\\abbr{do}}\n\\]\nOne of the most striking findings from experimental semantics and pragmatics is the pervasiveness of gradience in aggregated measures.1 While semanticists have long recognized the existence of gradience in some domains–e.g. gradable adjectives–we often assume categorical distinctions in other domains–e.g. factivity. And even where traditional approaches assume categorical distinctions, experimental methods often reveal continuous variation. For the reasons laid out above, understanding this gradience is crucial for developing theories that connect formal semantics to behavioral data.",
    "crumbs": [
      "Background",
      "Understanding gradience"
    ]
  },
  {
    "objectID": "background/understanding-gradience.html#footnotes",
    "href": "background/understanding-gradience.html#footnotes",
    "title": "Understanding gradience",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this course, we will focus mainly on gradience in aggregated inference judgments, but there is a deep literature on gradience in acceptability judgments within the experimental syntax literature (Bard, Robertson, and Sorace 1996; Keller 2000; Sorace and Keller 2005; Sprouse 2007, 2011; Featherston 2005, 2007; Gibson and Fedorenko 2010, 2013; Sprouse and Almeida 2013; Sprouse, Schütze, and Almeida 2013; Schütze and Sprouse 2014; Lau, Clark, and Lappin 2017; Sprouse et al. 2018).↩︎",
    "crumbs": [
      "Background",
      "Understanding gradience"
    ]
  },
  {
    "objectID": "background/theory-to-data.html",
    "href": "background/theory-to-data.html",
    "title": "From theory to data",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\updct}[1]{\\ct{upd\\_#1}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\newcommand{\\pure}[1]{\\bbox[border: 1px solid orange]{\\bbox[border: 4px solid transparent]{#1}}}\n\\newcommand{\\return}[1]{\\bbox[border: 1px solid black]{\\bbox[border: 4px solid transparent]{#1}}}\n\\def\\P{\\mathtt{P}}\n\\def\\Q{\\mathtt{Q}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\def\\ite{\\ct{if\\_then\\_else}}\n\\def\\Do{\\abbr{do}}\n\\]\nSemantic theory has achieved remarkable success in characterizing the compositional structure of natural language meaning. Through decades of careful theoretical work, semanticists have developed elegant formal systems that capture how complex meanings arise from the systematic combination of simpler parts. These theories explain two fundamental types of judgments that speakers make: acceptability judgments about whether strings are well-formed, and inference judgments about what follows from what speakers say.\nThe field now stands at an exciting juncture. The rise of large-scale experimental methods and computational modeling opens new opportunities to test and refine these theoretical insights against rich behavioral data. The challenge—and opportunity—is to connect our elegant formal theories to the messy, gradient patterns we observe when hundreds of speakers make thousands of judgments. How can we maintain the theoretical insights that formal semantics has achieved while extending them to account for this new empirical richness?\nProbabilistic Dynamic Semantics (PDS) aims to provide a systematic bridge between these theoretical insights and behavioral data. It takes the compositional analyses developed using traditional Montagovian methods and maps them to probabilistic models that can be quantitatively evaluated against experimental results. The goal is not to replace traditional semantics but to extend its reach, allowing us to test theoretical predictions at unprecedented scale while maintaining formal rigor.",
    "crumbs": [
      "Background",
      "From theory to data"
    ]
  },
  {
    "objectID": "background/theory-to-data.html#traditional-semantic-methodology-foundations-of-success",
    "href": "background/theory-to-data.html#traditional-semantic-methodology-foundations-of-success",
    "title": "From theory to data",
    "section": "Traditional Semantic Methodology: Foundations of Success",
    "text": "Traditional Semantic Methodology: Foundations of Success\nSemanticists study the systematic relationships between linguistic expressions and the inferences they support. The field’s methodology centers on two types of judgments:\nAcceptability judgments assess whether strings are well-formed relative to a language and in a particular context of use (Chomsky 1957; see Schütze 2016).   For example, in a context where a host asks what a guest wants with coffee, (1) is clearly acceptable, while (2) is not Sprouse and Villata (2021):\n\nWhat would you like with your coffee?\n#What would you like and your coffee?\n\nInference judgments assess relationships between strings (see Davis and Gillon 2004).   When speakers hear (3), they typically infer (4) (White 2019):\n\nJo loved that Mo left.\nMo left.\n\n\nObservational Adequacy\nA core desideratum for semantic theories is observational adequacy (Chomsky 1964): for any string \\(s \\in \\Sigma^*\\), we should predict how acceptable speakers find it in context, and for acceptable strings \\(s, s'\\), we should predict whether speakers judge \\(s'\\) inferable from \\(s\\). Achieving observational adequacy requires mapping vocabulary elements to abstractions that predict judgments parsimoniously.\nThese abstractions may be discrete or continuous, simple or richly structured. Through careful analysis of consistent inference patterns, semanticists have identified powerful generalizations. For instance, examining predicates like love, hate, be surprised, and know, theorists observed they all give rise to inferences about their complement clauses that survive under negation and questioning. This led to positing that they all share a property that predicts systematic inferential behavior across diverse predicates (Kiparsky and Kiparsky 1970; cf. Karttunen 1971).\n\n\nDescriptive Adequacy and Theoretical Depth\nBeyond observational adequacy lies descriptive adequacy: capturing data “in terms of significant generalizations that express underlying regularities in the language” (Chomsky 1964, 63). This drive for deeper explanation motivates the field’s emphasis on parsimony and formal precision.\nThe history of generative syntax illustrates two approaches to achieving descriptive adequacy:\n\nAnalysis-driven: Start with observationally adequate analyses in expressive formalisms, then extract generalizations as constraints\nHypothesis-driven: Begin with constrained formalisms (like CCG or minimalist grammars) and test their empirical coverage\n\nThe hypothesis-driven approach, which PDS adopts for semantics, aims to delineate phenomena through representational constraints. This becomes crucial when developing models that both accord with theoretical assumptions and can be evaluated quantitatively (Baroni 2022; Pavlick 2023).\n\n\nThe Power and Natural Boundaries of Traditional Methods\nThis methodology has yielded profound insights into semantic composition, scope phenomena, discourse dynamics, and the semantics-pragmatics interface more generally. By focusing on carefully constructed examples and native speaker intuitions, theorists have uncovered deep regularities in how meaning is constructed and interpreted.\nYet every methodology has natural boundaries. Traditional semantic methods excel at identifying patterns and building theories but face practical constraints when we ask:\n\nHow well do our generalizations, based on examining 5-10 predicates, extend to the thousands of predicates in the lexicon?\nWhat factors beyond semantic knowledge influence the judgments we observe?\nHow exactly does abstract semantic knowledge produce concrete behavioral responses?",
    "crumbs": [
      "Background",
      "From theory to data"
    ]
  },
  {
    "objectID": "background/theory-to-data.html#the-experimental-turn-new-opportunities-for-semantic-theory",
    "href": "background/theory-to-data.html#the-experimental-turn-new-opportunities-for-semantic-theory",
    "title": "From theory to data",
    "section": "The Experimental Turn: New Opportunities for Semantic Theory",
    "text": "The Experimental Turn: New Opportunities for Semantic Theory\nThe traditional methodology’s success has created a foundation solid enough to support exciting new extensions. Experimental semantics brings the tools of behavioral experimentation to bear on questions about meaning, allowing us to test and refine theoretical insights at unprecedented scale.\n\nScaling Semantic Investigation\nWhere traditional methods might examine a handful of predicates, experimental approaches can investigate entire lexical domains. Extending our example involving the verb love: English has thousands of similar clause-embedding predicates, each potentially varying in its inferential properties. We can now test whether generalizations based on canonical examples extend across these vast lexicons.\nThe MegaAttitude project (White and Rawlins 2016, 2018, 2020; White et al. 2018; An and White 2020; Moon and White 2020; Kane, Gantt, and White 2022) is one example of this approach. This project aims to collect inference judgments for hundreds of predicates across multiple contexts and inference types. This scale reveals patterns that are very difficult to see and evaluate the quality of using traditional methods—subtle distinctions between near-synonyms, unexpected predicate clusters, and systematic variation across semantic domains.\n\n\nTeasing Apart Contributing Factors\nExperimental methods also allow us to investigate the rich array of factors that influence inference judgments:\n\nSemantic knowledge: The core meanings of expressions\nWorld knowledge: Prior beliefs about plausibility\n\nContextual factors: The discourse context and QUD\nIndividual differences: Variation in how speakers interpret expressions\nResponse strategies: How participants use rating scales\n\nRather than viewing these as confounds, we can see them as windows into the cognitive processes underlying semantic interpretation. For instance, Degen and Tonhauser (2021) systematically manipulated world knowledge to show how prior beliefs modulate the strength of factive inferences, revealing the interplay between semantic and pragmatic factors.\n\n\nMaking Linking Hypotheses Explicit\nPerhaps most importantly, experimental approaches force us to make explicit what traditional methods leave implicit: the link between semantic representations and behavioral responses (Jasbi, Waldon, and Degen 2019; Waldon and Degen 2020; Phillips et al. 2021). When we say speakers judge that an inference follows, what cognitive processes produce that judgment? How do abstract semantic representations map onto the responses on some scale?\nThis is not merely a methodological detail—it’s a substantive theoretical question. Different linking hypotheses make different predictions about response patterns, allowing us to test not just our semantic theories but our assumptions about how those theories connect to behavior. Even if our real interest is in characterizing the semantic representations of speakers, we can’t ignore the way those representations map onto their responses in some task.",
    "crumbs": [
      "Background",
      "From theory to data"
    ]
  },
  {
    "objectID": "background/theory-to-data.html#understanding-gradience-a-taxonomy-of-uncertainty",
    "href": "background/theory-to-data.html#understanding-gradience-a-taxonomy-of-uncertainty",
    "title": "From theory to data",
    "section": "Understanding Gradience: A Taxonomy of Uncertainty",
    "text": "Understanding Gradience: A Taxonomy of Uncertainty\nOne of the most striking findings from experimental semantics is the pervasiveness of gradience in aggregated measures. While semanticists have long recognized the existence of gradience in some domains–e.g. gradable adjectives–we often assume categorical distinctions in other domains–e.g. factivity. And even where traditional approaches assume categorical distinctions, experimental methods often reveal continuous variation. For the reasons laid out above,understanding this gradience is crucial for developing theories that connect formal semantics to behavioral data.\n\nExamples of Potentially Unexpected Gradience\nThe kinds of distributionally and inferentially defined properties we develop generalizations around are not always readily apparent in large-scale datasets. An example we will look at in-depth in our second case study of the course is that, when attempting to measure veridicality/factivity, we end up with more gradience than we might have expected. We can illustrate this using the MegaAttitude datasets.\nFigure 1 shows veridicality judgments collected by White and Rawlins (2018) as part of their MegaVeridicality dataset.\n\n\n\n\n\n\nFigure 1: Veridicality judgments collected by White and Rawlins (2018) as part of their MegaVeridicality dataset.\n\n\n\nOne thing White and Rawlins (2018) note is the apparent gradience in these measures. This gradience presents a challenge if we want to use these measures to evaluate generalizations about the relationship between two properties. For instance, say we are interested in understanding the relationship betwen factivity and neg(ation)-raising.   A predicate is neg-raising if it gives rise to inferences of the form from (5) to (6):\n\nJo doesn’t think that Mo left.\nJo thinks that Mo didn’t leave.\n\nOne way of deriving a factivity measure from the MegaVeridicality dataset is to take the max along both dimensions, as shown in Figure 2. The idea here is that, it will give rise to veridicality inferences with both positive and negative matrix polarity.\n\n\n\n\n\n\nFigure 2: One way of deriving a factivity measure from the MegaVeridicality dataset.\n\n\n\nNow let’s suppose we’re interested in generalizations about the relationship between two measures. For instance, maybe want to evaluate the relationship between factivity and neg-raising, where we might tend to suspect that factives are not neg-raisers.\nFigure 3 shows a comparison of the measure of neg(ation)-raising from the MegaNegRaising dataset collected by An and White (2020) and the derived factivity measure from the MegaVeridicality dataset collected by White and Rawlins (2018).\n\n\n\n\n\n\nFigure 3: A comparison of the measure of neg(ation)-raising from the MegaNegRaising dataset collected by An and White (2020) and the derived factivity measure from the MegaVeridicality dataset collected by White and Rawlins (2018).\n\n\n\nThe challenge is that, once we move to relating continuous measures, rather than categorical distinctions, we don’t know what the relationship between measures should look like in any particular case. To illustrate, let’s consider another example. Anand and Hacquard (2014) propose that, if a predicate gives rise to inferences about both beliefs and preferences, it backgrounds the belief inferences. To evaluate this hypothesis, we might try to derive a measure of belief inferences and preference inferences and then relate them.\nTo this end, we can use the MegaIntensionality dataset collected by Kane, Gantt, and White (2022). Figure 4 shows a measure of belief inferences and Figure 5 shows a measure of desire inferences.\n\n\n\n\n\n\nFigure 4: A measure of belief inferences from the MegaIntensionality dataset collected by Kane, Gantt, and White (2022).\n\n\n\nAnd Figure 6 shows a comparison of the desire and belief measures.\n\n\n\n\n\n\nFigure 5: A measure of desire inferences from the MegaIntensionality dataset collected by Kane, Gantt, and White (2022).\n\n\n\nFigure 6 show the relationship between these two measures.\n\n\n\n\n\n\nFigure 6: A comparison of the desire and belief measures from the MegaIntensionality dataset collected by Kane, Gantt, and White (2022).\n\n\n\nThere are two main takeaways from this example. First, the generalization proposed by@anand_factivity_2014 is indeed supported by the data. Second, the relationship between these two measures is strikingly different from the relationship we observe between the continuous measures of factivity and neg-raising. We need some way of theorizing about these continuous relationships.\n\n\nTwo Fundamental Types of Uncertainty\nThe framework we’ll explore distinguishes two general types of uncertainty that can produce gradience: resolved (or type-level) uncertainty and unresolved (or token-level) uncertainty, both of which can arise from multiple sources.\nSources of Gradience in Inference Judgments\n├── Resolved (Type-Level) Uncertainty\n│   ├── Ambiguity\n│   │   ├── Lexical (e.g., \"run\" = locomote vs. manage)\n│   │   ├── Syntactic (e.g., attachment ambiguities)\n│   │   └── Semantic (e.g., scope ambiguities)\n│   └── Discourse Status\n│       └── QUD (Question Under Discussion)\n└── Unresolved (Token-Level) Uncertainty\n    ├── Vagueness (e.g., height of a \"tall\" person)\n    ├── World knowledge (e.g., likelihood that facts are true)\n    └── Task effects\n        ├── Response strategies\n        └── Response error\n\n\nResolved Uncertainty: Multiple Discrete Possibilities\nResolved uncertainty arises when speakers must choose among discrete interpretations.  Consider (7):\n\nMy uncle is running the race.\n\nThe verb run is ambiguous—the uncle might be a participant (locomotion) or the organizer (management). Asked “How likely is it that my uncle has good managerial skills?”, participants who interpret run as locomotion might respond near 0.2, while those interpreting it as management might respond near 0.8. The population average might be 0.5, but this reflects a mixture of discrete interpretations, not genuine gradience.\nThis uncertainty is “resolved” because once speakers fix an interpretation, the inference follows determinately. The gradience emerges from averaging across different resolutions, not from uncertainty within any single interpretation.\nA similar phenomenon is observable with anaphora.  Consider (8):\n\nWhenever anyone laughed, the magician scowled and their assistant smirked. They were secretly pleased.\n\nOne is quite likely to infer from (8) that the magician’s assistant is secretly pleased, but not necessarily that the magician is pleased, even though, in principle, it may be that both are, or even that only the magician is. Ultimately, the ambiguity is resolved when we fix the referent.\n\n\nUnresolved Uncertainty: Gradient Within Interpretations\nUnresolved uncertainty contrasts with resolved uncertainty in that it persists even after fixing all ambiguities.  Consider (9):\n\nMy uncle is tall.\n\nEven with no ambiguity about tall’s meaning, speakers remain uncertain whether the uncle exceeds any particular height threshold. This is classic vagueness—the predicate’s application conditions are inherently gradient (Fine 1975; Graff 2000; Christopher Kennedy 2007; Rooij 2011; Sorensen 2023).\nWorld knowledge creates another layer: even knowing someone runs races (locomotion sense), we remain uncertain about their speed, endurance, or likelihood of finishing. These uncertainties appear within individual trials, not just across participants.\n\n\nWhy This Distinction Matters\nThe type of uncertainty has profound implications for semantic theory:\n\nResolved uncertainty suggests discrete semantic representations with probabilistic selection\nUnresolved uncertainty suggests gradient representations or probabilistic reasoning within fixed meanings\n\nDifferent phenomena may involve different uncertainty types. As we’ll see, vagueness seems to give rise to unresolved uncertainty (the conditions of application of tall seem inherently uncertain), while factivity’s gradience is perhaps more puzzling: is it resolved uncertainty from ambiguous predicates, or unresolved uncertainty in projection itself?",
    "crumbs": [
      "Background",
      "From theory to data"
    ]
  },
  {
    "objectID": "background/theory-to-data.html#case-studies-testing-semantic-theory-at-scale",
    "href": "background/theory-to-data.html#case-studies-testing-semantic-theory-at-scale",
    "title": "From theory to data",
    "section": "Case Studies: Testing Semantic Theory at Scale",
    "text": "Case Studies: Testing Semantic Theory at Scale\nTo illustrate how PDS bridges formal semantics and experimental data, we’ll examine two case studies that exemplify different aspects of the framework.\n\nCase Study 1: Vagueness and Gradable Adjectives\nVague predicates provide an ideal starting point because everyone agrees they involve gradient uncertainty. Expressions like tall, expensive, and old lack sharp boundaries—there’s no precise height at which someone becomes tall (Lakoff 1973; Sadock 1977; Lasersohn 1999; Krifka 2007; Solt 2015).\nFormal semantic theories have long recognized this gradience. Degree-based approaches (Klein 1980; Bierwisch 1989; Kamp 1975; Chris Kennedy 1999; Christopher Kennedy and McNally 2005; Christopher Kennedy 2007; Barker 2002) analyze gradable adjectives as expressing relations to contextual thresholds:\n\ntall is true of \\(x\\) if \\(\\ct{height}(x) \\geq d_\\text{tall}\\) (context)\n\nThe threshold \\(d_\\text{tall}\\) varies with context—what counts as tall for a basketball player differs from tall for a child. But even within a fixed context, speakers show gradient judgments about borderline cases.\nThis makes vagueness ideal for demonstrating how PDS works. The framework can: - Maintain the compositional degree-based analysis from formal semantics - Add probability distributions over thresholds to capture gradient judgments - Model how context shifts these distributions - Link threshold distributions to slider scale responses\nRecent experimental work reveals additional complexity. Different adjective types show distinct patterns: - Relative adjectives (tall, wide): Maximum gradience in positive form - Absolute adjectives (clean, dry): Different threshold distributions - Minimum vs. maximum standard: Asymmetric patterns of imprecision\nThese patterns both support and refine formal theories, showing how experimental data can advance theoretical understanding. Recent years have seen partial integration into computational models (Lassiter and Goodman 2013, 2017; Qing and Franke 2014; Kao et al. 2014; Bumford and Rett 2021). We’ll show that PDS allows us to synthesize and compare these different partial approaches.\n\n\nCase Study 2: Factivity and Projection\nWhile vagueness involves expected gradience, factivity presents a puzzle. Traditional theory treats factivity as discrete—predicates either trigger presuppositions or they don’t (Kiparsky and Kiparsky 1970; Karttunen 1971).1 Yet experimental data reveals pervasive gradience.\nA predicate is factive if it triggers inferences about its complement that project through entailment-canceling operators.    Love appears factive because Mo left is inferrable from the standard family of sentences in (10)–(12):\n\nJo loves that Mo left.\nJo doesn’t love that Mo left.\n\nDoes Jo love that Mo left?\n\nBut when White and Rawlins (2018) (discussed above) and Degen and Tonhauser (2022) collected projection judgments at scale, they found continuous variation (Xue and Onea 2011; Smith and Hall 2011; Djärv and Bacovcin 2017 also observe similar patterns). Qualitatively, Degen and Tonhauser (2022) argue that there is no clear line separates factive from non-factive predicates. Mean projection ratings vary continuously from pretend (lowest) to be annoyed (highest).\n\n\n\nAggregate factivity measures from Degen and Tonhauser (2022), showing continuous variation in projection ratings across predicates under questioning.\n\n\nThis gradience poses a theoretical challenge (Simons 2007; Simons et al. 2010, 2017; Tonhauser, Beaver, and Degen 2018).\nKane, Gantt, and White (2022) later showed that this gradience is likely due to task effects. They demonstrate that when one applies a clustering model to these data that accounts for noise due to various factors, many of the standard subclasses of factives pop out. Some of these subclasses–e.g. the cognitive factives, which Karttunen (1971) observes to not always give rise factivity–appear to themselves be associated with non-necessary factive inferences.\nIn this case study, we’ll focus on understanding what gives rise to this gradience. We’ll consider two hypotheses that PDS allows us to state precisely and test against the data collected by Degen and Tonhauser (2021), which uses the same experimental paradigm as Degen and Tonhauser (2022):\nThe Fundamental Discreteness Hypothesis: Factivity remains discrete; gradience reflects: - Multiple predicate senses (factive and non-factive variants) - Structural ambiguity affecting projection (Varlokosta 1994; Giannakidou 1998, 1999, 2009; Roussou 2010; Farudi 2007; Abrusán 2011; Kastner 2015; Ozyildiz 2017) - Contextual variation in whether complements are at-issue (Simons et al. 2017; Roberts and Simons 2024; Qing, Goodman, and Lassiter 2016)\nThe Fundamental Gradience Hypothesis: No discrete factivity property exists. Gradient patterns reflect different degrees to which predicates support complement truth inferences (Tonhauser, Beaver, and Degen 2018).\nPDS allows us to implement both hypotheses formally and test their predictions against fine-grained response distributions—not just means, but entire judgment patterns including multimodality that might indicate mixture distributions. We’ll show how this approach can be applied to judgment data aimed at capturing factivity using various experimental paradigms (Tonhauser 2016; Djärv and Bacovcin 2017; Djärv, Zehr, and Schwarz 2018; White and Rawlins 2018; White et al. 2018; White 2021; Degen and Tonhauser 2021, 2022; Jeong 2021; Kane, Gantt, and White 2022).",
    "crumbs": [
      "Background",
      "From theory to data"
    ]
  },
  {
    "objectID": "background/theory-to-data.html#the-need-for-new-frameworks",
    "href": "background/theory-to-data.html#the-need-for-new-frameworks",
    "title": "From theory to data",
    "section": "The Need for New Frameworks",
    "text": "The Need for New Frameworks\nThese case studies illustrate what we need from a framework connecting formal semantics to experimental data:\nMaintain Compositionality: Theories must derive complex meanings compositionally, preserving insights from decades of formal semantic research. We cannot abandon compositionality just because judgments are gradient.\nModel Uncertainty Explicitly: The framework must represent both types of uncertainty—resolved ambiguities and unresolved gradience—and show how they interact during interpretation.\nMake Linking Hypotheses Precise: We need explicit theories of how semantic representations produce behavioral responses. What cognitive processes intervene between computing a meaning and moving a slider?\nEnable Quantitative Evaluation: Theories must make testable predictions about response distributions, not just average ratings. Different theories should be comparable using standard statistical metrics.\nAs we’ll see in the next section, existing computational approaches like Rational Speech Act (RSA) models attempt to bridge formal semantics with probabilistic reasoning (Frank and Goodman 2012; Goodman and Stuhlmüller 2013). While valuable, these approaches face challenges in maintaining the modularity that makes formal semantic theories powerful. This motivates the development of Probabilistic Dynamic Semantics—a framework that preserves semantic insights while adding the probabilistic tools needed to model gradient behavioral data.",
    "crumbs": [
      "Background",
      "From theory to data"
    ]
  },
  {
    "objectID": "background/theory-to-data.html#footnotes",
    "href": "background/theory-to-data.html#footnotes",
    "title": "From theory to data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe’ll spend a lot of time on Day 4 saying exactly what we mean by discrete here. Karttunen (1971), of course, classically argues that there are predicates that sometimes trigger presuppositions and sometimes don’t. For our purposes, we’ll say that this behavior is discrete in the sense that it’s more like ambiguity than vagueness. That is, we’ll show that uncertainty around factivity displays the hallmarks of resolved uncertainty.↩︎",
    "crumbs": [
      "Background",
      "From theory to data"
    ]
  },
  {
    "objectID": "background/rsa.html",
    "href": "background/rsa.html",
    "title": "Rational Speech Act models",
    "section": "",
    "text": "Rational speech act (RSA) models are a very popular approach to modeling pragmatic inference that integrates ideas from formal semantics into mathematically explicit models of Gricean reasoning (Grice 1975). Crucially, these models aim for a certain kind of modularity: they allow one to provide separate accounts of the literal semantics of expressions, on the one hand, and the inferences that people make when they encounter utterances of these expressions, on the other. They achieve this kind of modularity, essentially, by allowing one to state a theory of literal meaning and then to use a systematic recipe for turning it into a theory of pragmatic inference.\nHere we describe what is sometimes called vanilla RSA. Vanilla RSA is RSA more or less as it was originally formulated Frank and Goodman (2012) and Goodman and Stuhlmüller (2013) (see Degen (2023) for a recent comprehensive overview of the RSA literature). The basic idea is that there are two sets of models, listener models, and speaker models, which are kind of mirror images of each other.\n\nListener models\nIn particular, any given listener model \\(L_{i}\\) characterizes a probability distribution over possible worlds \\(w\\), given some utterance \\(u\\). \\[\n\\begin{aligned}\nP_{L_0}(w | u) &= \\frac{\\begin{cases}\nP_{L_0}(w) & ⟦u⟧^w = \\mathtt{T} \\\\\n0 & ⟦u⟧^w = \\mathtt{F}\n\\end{cases}}{∑_{w^\\prime}\\begin{cases}\nP_{L_0}(w^\\prime) & ⟦u⟧^{w^\\prime} = \\mathtt{T} \\\\\n0 & ⟦u⟧^{w^\\prime} = \\mathtt{F}\n\\end{cases}} \\\\[2mm]\nP_{L_i}(w | u) &= \\frac{P_{L_i}(u | w) * P_{L_i}(w)}{∑_{w^\\prime}P_{L_i}(u | w^\\prime) *\nP_{L_i}(w^\\prime)}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,(i &gt; 0) \\\\[2mm]\n& = \\frac{P_{L_i}(u | w) * P_{L_i}(w)}{P_{L_i}(u)}\n\\end{aligned}\n\\] In words, \\(P_{L_0}(w | u)\\) depends only on whether or not \\(u\\) and \\(w\\) are compatible. It thus acts as a filter, eliminating possible worlds from the prior in which the utterance \\(u\\) is false.\nThe definition of \\(P_{L_i}(w | u)\\) for \\(i &gt; 0\\) uses Bayes’ theorem. To state that this definition uses Bayes’ theorem is kind of a tautology when viewed simply as a mathematical description. Thus what this statement really means is something more operational: RSA models make distinguishing choices about the definitions of \\(P_{L_i}(u | w)\\) and \\(P_{L_i}(w)\\), and it is these latter choices which are used, in turn, to compute \\(P_{L_i}(w | u)\\).\nIn general, the choice \\(P_{L_i}(w)\\) of a prior distribution over \\(w\\) is made once and for all, regardless of the particular model, so we can just call this choice \\(P(w)\\). \\(P(w)\\) can be seen to give a representation of the context set in a given discourse; that is, the distribution over possible worlds known in common among the interlocutors, before anything is uttered.\n\n\nSpeaker models\nThe definition of \\(P_{L_i}(u | w)\\), on the other hand, is chosen to reflect the model \\(S_i\\) of the speaker, which brings us to the other set of models. Thus \\(P_{L_i}(u | w) = P_{S_i}(u | w)\\), where \\[\\begin{aligned}\nP_{S_i}(u | w) &= \\frac{e^{α * 𝕌_{S_i}(u; w)}}{∑_{u^\\prime}e^{α *\n𝕌_{S_i}(u^\\prime; w)}}\n\\end{aligned}\\] \\(𝕌_{S_i}(u; w)\\) is the utility \\(S_i\\) assigns to the utterance \\(u\\), given its intention to communicate the world \\(w\\). Utility for \\(S_i\\) is typically defined as \\[𝕌_{S_i}(u; w) = ln(P_{L_{i-1}}(w | u)) - C(u)\\] that is, the natural log of the probability that \\(L_{i-1}\\) assigns to \\(w\\) (given \\(u\\)), minus \\(u\\)’s cost (\\(C(u)\\)). \\(α\\) is known as the temperature (or the rationality parameter) associated with \\(S_i\\). When \\(α = 0\\), \\(S_i\\) chooses utterances randomly (from a uniform distribution), without attending to their utility in communicating \\(w\\). When \\(α\\) tends toward \\(∞\\), \\(S_i\\) becomes more and more deterministic in its choice of utterance, assigning more and more probability mass to the utterance that maximizes utility in communicating \\(w\\). A little more formally, \\[\\lim_{α → ∞}\\frac{e^{α * 𝕌_{S_i}(u; w)}}{∑_{u^\\prime}e^{α *\n𝕌_{S_i}(u^\\prime; w)}} = \\begin{cases}\n1 & u = \\arg\\max_{u^\\prime}(𝕌_{S_i}(u^\\prime; w)) \\\\\n0 & u ≠ \\arg\\max_{u^\\prime}(𝕌_{S_i}(u^\\prime; w))\n\\end{cases}\\] Because the cost \\(C(u)\\) only depends on \\(u\\), it is nice to view \\(e^{α * 𝕌_{S_i}(u; w)}\\) as factored into a prior and (something like) a likelihood, so that \\(P_{S_i}(u | w)\\) has a formulation symmetrical to that of \\(P_{L_i}(w | u)\\) (when \\(i &gt; 0\\)); that is, it can be formulated in the following way: \\[\\begin{aligned}\ne^{α * 𝕌_{S_i}(u; w)} &= P_{L_{i - 1}}(w | u)^α * \\frac{1}{e^{α * C(u)}}\n\\\\[2mm] &∝ P_{S_i}(w | u) * P_{S_i}(u) \\\\[2mm]\n&= P_{S_i}(w | u) * P(u)\n\\end{aligned}\\] In effect, we can define \\(P_{S_i}(w | u)\\), viewed as a function of \\(u\\), to be proportional to \\(P_{L_{i - 1}}(w | u)^α\\); meanwhile, we can define \\(P(u)\\), the prior probability over utterances, to be proportional to \\(\\frac{1}{e^{α * C(u)}}\\). (Note that if we ignore cost altogether, so that \\(C(u)\\) is always, say, 0, then \\(P(u)\\) just becomes a uniform distribution.)\nTaking these points into consideration, we may reformulate our speaker model, \\(S_i\\), as follows: \\[\\begin{aligned}\nP_{S_i}(u | w) &= \\frac{P_{S_i}(w | u) * P(u)}{∑_{u^\\prime}P_{S_i}(w |\nu^\\prime) * P(u^\\prime)} \\\\[2mm]\n&= \\frac{P_{S_i}(w | u) * P(u)}{P_{S_i}(w)}\n\\end{aligned}\\] In words, the speaker model, just like the listener model, may be viewed operationally in terms of Bayes’ theorem. Note that \\(P_{S_i}(w)\\), in general, defines a different distribution from \\(P(w)\\), the listener’s prior distribution over worlds (i.e., the context set). The former represents, not prior knowledge about the context, but rather something more like the relative “communicability” of a given possible world, given the distribution \\(P(u)\\) over utterances; that is, how likely a random utterance makes \\(w\\), though with the exponential \\(α\\) applied.\n\n\nAn example\nAn example helps illustrate how the probability distributions determined by RSA speaker and listener models are computed in practice. Let’s say there are seven cookies, as depicted in the image below.\n\n\n\nCookies (7 of them)\n\n\nFurther, say someone utters the sentence Jo ate five cookies. We’ll assume that the literal meaning of such a sentence is lower bounded: it is true just in case the number cookies Jo ate is at least five, i.e., \\[\nn_{\\textit{cookies}} ≥ 5\n\\] Let’s now consider the probability distributions computed by the models \\(L_{0}\\), \\(S_{1}\\), and \\(L_{1}\\), following the definitions given earlier.\n\nThe literal listener \\(L_{0}\\)\nRecall that the literal listener is a filter: \\[\nP_{L_{0}}(w ∣ u) ∝ 𝟙(w ≥ n) × P (w)\n\\] Let’s also assume that \\(P(w)\\), the prior distribution over the number of cookies Jo ate is uniform. Then, the literal listener is simply zeroing out the portion of this prior distribution in which Jo ate less than five cookies and renormalizing the resulting distribution. The following table illustrates this for Jo ate five cookies, as well as two other utterances. Here, \\(w\\) (the world) is identified with a possible inference; i.e., about how many cookies Jo actually ate.\n\n\n\n\\(w =\\)\n5\n6\n7\n\n\n\n\n\\(u = \\textit{Jo ate 5 cookies}\\)\n1/3\n1/3\n1/3\n\n\n\\(u = \\textit{Jo ate 6 cookies}\\)\n0\n1/2\n1/2\n\n\n\\(u = \\textit{Jo ate 7 cookies}\\)\n0\n0\n1\n\n\n\nThus if Jo ate five cookies is uttered, \\(L_{0}\\) assigns a probability of 1/3 to each of the possible inferences compatible with the utterance’s lower-bounded literal meaning.\n\n\nThe pragmatic speaker \\(S_{1}\\)\nHere is the pragmatic speaker model again, reformulated (i) as a proportionality statement, and (ii) by moving the cost term into the denominator: \\[\nP_{S_{1}}(u ∣ w) ∝ \\frac{P_{L_{0}}(w ∣ u)^{α}}{e^{α × C(u)}}\n\\] For the purposes of the example, let’s assume that the rationality parameter \\(α = 4\\), and that the cost \\(C(u)\\) of an utterance is constant across utterances. Let’s further assume that the speaker is only considering the utterances listed in the following table; i.e., its prior distribution—given the constant cost function—is uniform over these alternatives. Then, we obtain the following distributions over utterances for three possible worlds corresponding to the inference which the speaker intends to communicate.\n\n\n\n\\(w =\\)\n5\n6\n7\n\n\n\n\n\\(u = \\textit{Jo ate 5 cookies}\\)\n1\n0.16\n0.01\n\n\n\\(u = \\textit{Jo ate 6 cookies}\\)\n0\n0.84\n0.06\n\n\n\\(u = \\textit{Jo ate 7 cookies}\\)\n0\n0\n0.93\n\n\n\nThus if \\(S_{1}\\) wishes to convey that Jo ate exactly five cookies, it chooses the first utterance with a probability of 1. This is because the literal listener assigns 5 cookies a probability of 0 if one of the other two stronger sentences is uttered. Meanwhile, if it wishes to convey that Jo ate exactly six cookies, it chooses the first utterance with probability \\(\\frac{(1/3)^4}{(1/3)^4 + (1/2)^4} ≈ 0.16\\) and the second utterance with probability \\(\\frac{(1/2)^4}{(1/3)^4 + (1/2)^4} ≈ 0.84\\). Crucially, we see that probabilities are normalized within columns of the table, rather than rows, as is the case for the listener models.\n\n\nThe pragmatic listener \\(L_{1}\\)\nFinally, recall that the pragmatic listener model uses the pragmatic speaker model as a representation of the likelihood of some utterance, given an intended inference. \\[\nP_{L_{1}}(w ∣ u) ∝ P_{S_{1}}(u ∣ w) × P (w)\n\\] Given that there is a uniform prior distribution over numbers of cookies, we may obtain probability distributions for the same three utterances by taking the table in the previous subsection and renormalizing its probabilities within rows.\n\n\n\n\\(w =\\)\n5\n6\n7\n\n\n\n\n\\(u = \\textit{Jo ate 5 cookies}\\)\n0.85\n0.14\n0.01\n\n\n\\(u = \\textit{Jo ate 6 cookies}\\)\n0\n0.93\n0.07\n\n\n\\(u = \\textit{Jo ate 7 cookies}\\)\n0\n0\n1\n\n\n\nNote that if we had a non-uniform prior over numbers of cookies—e.g., if 6 is more probable than 5 (classic Jo)—we can simply multiply the entries of this table by their prior probabilities and renormalize them within rows once again.\n\n\n\nRSA discussion\nAs noted earlier, RSA models come with a very appealing feature: that they provide a modular separation between semantic and pragmatic concerns. In particular, the \\(L_{0}\\) model can be seen as instantiating a semantic analysis of some utterance (which is, ideally, provided by some external theory of the semantics of utterances), while the \\(L_{1}\\) model can be seen as instantiating a pragmatic theory that is built up from the semantic theory in a fairly deterministic way (once, e.g., cost parameters, rationality parameters, and prior distributions over utterance alternatives are fixed). Indeed, such a separation can be methodologically useful, since it allows one to test particular semantic theories in the face of human inference data that arises from pragmatic (as well as other) factors (see, e.g., Waldon and Degen (2020) for discussion)\n\nChallenges\nThere is a particular set of challenges for RSA models, as they are typically stated, that we aim to address in this course. Namely, it is not super obvious what role the Montagovian notion of semantic compositionality can play. Note that the account of the literal listener \\(L_{0}\\) must come “from outside”: RSA models are typically defined on top of analyses of sentence meaning, as opposed to the meanings of basic expressions, though the latter are presumably implicated in deriving the former. Thus there are certain questions about semantic compositionality which such models don’t address:\n\nHow may the semantics of individual expressions be studied in tandem with their pragmatic effects? How should such pragmatic effects be formally encoded in lexical meaning representations?\nHow do pragmatic effects compose, in order to yield the global pragmatic effects associated with entire utterances?\n\nOne of the aims of this course is to provide a framework in which the pragmatic effects of individual expressions may be stated and composed, and then tested against human inference data.\n\n\n\n\n\n\n\n\nReferences\n\nDegen, Judith. 2023. “The Rational Speech Act Framework.” Annual Review of Linguistics 9 (Volume 9, 2023): 519–40. https://doi.org/10.1146/annurev-linguistics-031220-010811.\n\n\nFrank, Michael C., and Noah D. Goodman. 2012. “Predicting Pragmatic Reasoning in Language Games.” Science 336 (6084): 998–98. https://doi.org/10.1126/science.1218633.\n\n\nGoodman, Noah D., and Andreas Stuhlmüller. 2013. “Knowledge and Implicature: Modeling Language Understanding as Social Cognition.” Topics in Cognitive Science 5 (1): 173–84. https://doi.org/10.1111/tops.12007.\n\n\nGrice, H. Paul. 1975. “Logic and Conversation.” In Syntax and Semantics, edited by Peter Cole and Jerry L. Morgan, 3, Speech Acts:41–58. New York: Academic Press.\n\n\nWaldon, Brandon, and Judith Degen. 2020. “Modeling Behavior in Truth Value Judgment Task Experiments.” In Proceedings of the Society for Computation in Linguistics 2020, edited by Allyson Ettinger, Gaja Jarosz, and Joe Pater, 238–47. New York, New York: Association for Computational Linguistics. https://aclanthology.org/2020.scil-1.29/.",
    "crumbs": [
      "Background",
      "Rational Speech Act models"
    ]
  },
  {
    "objectID": "background/case-studies.html",
    "href": "background/case-studies.html",
    "title": "Two case studies",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\updct}[1]{\\ct{upd\\_#1}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\newcommand{\\pure}[1]{\\bbox[border: 1px solid orange]{\\bbox[border: 4px solid transparent]{#1}}}\n\\newcommand{\\return}[1]{\\bbox[border: 1px solid black]{\\bbox[border: 4px solid transparent]{#1}}}\n\\def\\P{\\mathtt{P}}\n\\def\\Q{\\mathtt{Q}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\def\\ite{\\ct{if\\_then\\_else}}\n\\def\\Do{\\abbr{do}}\n\\]\nTo illustrate how PDS bridges formal semantics and experimental data, we’ll examine two case studies that exemplify different aspects of the framework.",
    "crumbs": [
      "Background",
      "Two case studies"
    ]
  },
  {
    "objectID": "background/case-studies.html#footnotes",
    "href": "background/case-studies.html#footnotes",
    "title": "Two case studies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe’ll spend a lot of time on Day 4 saying exactly what we mean by discrete here. Karttunen (1971), of course, classically argues that there are predicates that sometimes trigger presuppositions and sometimes don’t. For our purposes, we’ll say that this behavior is discrete in the sense that it’s more like ambiguity than vagueness. That is, we’ll show that uncertainty around factivity displays the hallmarks of resolved uncertainty.↩︎",
    "crumbs": [
      "Background",
      "Two case studies"
    ]
  },
  {
    "objectID": "adjectives/adjectives.html",
    "href": "adjectives/adjectives.html",
    "title": "Gradable adjectives",
    "section": "",
    "text": "Note\n\n\n\nThis module will become available on August 7.",
    "crumbs": [
      "Gradable adjectives",
      "Gradable adjectives"
    ]
  },
  {
    "objectID": "factivity/factivity.html",
    "href": "factivity/factivity.html",
    "title": "Factivity",
    "section": "",
    "text": "Note\n\n\n\nThis module will become available on August 8.",
    "crumbs": [
      "Factivity inferences",
      "Factivity"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probabilistic dynamic semantics",
    "section": "",
    "text": "Outline\nThe recent advent of linguistic datasets and their associated statistical models have given rise to two major kinds of questions bearing on linguistic theory and methodology:\n\nHow can semanticists use such datasets? That is, how can the statistical properties of a dataset inform semantic theory directly, and what guiding principles regulate the link between such properties and semantic theory?\nHow should semantic theories themselves be modified so that they may characterize not only informally collected acceptability and inference judgments, but statistical generalizations observed from datasets?\n\nThis course brings the compositional, algebraic view of meaning employed by semanticists into contact with linguistic datasets by introducing and applying the framework of Probabilistic Dynamic Semantics (Grove and White 2024a, 2025, 2024b). PDS seamlessly integrates theories of semantic competence with accounts of linguistic behavior in experimental settings by taking a modular approach: given a dataset involving some semantic phenomenon, and which exhibits certain statistical properties, this course offers an approach to developing both (a) theories of the meanings assigned to the expressions present in the dataset, and (b) linking hypotheses that directly relate these theories to linguistic behavior.\n\n\nExisting probabilistic approaches to meaning\nThe ideas developed in this course build on and respond to existing probabilistic approaches to semantics and pragmatics, including those which use computational modeling to characterize inference (Zeevat and Schmitz 2015; Franke and Jäger 2016; Brasoveanu and Dotlačil 2020; Bernardy et al. 2022; Noah D. Goodman, Tenenbaum, and Contributors 2016). Such models are motivated, in part, by the observation that linguistic inference tends to display substantial gradience, giving rise to quantitative patterns that traditional semantic theory has difficulty capturing. Meanwhile, they often aim to explain Gricean linguistic behavior (Grice 1975) by regarding humans as Bayesian reasoners. Indeed, due to this emphasis on pragmatic principles, much modeling work blurs the semantics/pragmatics distinction, rendering the connection to traditional semantic theory somewhat opaque.\nTo take a paradigm case, models within the Rational Speech Act (RSA) framework consider human interpreters as inferring meanings for an utterance which maximize the utterance’s utility relative to a set of possible alternative utterances (Frank and Goodman 2012; Lassiter 2011; Noah D. Goodman and Stuhlmüller 2013; Noah D. Goodman and Frank 2016; Lassiter and Goodman 2017; Degen 2023). Probabilistic models of linguistic inference, including RSA, tend to encode Bayesian principles of probabilistic update in terms of Bayes’ theorem, which states that the posterior probability of an event given an observation is proportional to the prior probability of the event, multiplied by the likelihood of the observation given the event. RSA models give an explicit operational interpretation to Bayes’ theorem by assuming that prior distributions over inferences encode world knowledge, and that likelihoods represent the utility-maximizing behavior of a pragmatic speaker.\nDespite their success in modeling a wide variety of semantic and pragmatic phenomena, probabilistic models of linguistic data remain largely divorced from semantic and pragmatic practice, both in theory and in implementation. RSA models, for example, regard the semantic interpretations which humans pragmatically reason about as being provided by a literal listener that determines a distribution over inferences, given an utterance (Degen 2023). But aside from the constraint that the literal listener’s posterior distribution is proportional to its prior distribution (i.e., that it acts as a filter), the semantic components of RSA models are generally designed by researchers on an ad hoc basis: on the one hand, the space (I) of possible inferences must be decided by individual researchers in a way that depends on the task being modeled; on the other hand, the relation (⟦·⟧) between utterances and inferences is often assumed without a justified connection to any semantic theory using, e.g., an explicit grammar fragment in the style of Montague (1973).\n\n\nPDS as a bridge between probabilistic models and semantic theory\nGiven this background, this course introduces a novel approach to probabilistic meaning which integrates traditional Montague semantics, as well as ideas in compositional dynamic semantics, with probabilistic computational models in a completely seamless fashion. The theoretical framework and methodology we introduce retain the beneficial features of both kinds of approach to meaning: PDS may be used to construct probabilistic models of human inference data, and it is in principle compatible with existing probabilistic modeling paradigms such as RSA; meanwhile, it seamlessly connects probabilistic models to compositional dynamic semantics in the Montagovian tradition by providing a setting to write full-fledged grammar fragments.\nPDS additionally provides a theory of dynamic discourse update, integrating aspects of discourse such as the common ground, the question under discussion (Ginzburg 1996; Roberts 2012; Farkas and Bruce 2010), and uncertainty about lexical meaning. Crucially, given a semantic theory of some discourse phenomenon couched with PDS, one may obtain a probabilistic model of some linguistic dataset, given a particular response function (Grove and White 2024a, 2025, 2024b). We introduce PDS in the context empirical datasets studying factivity, gradable adjectives, and the question under discussion.\n\n\n\n\n\n\n\nReferences\n\nBernardy, Jean-Philippe, Rasmus Blanck, Stergios Chatzikyriakidis, and Aleksandre Maskharashvili. 2022. “Bayesian Natural Language Semantics and Pragmatics.” In Probabilistic Approaches to Linguistic Theory, edited by Jean-Philippe Bernardy, Rasmus Blanck, Stergios Chatzikyriakidis, Shalom Lappin, and Aleksandre Maskharashvili. CSLI Publications.\n\n\nBrasoveanu, Adrian, and Jakub Dotlačil. 2020. Computational Cognitive Modeling and Linguistic Theory. Vol. 6. Language, Cognition, and Mind. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-31846-8.\n\n\nDegen, Judith. 2023. “The Rational Speech Act Framework.” Annual Review of Linguistics 9 (Volume 9, 2023): 519–40. https://doi.org/10.1146/annurev-linguistics-031220-010811.\n\n\nFarkas, Donka F., and Kim B. Bruce. 2010. “On Reacting to Assertions and Polar Questions.” Journal of Semantics 27 (1): 81–118. https://doi.org/10.1093/jos/ffp010.\n\n\nFrank, Michael C., and Noah D. Goodman. 2012. “Predicting Pragmatic Reasoning in Language Games.” Science 336 (6084): 998–98. https://doi.org/10.1126/science.1218633.\n\n\nFranke, Michael, and Gerhard Jäger. 2016. “Probabilistic Pragmatics, or Why Bayes’ Rule Is Probably Important for Pragmatics.” Zeitschrift Für Sprachwissenschaft 35 (1): 3–44. https://doi.org/10.1515/zfs-2016-0002.\n\n\nGinzburg, Jonathan. 1996. “Dynamics and the Semantics of Dialogue.” In Logic, Language, and Computation, edited by Jerry Seligman and Dag Westerståhl, 1:221–37. Stanford: CSLI Publications.\n\n\nGoodman, Noah D., and Michael C. Frank. 2016. “Pragmatic Language Interpretation as Probabilistic Inference.” Trends in Cognitive Sciences 20 (11): 818–29. https://doi.org/10.1016/j.tics.2016.08.005.\n\n\nGoodman, Noah D., and Andreas Stuhlmüller. 2013. “Knowledge and Implicature: Modeling Language Understanding as Social Cognition.” Topics in Cognitive Science 5 (1): 173–84. https://doi.org/10.1111/tops.12007.\n\n\nGoodman, Noah D, Joshua B. Tenenbaum, and The ProbMods Contributors. 2016. “Probabilistic Models of Cognition.” http://probmods.org/v2.\n\n\nGrice, H. Paul. 1975. “Logic and Conversation.” In Syntax and Semantics, edited by Peter Cole and Jerry L. Morgan, 3, Speech Acts:41–58. New York: Academic Press.\n\n\nGrove, Julian, and Aaron Steven White. 2024a. “Factivity, Presupposition Projection, and the Role of Discrete Knowlege in Gradient Inference Judgments.” LingBuzz. https://ling.auf.net/lingbuzz/007450.\n\n\n———. 2024b. “Probabilistic Dynamic Semantics.” University of Rochester. https://ling.auf.net/lingbuzz/008478.\n\n\n———. 2025. “Modeling the Prompt in Inference Judgment Tasks.” Experiments in Linguistic Meaning 3 (January): 176–87. https://doi.org/10.3765/elm.3.5857.\n\n\nLassiter, Daniel. 2011. “Vagueness as Probabilistic Linguistic Knowledge.” In Vagueness in Communication, edited by Rick Nouwen, Robert van Rooij, Uli Sauerland, and Hans-Christian Schmitz, 127–50. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-642-18446-8_8.\n\n\nLassiter, Daniel, and Noah D. Goodman. 2017. “Adjectival Vagueness in a Bayesian Model of Interpretation.” Synthese 194 (10): 3801–36. https://doi.org/10.1007/s11229-015-0786-1.\n\n\nMontague, Richard. 1973. “The Proper Treatment of Quantification in Ordinary English.” In Approaches to Natural Language: Proceedings of the 1970 Stanford Workshop on Grammar and Semantics, edited by K. J. J. Hintikka, J. M. E. Moravcsik, and P. Suppes, 221–42. Synthese Library. Dordrecht: Springer Netherlands. https://doi.org/10.1007/978-94-010-2506-5_10.\n\n\nRoberts, Craige. 2012. “Information Structure: Towards an Integrated Formal Theory of Pragmatics.” Semantics and Pragmatics 5 (December): 6:1–69. https://doi.org/10.3765/sp.5.6.\n\n\nZeevat, Henk, and Hans-Christian Schmitz, eds. 2015. Bayesian Natural Language Semantics and Pragmatics. Vol. 2. Language, Cognition, and Mind. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-17064-0.",
    "crumbs": [
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "background/new-frameworks.html",
    "href": "background/new-frameworks.html",
    "title": "The need for new frameworks",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\updct}[1]{\\ct{upd\\_#1}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\newcommand{\\pure}[1]{\\bbox[border: 1px solid orange]{\\bbox[border: 4px solid transparent]{#1}}}\n\\newcommand{\\return}[1]{\\bbox[border: 1px solid black]{\\bbox[border: 4px solid transparent]{#1}}}\n\\def\\P{\\mathtt{P}}\n\\def\\Q{\\mathtt{Q}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\def\\ite{\\ct{if\\_then\\_else}}\n\\def\\Do{\\abbr{do}}\n\\]\n\nThese case studies illustrate what we need from a framework connecting formal semantics to experimental data:\nMaintain Compositionality: Theories must derive complex meanings compositionally, preserving insights from decades of formal semantic research. We cannot abandon compositionality just because judgments are gradient.\nModel Uncertainty Explicitly: The framework must represent both types of uncertainty—resolved ambiguities and unresolved gradience—and show how they interact during interpretation.\nMake Linking Hypotheses Precise: We need explicit theories of how semantic representations produce behavioral responses. What cognitive processes intervene between computing a meaning and moving a slider?\nEnable Quantitative Evaluation: Theories must make testable predictions about response distributions, not just average ratings. Different theories should be comparable using standard statistical metrics.\nAs we’ll see in the next section, existing computational approaches like Rational Speech Act (RSA) models attempt to bridge formal semantics with probabilistic reasoning (Frank and Goodman 2012; Goodman and Stuhlmüller 2013). While valuable, these approaches face challenges in maintaining the modularity that makes formal semantic theories powerful. This motivates the development of Probabilistic Dynamic Semantics—a framework that preserves semantic insights while adding the probabilistic tools needed to model gradient behavioral data.\n\n\n\n\n\n\nReferences\n\nFrank, Michael C., and Noah D. Goodman. 2012. “Predicting Pragmatic Reasoning in Language Games.” Science 336 (6084): 998–98. https://doi.org/10.1126/science.1218633.\n\n\nGoodman, Noah D., and Andreas Stuhlmüller. 2013. “Knowledge and Implicature: Modeling Language Understanding as Social Cognition.” Topics in Cognitive Science 5 (1): 173–84. https://doi.org/10.1111/tops.12007.",
    "crumbs": [
      "Background",
      "The need for new frameworks"
    ]
  },
  {
    "objectID": "background/theoretically-oriented-approach.html",
    "href": "background/theoretically-oriented-approach.html",
    "title": "Rational Speech Act models",
    "section": "",
    "text": "Rational speech act (RSA) models are a very popular approach to modeling pragmatic inference that integrates ideas from formal semantics into mathematically explicit models of Gricean reasoning (Grice 1975). Crucially, these models aim for a certain kind of modularity: they allow one to provide separate accounts of the literal semantics of expressions, on the one hand, and the inferences that people make when they encounter utterances of these expressions, on the other. They achieve this kind of modularity, essentially, by allowing one to state a theory of literal meaning and then to use a systematic recipe for turning it into a theory of pragmatic inference.\nHere we describe what is sometimes called vanilla RSA. Vanilla RSA is RSA more or less as it was originally formulated Frank and Goodman (2012) and Goodman and Stuhlmüller (2013) (see Degen (2023) for a recent comprehensive overview of the RSA literature). The basic idea is that there are two sets of models, listener models, and speaker models, which are kind of mirror images of each other.\n\nListener models\nIn particular, any given listener model \\(L_{i}\\) characterizes a probability distribution over possible worlds \\(w\\), given some utterance \\(u\\). \\[\n\\begin{aligned}\nP_{L_0}(w | u) &= \\frac{\\begin{cases}\nP_{L_0}(w) & ⟦u⟧^w = \\mathtt{T} \\\\\n0 & ⟦u⟧^w = \\mathtt{F}\n\\end{cases}}{∑_{w^\\prime}\\begin{cases}\nP_{L_0}(w^\\prime) & ⟦u⟧^{w^\\prime} = \\mathtt{T} \\\\\n0 & ⟦u⟧^{w^\\prime} = \\mathtt{F}\n\\end{cases}} \\\\[2mm]\nP_{L_i}(w | u) &= \\frac{P_{L_i}(u | w) * P_{L_i}(w)}{∑_{w^\\prime}P_{L_i}(u | w^\\prime) *\nP_{L_i}(w^\\prime)}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,(i &gt; 0) \\\\[2mm]\n& = \\frac{P_{L_i}(u | w) * P_{L_i}(w)}{P_{L_i}(u)}\n\\end{aligned}\n\\] In words, \\(P_{L_0}(w | u)\\) depends only on whether or not \\(u\\) and \\(w\\) are compatible. It thus acts as a filter, eliminating possible worlds from the prior in which the utterance \\(u\\) is false.\nThe definition of \\(P_{L_i}(w | u)\\) for \\(i &gt; 0\\) uses Bayes’ theorem. To state that this definition uses Bayes’ theorem is kind of a tautology when viewed simply as a mathematical description. Thus what this statement really means is something more operational: RSA models make distinguishing choices about the definitions of \\(P_{L_i}(u | w)\\) and \\(P_{L_i}(w)\\), and it is these latter choices which are used, in turn, to compute \\(P_{L_i}(w | u)\\).\nIn general, the choice \\(P_{L_i}(w)\\) of a prior distribution over \\(w\\) is made once and for all, regardless of the particular model, so we can just call this choice \\(P(w)\\). \\(P(w)\\) can be seen to give a representation of the context set in a given discourse; that is, the distribution over possible worlds known in common among the interlocutors, before anything is uttered.\n\n\nSpeaker models\nThe definition of \\(P_{L_i}(u | w)\\), on the other hand, is chosen to reflect the model \\(S_i\\) of the speaker, which brings us to the other set of models. Thus \\(P_{L_i}(u | w) = P_{S_i}(u | w)\\), where \\[\\begin{aligned}\nP_{S_i}(u | w) &= \\frac{e^{α * 𝕌_{S_i}(u; w)}}{∑_{u^\\prime}e^{α *\n𝕌_{S_i}(u^\\prime; w)}}\n\\end{aligned}\\] \\(𝕌_{S_i}(u; w)\\) is the utility \\(S_i\\) assigns to the utterance \\(u\\), given its intention to communicate the world \\(w\\). Utility for \\(S_i\\) is typically defined as \\[𝕌_{S_i}(u; w) = ln(P_{L_{i-1}}(w | u)) - C(u)\\] that is, the natural log of the probability that \\(L_{i-1}\\) assigns to \\(w\\) (given \\(u\\)), minus \\(u\\)’s cost (\\(C(u)\\)). \\(α\\) is known as the temperature (or the rationality parameter) associated with \\(S_i\\). When \\(α = 0\\), \\(S_i\\) chooses utterances randomly (from a uniform distribution), without attending to their utility in communicating \\(w\\). When \\(α\\) tends toward \\(∞\\), \\(S_i\\) becomes more and more deterministic in its choice of utterance, assigning more and more probability mass to the utterance that maximizes utility in communicating \\(w\\). A little more formally, \\[\\lim_{α → ∞}\\frac{e^{α * 𝕌_{S_i}(u; w)}}{∑_{u^\\prime}e^{α *\n𝕌_{S_i}(u^\\prime; w)}} = \\begin{cases}\n1 & u = \\arg\\max_{u^\\prime}(𝕌_{S_i}(u^\\prime; w)) \\\\\n0 & u ≠ \\arg\\max_{u^\\prime}(𝕌_{S_i}(u^\\prime; w))\n\\end{cases}\\] Because the cost \\(C(u)\\) only depends on \\(u\\), it is nice to view \\(e^{α * 𝕌_{S_i}(u; w)}\\) as factored into a prior and (something like) a likelihood, so that \\(P_{S_i}(u | w)\\) has a formulation symmetrical to that of \\(P_{L_i}(w | u)\\) (when \\(i &gt; 0\\)); that is, it can be formulated in the following way: \\[\\begin{aligned}\ne^{α * 𝕌_{S_i}(u; w)} &= P_{L_{i - 1}}(w | u)^α * \\frac{1}{e^{α * C(u)}}\n\\\\[2mm] &∝ P_{S_i}(w | u) * P_{S_i}(u) \\\\[2mm]\n&= P_{S_i}(w | u) * P(u)\n\\end{aligned}\\] In effect, we can define \\(P_{S_i}(w | u)\\), viewed as a function of \\(u\\), to be proportional to \\(P_{L_{i - 1}}(w | u)^α\\); meanwhile, we can define \\(P(u)\\), the prior probability over utterances, to be proportional to \\(\\frac{1}{e^{α * C(u)}}\\). (Note that if we ignore cost altogether, so that \\(C(u)\\) is always, say, 0, then \\(P(u)\\) just becomes a uniform distribution.)\nTaking these points into consideration, we may reformulate our speaker model, \\(S_i\\), as follows: \\[\\begin{aligned}\nP_{S_i}(u | w) &= \\frac{P_{S_i}(w | u) * P(u)}{∑_{u^\\prime}P_{S_i}(w |\nu^\\prime) * P(u^\\prime)} \\\\[2mm]\n&= \\frac{P_{S_i}(w | u) * P(u)}{P_{S_i}(w)}\n\\end{aligned}\\] In words, the speaker model, just like the listener model, may be viewed operationally in terms of Bayes’ theorem. Note that \\(P_{S_i}(w)\\), in general, defines a different distribution from \\(P(w)\\), the listener’s prior distribution over worlds (i.e., the context set). The former represents, not prior knowledge about the context, but rather something more like the relative “communicability” of a given possible world, given the distribution \\(P(u)\\) over utterances; that is, how likely a random utterance makes \\(w\\), though with the exponential \\(α\\) applied.\n\n\nAn example\nAn example helps illustrate how the probability distributions determined by RSA speaker and listener models are computed in practice. Let’s say there are seven cookies, as depicted in the image below.\n\n\n\nCookies (7 of them)\n\n\nFurther, say someone utters the sentence Jo ate five cookies. We’ll assume that the literal meaning of such a sentence is lower bounded: it is true just in case the number cookies Jo ate is at least five, i.e., \\[\nn_{\\textit{cookies}} ≥ 5\n\\] Let’s now consider the probability distributions computed by the models \\(L_{0}\\), \\(S_{1}\\), and \\(L_{1}\\), following the definitions given earlier.\n\nThe literal listener \\(L_{0}\\)\nRecall that the literal listener is a filter: \\[\nP_{L_{0}}(w ∣ u) ∝ 𝟙(w ≥ n) × P (w)\n\\] Let’s also assume that \\(P(w)\\), the prior distribution over the number of cookies Jo ate is uniform. Then, the literal listener is simply zeroing out the portion of this prior distribution in which Jo ate less than five cookies and renormalizing the resulting distribution. The following table illustrates this for Jo ate five cookies, as well as two other utterances. Here, \\(w\\) (the world) is identified with a possible inference; i.e., about how many cookies Jo actually ate.\n\n\n\n\\(w =\\)\n5\n6\n7\n\n\n\n\n\\(u = \\textit{Jo ate 5 cookies}\\)\n1/3\n1/3\n1/3\n\n\n\\(u = \\textit{Jo ate 6 cookies}\\)\n0\n1/2\n1/2\n\n\n\\(u = \\textit{Jo ate 7 cookies}\\)\n0\n0\n1\n\n\n\nThus if Jo ate five cookies is uttered, \\(L_{0}\\) assigns a probability of 1/3 to each of the possible inferences compatible with the utterance’s lower-bounded literal meaning.\n\n\nThe pragmatic speaker \\(S_{1}\\)\nHere is the pragmatic speaker model again, reformulated (i) as a proportionality statement, and (ii) by moving the cost term into the denominator: \\[\nP_{S_{1}}(u ∣ w) ∝ \\frac{P_{L_{0}}(w ∣ u)^{α}}{e^{α × C(u)}}\n\\] For the purposes of the example, let’s assume that the rationality parameter \\(α = 4\\), and that the cost \\(C(u)\\) of an utterance is constant across utterances. Let’s further assume that the speaker is only considering the utterances listed in the following table; i.e., its prior distribution—given the constant cost function—is uniform over these alternatives. Then, we obtain the following distributions over utterances for three possible worlds corresponding to the inference which the speaker intends to communicate.\n\n\n\n\\(w =\\)\n5\n6\n7\n\n\n\n\n\\(u = \\textit{Jo ate 5 cookies}\\)\n1\n0.16\n0.01\n\n\n\\(u = \\textit{Jo ate 6 cookies}\\)\n0\n0.84\n0.06\n\n\n\\(u = \\textit{Jo ate 7 cookies}\\)\n0\n0\n0.93\n\n\n\nThus if \\(S_{1}\\) wishes to convey that Jo ate exactly five cookies, it chooses the first utterance with a probability of 1. This is because the literal listener assigns 5 cookies a probability of 0 if one of the other two stronger sentences is uttered. Meanwhile, if it wishes to convey that Jo ate exactly six cookies, it chooses the first utterance with probability \\(\\frac{(1/3)^4}{(1/3)^4 + (1/2)^4} ≈ 0.16\\) and the second utterance with probability \\(\\frac{(1/2)^4}{(1/3)^4 + (1/2)^4} ≈ 0.84\\). Crucially, we see that probabilities are normalized within columns of the table, rather than rows, as is the case for the listener models.\n\n\nThe pragmatic listener \\(L_{1}\\)\nFinally, recall that the pragmatic listener model uses the pragmatic speaker model as a representation of the likelihood of some utterance, given an intended inference. \\[\nP_{L_{1}}(w ∣ u) ∝ P_{S_{1}}(u ∣ w) × P (w)\n\\] Given that there is a uniform prior distribution over numbers of cookies, we may obtain probability distributions for the same three utterances by taking the table in the previous subsection and renormalizing its probabilities within rows.\n\n\n\n\\(w =\\)\n5\n6\n7\n\n\n\n\n\\(u = \\textit{Jo ate 5 cookies}\\)\n0.85\n0.14\n0.01\n\n\n\\(u = \\textit{Jo ate 6 cookies}\\)\n0\n0.93\n0.07\n\n\n\\(u = \\textit{Jo ate 7 cookies}\\)\n0\n0\n1\n\n\n\nNote that if we had a non-uniform prior over numbers of cookies—e.g., if 6 is more probable than 5 (classic Jo)—we can simply multiply the entries of this table by their prior probabilities and renormalize them within rows once again.\n\n\n\nRSA discussion\nAs noted earlier, RSA models come with a very appealing feature: that they provide a modular separation between semantic and pragmatic concerns. In particular, the \\(L_{0}\\) model can be seen as instantiating a semantic analysis of some utterance (which is, ideally, provided by some external theory of the semantics of utterances), while the \\(L_{1}\\) model can be seen as instantiating a pragmatic theory that is built up from the semantic theory in a fairly deterministic way (once, e.g., cost parameters, rationality parameters, and prior distributions over utterance alternatives are fixed). Indeed, such a separation can be methodologically useful, since it allows one to test particular semantic theories in the face of human inference data that arises from pragmatic (as well as other) factors (see, e.g., Waldon and Degen (2020) for discussion)\n\nChallenges\nThere is a particular set of challenges for RSA models, as they are typically stated, that we aim to address in this course. Namely, it is not super obvious what role the Montagovian notion of semantic compositionality can play. Note that the account of the literal listener \\(L_{0}\\) must come “from outside”: RSA models are typically defined on top of analyses of sentence meaning, as opposed to the meanings of basic expressions, though the latter are presumably implicated in deriving the former. Thus there are certain questions about semantic compositionality which such models don’t address:\n\nHow may the semantics of individual expressions be studied in tandem with their pragmatic effects? How should such pragmatic effects be formally encoded in lexical meaning representations?\nHow do pragmatic effects compose, in order to yield the global pragmatic effects associated with entire utterances?\n\nOne of the aims of this course is to provide a framework in which the pragmatic effects of individual expressions may be stated and composed, and then tested against human inference data.\n\n\n\n\n\n\n\n\nReferences\n\nDegen, Judith. 2023. “The Rational Speech Act Framework.” Annual Review of Linguistics 9 (Volume 9, 2023): 519–40. https://doi.org/10.1146/annurev-linguistics-031220-010811.\n\n\nFrank, Michael C., and Noah D. Goodman. 2012. “Predicting Pragmatic Reasoning in Language Games.” Science 336 (6084): 998–98. https://doi.org/10.1126/science.1218633.\n\n\nGoodman, Noah D., and Andreas Stuhlmüller. 2013. “Knowledge and Implicature: Modeling Language Understanding as Social Cognition.” Topics in Cognitive Science 5 (1): 173–84. https://doi.org/10.1111/tops.12007.\n\n\nGrice, H. Paul. 1975. “Logic and Conversation.” In Syntax and Semantics, edited by Peter Cole and Jerry L. Morgan, 3, Speech Acts:41–58. New York: Academic Press.\n\n\nWaldon, Brandon, and Judith Degen. 2020. “Modeling Behavior in Truth Value Judgment Task Experiments.” In Proceedings of the Society for Computation in Linguistics 2020, edited by Allyson Ettinger, Gaja Jarosz, and Joe Pater, 238–47. New York, New York: Association for Computational Linguistics. https://aclanthology.org/2020.scil-1.29/."
  },
  {
    "objectID": "background/experimental-turn.html",
    "href": "background/experimental-turn.html",
    "title": "The experimental turn",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\updct}[1]{\\ct{upd\\_#1}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\newcommand{\\pure}[1]{\\bbox[border: 1px solid orange]{\\bbox[border: 4px solid transparent]{#1}}}\n\\newcommand{\\return}[1]{\\bbox[border: 1px solid black]{\\bbox[border: 4px solid transparent]{#1}}}\n\\def\\P{\\mathtt{P}}\n\\def\\Q{\\mathtt{Q}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\def\\ite{\\ct{if\\_then\\_else}}\n\\def\\Do{\\abbr{do}}\n\\]\n\nThe traditional methodology’s success has created a foundation solid enough to support exciting new extensions. Experimental semantics brings the tools of behavioral experimentation to bear on questions about meaning, allowing us to test and refine theoretical insights at unprecedented scale.\n\nScaling Semantic Investigation\nWhere traditional methods might examine a handful of predicates, experimental approaches can investigate entire lexical domains. Extending our example involving the verb love: English has thousands of similar clause-embedding predicates, each potentially varying in its inferential properties. We can now test whether generalizations based on canonical examples extend across these vast lexicons.\nThe MegaAttitude project (White and Rawlins 2016, 2018, 2020; White et al. 2018; An and White 2020; Moon and White 2020; Kane, Gantt, and White 2022) is one example of this approach. This project aims to collect inference judgments for hundreds of predicates across multiple contexts and inference types. This scale reveals patterns that are very difficult to see and evaluate the quality of using traditional methods—subtle distinctions between near-synonyms, unexpected predicate clusters, and systematic variation across semantic domains.\n\n\nTeasing Apart Contributing Factors\nExperimental methods also allow us to investigate the rich array of factors that influence inference judgments:\n\nSemantic knowledge: The core meanings of expressions\nWorld knowledge: Prior beliefs about plausibility\n\nContextual factors: The discourse context and QUD\nIndividual differences: Variation in how speakers interpret expressions\nResponse strategies: How participants use rating scales\n\nRather than viewing these as confounds, we can see them as windows into the cognitive processes underlying semantic interpretation. For instance, Degen and Tonhauser (2021) systematically manipulated world knowledge to show how prior beliefs modulate the strength of factive inferences, revealing the interplay between semantic and pragmatic factors.\n\n\nMaking Linking Hypotheses Explicit\nPerhaps most importantly, experimental approaches force us to make explicit what traditional methods leave implicit: the link between semantic representations and behavioral responses (Jasbi, Waldon, and Degen 2019; Waldon and Degen 2020; Phillips et al. 2021). When we say speakers judge that an inference follows, what cognitive processes produce that judgment? How do abstract semantic representations map onto the responses on some scale?\nThis is not merely a methodological detail—it’s a substantive theoretical question. Different linking hypotheses make different predictions about response patterns, allowing us to test not just our semantic theories but our assumptions about how those theories connect to behavior. Even if our real interest is in characterizing the semantic representations of speakers, we can’t ignore the way those representations map onto their responses in some task.\n\n\n\n\n\n\n\nReferences\n\nAn, Hannah, and Aaron White. 2020. “The Lexical and Grammatical Sources of Neg-Raising Inferences.” Proceedings of the Society for Computation in Linguistics 3 (1): 220–33. https://doi.org/https://doi.org/10.7275/yts0-q989.\n\n\nDegen, Judith, and Judith Tonhauser. 2021. “Prior Beliefs Modulate Projection.” Open Mind 5 (September): 59–70. https://doi.org/10.1162/opmi_a_00042.\n\n\nJasbi, Masoud, Brandon Waldon, and Judith Degen. 2019. “Linking Hypothesis and Number of Response Options Modulate Inferred Scalar Implicature Rate.” Frontiers in Psychology 10 (February). https://doi.org/10.3389/fpsyg.2019.00189.\n\n\nKane, Benjamin, Will Gantt, and Aaron Steven White. 2022. “Intensional Gaps: Relating Veridicality, Factivity, Doxasticity, Bouleticity, and Neg-Raising.” Semantics and Linguistic Theory 31 (0): 570–605. https://doi.org/10.3765/salt.v31i0.5137.\n\n\nMoon, Ellise, and Aaron White. 2020. “The Source of Nonfinite Temporal Interpretation.” In Proceedings of the 50th Annual Meeting of the North East Linguistic Society, edited by Mariam Asatryan, Yixiao Song, and Ayana Whitmal, 3:11–24. Amherst: GLSA Publications.\n\n\nPhillips, Colin, Phoebe Gaston, Nick Huang, and Hanna Muller. 2021. “Theories All the Way Down: Remarks on ‘Theoretical’ and ‘Experimental’ Linguistics.” In The Cambridge Handbook of Experimental Syntax, edited by Grant Goodall, 587–616. Cambridge Handbooks in Language and Linguistics. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108569620.023.\n\n\nWaldon, Brandon, and Judith Degen. 2020. “Modeling Behavior in Truth Value Judgment Task Experiments.” In Proceedings of the Society for Computation in Linguistics 2020, edited by Allyson Ettinger, Gaja Jarosz, and Joe Pater, 238–47. New York, New York: Association for Computational Linguistics. https://aclanthology.org/2020.scil-1.29/.\n\n\nWhite, Aaron Steven, and Kyle Rawlins. 2016. “A Computational Model of S-Selection.” Semantics and Linguistic Theory 26 (0): 641–63. https://doi.org/10.3765/salt.v26i0.3819.\n\n\n———. 2018. “The Role of Veridicality and Factivity in Clause Selection.” In NELS 48: Proceedings of the Forty-Eighth Annual Meeting of the North East Linguistic Society, edited by Sherry Hucklebridge and Max Nelson, 48:221–34. University of Iceland: GLSA (Graduate Linguistics Student Association), Department of Linguistics, University of Massachusetts.\n\n\n———. 2020. “Frequency, Acceptability, and Selection: A Case Study of Clause-Embedding.” Glossa: A Journal of General Linguistics 5 (1). https://doi.org/10.5334/gjgl.1001.\n\n\nWhite, Aaron Steven, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme. 2018. “Lexicosyntactic Inference in Neural Models.” In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 4717–24. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/D18-1501.",
    "crumbs": [
      "Background",
      "The experimental turn"
    ]
  },
  {
    "objectID": "background/formal-pragmatics.html",
    "href": "background/formal-pragmatics.html",
    "title": "Formal pragmatics",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\updct}[1]{\\ct{upd\\_#1}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\newcommand{\\pure}[1]{\\bbox[border: 1px solid orange]{\\bbox[border: 4px solid transparent]{#1}}}\n\\newcommand{\\return}[1]{\\bbox[border: 1px solid black]{\\bbox[border: 4px solid transparent]{#1}}}\n\\def\\P{\\mathtt{P}}\n\\def\\Q{\\mathtt{Q}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\def\\ite{\\ct{if\\_then\\_else}}\n\\def\\Do{\\abbr{do}}\n\\]",
    "crumbs": [
      "Background",
      "Formal pragmatics"
    ]
  },
  {
    "objectID": "background/formal-pragmatics.html#footnotes",
    "href": "background/formal-pragmatics.html#footnotes",
    "title": "Formal pragmatics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThus for the purposes of the example, we’re eschewing the variable/register-based systems of Groenendijk and Stokhof (1991) and Muskens (1996).↩︎",
    "crumbs": [
      "Background",
      "Formal pragmatics"
    ]
  },
  {
    "objectID": "background/setting-stage.html",
    "href": "background/setting-stage.html",
    "title": "Setting the stage",
    "section": "",
    "text": "To round out this set of notes, we give a brief overview of the main desiderata which we aim to have PDS satisfy. These are the following:\n\ncompositionality: models of inference should be derived compositionally from semantic grammar fragments.\nmodularity: factors affecting inference judgments should be theorized about independently and combined.\nabstraction: models of meaning and inference should be statable abstractly, without reference to implementation.\n\nWe say a little more about these here, in turn.\n\nCompositionality for models\nWhat could it mean for models of linguistic inference (e.g., as represented in a judgment dataset) to be compositional? Our basic basic strategy is to build the distributional assumptions associated with, e.g., a mixed-effects model, into the semantics. This way, when basic meanings compose, so do these distributional assumptions.\nIndeed, it is reasonable to ask what such distributional assumptions represent. Our answer is uncertainty; specifically, uncertainty about which particular inferences are licensed on any given occasion of language use. Importantly, these kinds of assumptions may be combined when determining the meanings of complex expressions from the meanings of the more basic expressions they contain. Thus while the meaning of a sentence such as Jo laughs might be determined compositionally as\n\\[\n⟦\\textit{jo laughs}⟧ = ⟦\\textit{laughs}⟧ ▹ ⟦\\textit{jo}⟧ = laughs(j)\n\\]\nwithin a traditional semantic framework, PDS, instead, compositionally associates this sentence with a probability distribution.\n\\[⟦\\textit{jo laughs}⟧ = ⟦\\textit{laughs}⟧ ▹ ⟦\\textit{jo}⟧ = \\begin{array}[t]{l}\nj ∼ JoDistr \\\\\nlaugh ∼ LaughDistr \\\\\nReturn (laugh(j))\n\\end{array}\n\\]\nThis distribution is a distribution over truth values: it assigns some probability \\(p\\) to True and \\(1 - p\\) to False. Moreover, it is determined by certain sampling statements (whose interpretations we will formally define tomorrow). Informally, the meaning of Jo takes some distribution over entities, while the meaning of laughs takes some distribution over functions from entities to truth values. These distributions may then be combined to yield a distribution over values gotten by applying such functions applied to such entities; i.e., a distribution over truth values. Remarkably, the distributions over the meanings of such basic expressions end up corresponding exactly, within PDS, to the parameters of some hierarchical Bayesian (e.g., mixed-effects) model which may be used to fit human inference judgment data.\n\n\nModularity\nWe also want theories constructed within PDS to be modular. Specifically, we want the factors affecting inference to be able to be theorized about independently and combined. These include:\n\nlexical and compositional semantics\nworld knowledge\nresponse behavior: how does someone use a testing instrument (e.g., slider scale)?\n\nAn upshot of this feature is that PDS can have different uses. For example, one could swap out a model of response behavior for a model of likely utterances (perhaps, \\(S_{1}\\)).\n\n\nAbstraction\nFinally, we want such theories to display a certain amount of abstraction. That is, we should be able to state models of inference judgment data that:\n\ndescribe probability distributions,\ndo not concern themselves with how distributions are computed.\n\nThere are a couple useful consequences of this feature. First, it allows traditional semantic theories to be plugged into PDS rather seamlessly. Second, it allows separation between theories stated within PDS and model stated within those thoeries. This second consequence allows:\n\nAllows flexibility about implementation.\nAllows the theory to be simpler.\nAllows seamless integration between formal semantics and probabilistic semantics. (More tomorrow!)",
    "crumbs": [
      "Background",
      "Setting the stage"
    ]
  },
  {
    "objectID": "pds-intro/pds-intro.html",
    "href": "pds-intro/pds-intro.html",
    "title": "Introdution to probabilitsic dynamic semantics",
    "section": "",
    "text": "Note\n\n\n\nThis module will become available on August 5 and 6.",
    "crumbs": [
      "Introduction to probabilistic dynamic semantics",
      "Introdution to probabilitsic dynamic semantics"
    ]
  }
]