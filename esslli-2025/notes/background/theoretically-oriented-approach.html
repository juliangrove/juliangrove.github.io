<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Rational Speech Act models – Probabilistic dynamic semantics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-11cbd3234f06388db87dcfbb7d560d62.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-3c5b3ed224f457cdcbda003fac2adf13.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">
      Probabilistic dynamic semantics
      </li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Probabilistic dynamic semantics</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probabilistic dynamic semantics</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Background</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../background/theory-to-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From theory to data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../background/formal-pragmatics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Formal pragmatics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../background/experimental-turn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The experimental turn</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../background/understanding-gradience.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Understanding gradience</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../background/case-studies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Two case studies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../background/new-frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The need for new frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../background/rsa.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rational Speech Act models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../background/setting-stage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setting the stage</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Introduction to probabilistic dynamic semantics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pds-intro/pds-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introdution to probabilitsic dynamic semantics</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Gradable adjectives</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../adjectives/adjectives.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Gradable adjectives</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Factivity inferences</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../factivity/factivity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Factivity</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#an-example" id="toc-an-example" class="nav-link active" data-scroll-target="#an-example">An example</a></li>
  <li><a href="#rsa-discussion" id="toc-rsa-discussion" class="nav-link" data-scroll-target="#rsa-discussion">RSA discussion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Rational Speech Act models</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Rational speech act (RSA) models are a very popular approach to modeling pragmatic inference that integrates ideas from formal semantics into mathematically explicit models of Gricean reasoning <span class="citation" data-cites="grice_logic_1975">(<a href="#ref-grice_logic_1975" role="doc-biblioref">Grice 1975</a>)</span>. Crucially, these models aim for a certain kind of <em>modularity</em>: they allow one to provide separate accounts of the literal semantics of expressions, on the one hand, and the inferences that people make when they encounter utterances of these expressions, on the other. They achieve this kind of modularity, essentially, by allowing one to state a theory of literal meaning and then to use a systematic recipe for turning it into a theory of pragmatic inference.</p>
<p>Here we describe what is sometimes called <em>vanilla</em> RSA. Vanilla RSA is RSA more or less as it was originally formulated <span class="citation" data-cites="frank_predicting_2012">Frank and Goodman (<a href="#ref-frank_predicting_2012" role="doc-biblioref">2012</a>)</span> and <span class="citation" data-cites="goodman_knowledge_2013">Goodman and Stuhlmüller (<a href="#ref-goodman_knowledge_2013" role="doc-biblioref">2013</a>)</span> (see <span class="citation" data-cites="degen_rational_2023">Degen (<a href="#ref-degen_rational_2023" role="doc-biblioref">2023</a>)</span> for a recent comprehensive overview of the RSA literature). The basic idea is that there are two sets of models, <em>listener</em> models, and <em>speaker</em> models, which are kind of mirror images of each other.</p>
<section id="listener-models" class="level4">
<h4 class="anchored" data-anchor-id="listener-models">Listener models</h4>
<p>In particular, any given listener model <span class="math inline">\(L_{i}\)</span> characterizes a probability distribution over possible worlds <span class="math inline">\(w\)</span>, given some utterance <span class="math inline">\(u\)</span>. <span class="math display">\[
\begin{aligned}
P_{L_0}(w | u) &amp;= \frac{\begin{cases}
P_{L_0}(w) &amp; ⟦u⟧^w = \mathtt{T} \\
0 &amp; ⟦u⟧^w = \mathtt{F}
\end{cases}}{∑_{w^\prime}\begin{cases}
P_{L_0}(w^\prime) &amp; ⟦u⟧^{w^\prime} = \mathtt{T} \\
0 &amp; ⟦u⟧^{w^\prime} = \mathtt{F}
\end{cases}} \\[2mm]
P_{L_i}(w | u) &amp;= \frac{P_{L_i}(u | w) * P_{L_i}(w)}{∑_{w^\prime}P_{L_i}(u | w^\prime) *
P_{L_i}(w^\prime)}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(i &gt; 0) \\[2mm]
&amp; = \frac{P_{L_i}(u | w) * P_{L_i}(w)}{P_{L_i}(u)}
\end{aligned}
\]</span> In words, <span class="math inline">\(P_{L_0}(w | u)\)</span> depends only on whether or not <span class="math inline">\(u\)</span> and <span class="math inline">\(w\)</span> are compatible. It thus acts as a <em>filter</em>, eliminating possible worlds from the prior in which the utterance <span class="math inline">\(u\)</span> is false.</p>
<p>The definition of <span class="math inline">\(P_{L_i}(w | u)\)</span> for <span class="math inline">\(i &gt; 0\)</span> uses <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ theorem</a>. To state that this definition <em>uses</em> Bayes’ theorem is kind of a tautology when viewed simply as a mathematical description. Thus what this statement really means is something more operational: RSA models make distinguishing choices about the definitions of <span class="math inline">\(P_{L_i}(u | w)\)</span> and <span class="math inline">\(P_{L_i}(w)\)</span>, and it is these latter choices which are used, in turn, to compute <span class="math inline">\(P_{L_i}(w | u)\)</span>.</p>
<p>In general, the choice <span class="math inline">\(P_{L_i}(w)\)</span> of a prior distribution over <span class="math inline">\(w\)</span> is made once and for all, regardless of the particular model, so we can just call this choice <span class="math inline">\(P(w)\)</span>. <span class="math inline">\(P(w)\)</span> can be seen to give a representation of the <em>context set</em> in a given discourse; that is, the distribution over possible worlds known in common among the interlocutors, before anything is uttered.</p>
</section>
<section id="speaker-models" class="level4">
<h4 class="anchored" data-anchor-id="speaker-models">Speaker models</h4>
<p>The definition of <span class="math inline">\(P_{L_i}(u | w)\)</span>, on the other hand, is chosen to reflect the model <span class="math inline">\(S_i\)</span> of the <em>speaker</em>, which brings us to the <em>other</em> set of models. Thus <span class="math inline">\(P_{L_i}(u | w) = P_{S_i}(u | w)\)</span>, where <span class="math display">\[\begin{aligned}
P_{S_i}(u | w) &amp;= \frac{e^{α * 𝕌_{S_i}(u; w)}}{∑_{u^\prime}e^{α *
𝕌_{S_i}(u^\prime; w)}}
\end{aligned}\]</span> <span class="math inline">\(𝕌_{S_i}(u; w)\)</span> is the <em>utility</em> <span class="math inline">\(S_i\)</span> assigns to the utterance <span class="math inline">\(u\)</span>, given its intention to communicate the world <span class="math inline">\(w\)</span>. Utility for <span class="math inline">\(S_i\)</span> is typically defined as <span class="math display">\[𝕌_{S_i}(u; w) = ln(P_{L_{i-1}}(w | u)) - C(u)\]</span> that is, the natural log of the probability that <span class="math inline">\(L_{i-1}\)</span> assigns to <span class="math inline">\(w\)</span> (given <span class="math inline">\(u\)</span>), minus <span class="math inline">\(u\)</span>’s cost (<span class="math inline">\(C(u)\)</span>). <span class="math inline">\(α\)</span> is known as the <em>temperature</em> (or the <em>rationality parameter</em>) associated with <span class="math inline">\(S_i\)</span>. When <span class="math inline">\(α = 0\)</span>, <span class="math inline">\(S_i\)</span> chooses utterances randomly (from a uniform distribution), without attending to their utility in communicating <span class="math inline">\(w\)</span>. When <span class="math inline">\(α\)</span> tends toward <span class="math inline">\(∞\)</span>, <span class="math inline">\(S_i\)</span> becomes more and more deterministic in its choice of utterance, assigning more and more probability mass to the utterance that maximizes utility in communicating <span class="math inline">\(w\)</span>. A little more formally, <span class="math display">\[\lim_{α → ∞}\frac{e^{α * 𝕌_{S_i}(u; w)}}{∑_{u^\prime}e^{α *
𝕌_{S_i}(u^\prime; w)}} = \begin{cases}
1 &amp; u = \arg\max_{u^\prime}(𝕌_{S_i}(u^\prime; w)) \\
0 &amp; u ≠ \arg\max_{u^\prime}(𝕌_{S_i}(u^\prime; w))
\end{cases}\]</span> Because the cost <span class="math inline">\(C(u)\)</span> only depends on <span class="math inline">\(u\)</span>, it is nice to view <span class="math inline">\(e^{α * 𝕌_{S_i}(u; w)}\)</span> as factored into a prior and (something like) a likelihood, so that <span class="math inline">\(P_{S_i}(u | w)\)</span> has a formulation symmetrical to that of <span class="math inline">\(P_{L_i}(w | u)\)</span> (when <span class="math inline">\(i &gt; 0\)</span>); that is, it can be formulated in the following way: <span class="math display">\[\begin{aligned}
e^{α * 𝕌_{S_i}(u; w)} &amp;= P_{L_{i - 1}}(w | u)^α * \frac{1}{e^{α * C(u)}}
\\[2mm] &amp;∝ P_{S_i}(w | u) * P_{S_i}(u) \\[2mm]
&amp;= P_{S_i}(w | u) * P(u)
\end{aligned}\]</span> In effect, we can define <span class="math inline">\(P_{S_i}(w | u)\)</span>, viewed as a function of <span class="math inline">\(u\)</span>, to be proportional to <span class="math inline">\(P_{L_{i - 1}}(w | u)^α\)</span>; meanwhile, we can define <span class="math inline">\(P(u)\)</span>, the prior probability over utterances, to be proportional to <span class="math inline">\(\frac{1}{e^{α * C(u)}}\)</span>. (Note that if we ignore cost altogether, so that <span class="math inline">\(C(u)\)</span> is always, say, 0, then <span class="math inline">\(P(u)\)</span> just becomes a uniform distribution.)</p>
<p>Taking these points into consideration, we may reformulate our speaker model, <span class="math inline">\(S_i\)</span>, as follows: <span class="math display">\[\begin{aligned}
P_{S_i}(u | w) &amp;= \frac{P_{S_i}(w | u) * P(u)}{∑_{u^\prime}P_{S_i}(w |
u^\prime) * P(u^\prime)} \\[2mm]
&amp;= \frac{P_{S_i}(w | u) * P(u)}{P_{S_i}(w)}
\end{aligned}\]</span> In words, the speaker model, just like the listener model, may be viewed operationally in terms of Bayes’ theorem. Note that <span class="math inline">\(P_{S_i}(w)\)</span>, in general, defines a different distribution from <span class="math inline">\(P(w)\)</span>, the listener’s prior distribution over worlds (i.e., the context set). The former represents, not prior knowledge about the <em>context</em>, but rather something more like the relative “communicability” of a given possible world, given the distribution <span class="math inline">\(P(u)\)</span> over utterances; that is, how likely a random utterance makes <span class="math inline">\(w\)</span>, though with the exponential <span class="math inline">\(α\)</span> applied.</p>
</section>
<section id="an-example" class="level3">
<h3 class="anchored" data-anchor-id="an-example">An example</h3>
<p>An example helps illustrate how the probability distributions determined by RSA speaker and listener models are computed in practice. Let’s say there are seven cookies, as depicted in the image below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/7cookies.jpg" class="img-fluid figure-img" width="300"></p>
<figcaption>Cookies (7 of them)</figcaption>
</figure>
</div>
<p>Further, say someone utters the sentence <em>Jo ate five cookies</em>. We’ll assume that the literal meaning of such a sentence is lower bounded: it is true just in case the number cookies Jo ate is at least five, i.e., <span class="math display">\[
n_{\textit{cookies}} ≥ 5
\]</span> Let’s now consider the probability distributions computed by the models <span class="math inline">\(L_{0}\)</span>, <span class="math inline">\(S_{1}\)</span>, and <span class="math inline">\(L_{1}\)</span>, following the definitions given earlier.</p>
<section id="the-literal-listener-l_0" class="level4">
<h4 class="anchored" data-anchor-id="the-literal-listener-l_0">The literal listener <span class="math inline">\(L_{0}\)</span></h4>
<p>Recall that the literal listener is a <em>filter</em>: <span class="math display">\[
P_{L_{0}}(w ∣ u) ∝ 𝟙(w ≥ n) × P (w)
\]</span> Let’s also assume that <span class="math inline">\(P(w)\)</span>, the <em>prior</em> distribution over the number of cookies Jo ate is uniform. Then, the literal listener is simply zeroing out the portion of this prior distribution in which Jo ate <em>less than five</em> cookies and renormalizing the resulting distribution. The following table illustrates this for <em>Jo ate five cookies</em>, as well as two other utterances. Here, <span class="math inline">\(w\)</span> (the world) is identified with a possible inference; i.e., about how many cookies Jo actually ate.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(w =\)</span></th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(u = \textit{Jo ate 5 cookies}\)</span></td>
<td>1/3</td>
<td>1/3</td>
<td>1/3</td>
</tr>
<tr class="even">
<td><span class="math inline">\(u = \textit{Jo ate 6 cookies}\)</span></td>
<td>0</td>
<td>1/2</td>
<td>1/2</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(u = \textit{Jo ate 7 cookies}\)</span></td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Thus if <em>Jo ate five cookies</em> is uttered, <span class="math inline">\(L_{0}\)</span> assigns a probability of 1/3 to each of the possible inferences compatible with the utterance’s lower-bounded literal meaning.</p>
</section>
<section id="the-pragmatic-speaker-s_1" class="level4">
<h4 class="anchored" data-anchor-id="the-pragmatic-speaker-s_1">The pragmatic speaker <span class="math inline">\(S_{1}\)</span></h4>
<p>Here is the pragmatic speaker model again, reformulated (i) as a proportionality statement, and (ii) by moving the cost term into the denominator: <span class="math display">\[
P_{S_{1}}(u ∣ w) ∝ \frac{P_{L_{0}}(w ∣ u)^{α}}{e^{α × C(u)}}
\]</span> For the purposes of the example, let’s assume that the rationality parameter <span class="math inline">\(α = 4\)</span>, and that the cost <span class="math inline">\(C(u)\)</span> of an utterance is constant across utterances. Let’s further assume that the speaker is only considering the utterances listed in the following table; i.e., its prior distribution—given the constant cost function—is uniform over these alternatives. Then, we obtain the following distributions over <em>utterances</em> for three possible worlds corresponding to the inference which the speaker intends to communicate.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(w =\)</span></th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(u = \textit{Jo ate 5 cookies}\)</span></td>
<td>1</td>
<td>0.16</td>
<td>0.01</td>
</tr>
<tr class="even">
<td><span class="math inline">\(u = \textit{Jo ate 6 cookies}\)</span></td>
<td>0</td>
<td>0.84</td>
<td>0.06</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(u = \textit{Jo ate 7 cookies}\)</span></td>
<td>0</td>
<td>0</td>
<td>0.93</td>
</tr>
</tbody>
</table>
<p>Thus if <span class="math inline">\(S_{1}\)</span> wishes to convey that Jo ate exactly five cookies, it chooses the first utterance with a probability of 1. This is because the literal listener assigns 5 cookies a probability of 0 if one of the other two stronger sentences is uttered. Meanwhile, if it wishes to convey that Jo ate exactly six cookies, it chooses the first utterance with probability <span class="math inline">\(\frac{(1/3)^4}{(1/3)^4 + (1/2)^4} ≈ 0.16\)</span> and the second utterance with probability <span class="math inline">\(\frac{(1/2)^4}{(1/3)^4 + (1/2)^4} ≈ 0.84\)</span>. Crucially, we see that probabilities are normalized within <em>columns</em> of the table, rather than rows, as is the case for the listener models.</p>
</section>
<section id="the-pragmatic-listener-l_1" class="level4">
<h4 class="anchored" data-anchor-id="the-pragmatic-listener-l_1">The pragmatic listener <span class="math inline">\(L_{1}\)</span></h4>
<p>Finally, recall that the pragmatic listener model uses the pragmatic speaker model as a representation of the likelihood of some utterance, given an intended inference. <span class="math display">\[
P_{L_{1}}(w ∣ u) ∝ P_{S_{1}}(u ∣ w) × P (w)
\]</span> Given that there is a uniform prior distribution over numbers of cookies, we may obtain probability distributions for the same three utterances by taking the table in the previous subsection and renormalizing its probabilities within rows.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(w =\)</span></th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(u = \textit{Jo ate 5 cookies}\)</span></td>
<td>0.85</td>
<td>0.14</td>
<td>0.01</td>
</tr>
<tr class="even">
<td><span class="math inline">\(u = \textit{Jo ate 6 cookies}\)</span></td>
<td>0</td>
<td>0.93</td>
<td>0.07</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(u = \textit{Jo ate 7 cookies}\)</span></td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Note that if we had a non-uniform prior over numbers of cookies—e.g., if 6 is more probable than 5 (classic Jo)—we can simply multiply the entries of this table by their prior probabilities and renormalize them within rows once again.</p>
</section>
</section>
<section id="rsa-discussion" class="level3">
<h3 class="anchored" data-anchor-id="rsa-discussion">RSA discussion</h3>
<p>As noted earlier, RSA models come with a very appealing feature: that they provide a modular separation between semantic and pragmatic concerns. In particular, the <span class="math inline">\(L_{0}\)</span> model can be seen as instantiating a semantic analysis of some utterance (which is, ideally, provided by some external theory of the semantics of utterances), while the <span class="math inline">\(L_{1}\)</span> model can be seen as instantiating a pragmatic theory that is built up from the semantic theory in a fairly deterministic way (once, e.g., cost parameters, rationality parameters, and prior distributions over utterance alternatives are fixed). Indeed, such a separation can be methodologically useful, since it allows one to <em>test</em> particular semantic theories in the face of human inference data that arises from pragmatic (as well as other) factors (see, e.g., <span class="citation" data-cites="waldon_modeling_2020">Waldon and Degen (<a href="#ref-waldon_modeling_2020" role="doc-biblioref">2020</a>)</span> for discussion)</p>
<section id="challenges" class="level4">
<h4 class="anchored" data-anchor-id="challenges">Challenges</h4>
<p>There is a particular set of challenges for RSA models, as they are typically stated, that we aim to address in this course. Namely, it is not super obvious what role the Montagovian notion of semantic compositionality can play. Note that the account of the literal listener <span class="math inline">\(L_{0}\)</span> must come “from outside”: RSA models are typically defined on top of analyses of sentence meaning, as opposed to the meanings of basic expressions, though the latter are presumably implicated in deriving the former. Thus there are certain questions about semantic compositionality which such models don’t address:</p>
<ul>
<li>How may the semantics of individual expressions be studied in tandem with their pragmatic effects? How should such pragmatic effects be formally encoded in lexical meaning representations?</li>
<li>How do pragmatic effects <em>compose</em>, in order to yield the global pragmatic effects associated with entire utterances?</li>
</ul>
<p>One of the aims of this course is to provide a framework in which the pragmatic effects of individual expressions may be stated and composed, and then tested against human inference data.</p>


<!-- -->


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-degen_rational_2023" class="csl-entry" role="listitem">
Degen, Judith. 2023. <span>“The <span>Rational</span> <span>Speech</span> <span>Act</span> <span>Framework</span>.”</span> <em>Annual Review of Linguistics</em> 9 (Volume 9, 2023): 519–40. <a href="https://doi.org/10.1146/annurev-linguistics-031220-010811">https://doi.org/10.1146/annurev-linguistics-031220-010811</a>.
</div>
<div id="ref-frank_predicting_2012" class="csl-entry" role="listitem">
Frank, Michael C., and Noah D. Goodman. 2012. <span>“Predicting <span>Pragmatic</span> <span>Reasoning</span> in <span>Language</span> <span>Games</span>.”</span> <em>Science</em> 336 (6084): 998–98. <a href="https://doi.org/10.1126/science.1218633">https://doi.org/10.1126/science.1218633</a>.
</div>
<div id="ref-goodman_knowledge_2013" class="csl-entry" role="listitem">
Goodman, Noah D., and Andreas Stuhlmüller. 2013. <span>“Knowledge and Implicature: Modeling Language Understanding as Social Cognition.”</span> <em>Topics in Cognitive Science</em> 5 (1): 173–84. <a href="https://doi.org/10.1111/tops.12007">https://doi.org/10.1111/tops.12007</a>.
</div>
<div id="ref-grice_logic_1975" class="csl-entry" role="listitem">
Grice, H. Paul. 1975. <span>“Logic and <span>Conversation</span>.”</span> In <em>Syntax and <span>Semantics</span></em>, edited by Peter Cole and Jerry L. Morgan, 3, Speech Acts:41–58. New York: Academic Press.
</div>
<div id="ref-waldon_modeling_2020" class="csl-entry" role="listitem">
Waldon, Brandon, and Judith Degen. 2020. <span>“Modeling <span>Behavior</span> in <span>Truth</span> <span>Value</span> <span>Judgment</span> <span>Task</span> <span>Experiments</span>.”</span> In <em>Proceedings of the <span>Society</span> for <span>Computation</span> in <span>Linguistics</span> 2020</em>, edited by Allyson Ettinger, Gaja Jarosz, and Joe Pater, 238–47. New York, New York: Association for Computational Linguistics. <a href="https://aclanthology.org/2020.scil-1.29/">https://aclanthology.org/2020.scil-1.29/</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Rational Speech Act models"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> ../../pds.bib</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    css: ../styles.css</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>Rational speech act (RSA) models are a very popular approach to modeling pragmatic inference that integrates ideas from formal semantics into mathematically explicit models of Gricean reasoning <span class="co">[</span><span class="ot">@grice_logic_1975</span><span class="co">]</span>.</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>Crucially, these models aim for a certain kind of *modularity*:</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>they allow one to provide separate accounts of the literal semantics of expressions, on the one hand, and the inferences that people make when they encounter utterances of these expressions, on the other.</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>They achieve this kind of modularity, essentially, by allowing one to state a theory of literal meaning and then to use a systematic recipe for turning it into a theory of pragmatic inference.</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>Here we describe what is sometimes called *vanilla* RSA.</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>Vanilla RSA is RSA more or less as it was originally formulated @frank_predicting_2012 and @goodman_knowledge_2013 (see @degen_rational_2023 for a recent comprehensive overview of the RSA literature).</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>The basic idea is that there are two sets of models, *listener* models, and *speaker* models, which are kind of mirror images of each other.</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Listener models</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>In particular, any given listener model $L_{i}$ characterizes a probability distribution over possible worlds $w$, given some utterance $u$.</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>P_{L_0}(w | u) &amp;= \frac{\begin{cases}</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>P_{L_0}(w) &amp; ⟦u⟧^w = \mathtt{T} <span class="sc">\\</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>0 &amp; ⟦u⟧^w = \mathtt{F}</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>\end{cases}}{∑_{w^\prime}\begin{cases}</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>P_{L_0}(w^\prime) &amp; ⟦u⟧^{w^\prime} = \mathtt{T} <span class="sc">\\</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>0 &amp; ⟦u⟧^{w^\prime} = \mathtt{F}</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>\end{cases}} <span class="sc">\\</span><span class="co">[</span><span class="ot">2mm</span><span class="co">]</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>P_{L_i}(w | u) &amp;= \frac{P_{L_i}(u | w) * P_{L_i}(w)}{∑_{w^\prime}P_{L_i}(u | w^\prime) *</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>P_{L_i}(w^\prime)}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(i &gt; 0) <span class="sc">\\</span><span class="co">[</span><span class="ot">2mm</span><span class="co">]</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{P_{L_i}(u | w) * P_{L_i}(w)}{P_{L_i}(u)}</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>In words, $P_{L_0}(w | u)$ depends only on whether or not $u$ and $w$ are compatible.</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>It thus acts as a *filter*, eliminating possible worlds from the prior in which the utterance $u$ is false.</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>The definition of $P_{L_i}(w | u)$ for $i &gt; 0$ uses <span class="co">[</span><span class="ot">Bayes' theorem</span><span class="co">](https://en.wikipedia.org/wiki/Bayes%27_theorem)</span>.</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>To state that this definition *uses* Bayes' theorem is kind of a tautology when viewed simply as a mathematical description.</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>Thus what this statement really means is something more operational:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>RSA models make distinguishing choices about the definitions of $P_{L_i}(u | w)$ and $P_{L_i}(w)$, and it is these latter</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>choices which are used, in turn, to compute $P_{L_i}(w | u)$.</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>In general, the choice $P_{L_i}(w)$ of a prior distribution over $w$ is made once and for all, regardless of the particular model, so we can just call this choice $P(w)$.</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>$P(w)$ can be seen to give a representation of the *context set* in a given discourse;</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>that is, the distribution over possible worlds known in common among the interlocutors, before anything is uttered.</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Speaker models</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>The definition of $P_{L_i}(u | w)$, on the other hand, is chosen to reflect the model $S_i$ of the *speaker*, which brings us to the *other* set of</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>models.</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>Thus $P_{L_i}(u | w) = P_{S_i}(u | w)$, where</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>P_{S_i}(u | w) &amp;= \frac{e^{α * 𝕌_{S_i}(u; w)}}{∑_{u^\prime}e^{α *</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>𝕌_{S_i}(u^\prime; w)}}</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>$𝕌_{S_i}(u; w)$ is the *utility* $S_i$ assigns to the utterance $u$, given its intention to communicate the world $w$. Utility for $S_i$ is typically defined as</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>$$𝕌_{S_i}(u; w) = ln(P_{L_{i-1}}(w | u)) - C(u)$$</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>that is, the natural log of the probability that $L_{i-1}$ assigns to $w$ (given $u$), minus $u$'s cost ($C(u)$).</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>$α$ is known as the *temperature* (or the  *rationality parameter*) associated with $S_i$.</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>When $α = 0$, $S_i$ chooses utterances randomly (from a uniform distribution), without attending to their utility in communicating $w$.</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>When $α$ tends toward $∞$, $S_i$ becomes more and more deterministic in its choice of utterance, assigning more and more probability mass to the utterance that maximizes utility in communicating $w$.</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>A little more formally,</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>$$\lim_{α → ∞}\frac{e^{α * 𝕌_{S_i}(u; w)}}{∑_{u^\prime}e^{α *</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>𝕌_{S_i}(u^\prime; w)}} = \begin{cases}</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>1 &amp; u = \arg\max_{u^\prime}(𝕌_{S_i}(u^\prime; w)) <span class="sc">\\</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>0 &amp; u ≠ \arg\max_{u^\prime}(𝕌_{S_i}(u^\prime; w))</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>\end{cases}$$</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>Because the cost $C(u)$ only depends on $u$, it is nice to view $e^{α * 𝕌_{S_i}(u; w)}$ as factored into a prior and (something like) a likelihood, so that $P_{S_i}(u | w)$ has a formulation symmetrical to that of $P_{L_i}(w | u)$ (when $i &gt; 0$); that is, it can be formulated in the following way:</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>e^{α * 𝕌_{S_i}(u; w)} &amp;= P_{L_{i - 1}}(w | u)^α * \frac{1}{e^{α * C(u)}}</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span><span class="co">[</span><span class="ot">2mm</span><span class="co">]</span> &amp;∝ P_{S_i}(w | u) * P_{S_i}(u) <span class="sc">\\</span><span class="co">[</span><span class="ot">2mm</span><span class="co">]</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>&amp;= P_{S_i}(w | u) * P(u)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>In effect, we can define $P_{S_i}(w | u)$, viewed as a function of $u$, to be proportional to $P_{L_{i - 1}}(w | u)^α$; meanwhile, we can define $P(u)$, the prior probability over utterances, to be proportional to $\frac{1}{e^{α * C(u)}}$. (Note that if we ignore cost altogether, so that $C(u)$ is always, say, 0, then $P(u)$ just becomes a uniform distribution.)</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>Taking these points into consideration, we may reformulate our speaker model, $S_i$, as follows:</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>P_{S_i}(u | w) &amp;= \frac{P_{S_i}(w | u) * P(u)}{∑_{u^\prime}P_{S_i}(w |</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>u^\prime) * P(u^\prime)} <span class="sc">\\</span><span class="co">[</span><span class="ot">2mm</span><span class="co">]</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{P_{S_i}(w | u) * P(u)}{P_{S_i}(w)}</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>In words, the speaker model, just like the listener model, may be viewed operationally in terms of Bayes' theorem.</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>Note that $P_{S_i}(w)$, in general, defines a different distribution from $P(w)$, the listener's prior distribution over worlds (i.e., the context set).</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>The former represents, not prior knowledge about the *context*, but rather something more like the relative "communicability" of a given possible world, given the distribution $P(u)$ over utterances;</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>that is, how likely a random utterance makes $w$, though with the exponential $α$ applied.</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="fu">### An example</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>An example helps illustrate how the probability distributions determined by RSA speaker and listener models are computed in practice.</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>Let's say there are seven cookies, as depicted in the image below.</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a><span class="al">![Cookies (7 of them)](images/7cookies.jpg)</span>{width=300 fig-align="center"}</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>Further, say someone utters the sentence *Jo ate five cookies*.</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>We'll assume that the literal meaning of such a sentence is lower bounded:</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>it is true just in case the number cookies Jo ate is at least five, i.e., </span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>n_{\textit{cookies}} ≥ 5</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>Let's now consider the probability distributions computed by the models $L_{0}$, $S_{1}$, and $L_{1}$, following the definitions given earlier.</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The literal listener $L_{0}$</span></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>Recall that the literal listener is a *filter*:</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>P_{L_{0}}(w ∣ u) ∝ 𝟙(w ≥ n) × P (w)</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>Let's also assume that $P(w)$, the *prior* distribution over the number of cookies Jo ate is uniform.</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>Then, the literal listener is simply zeroing out the portion of this prior distribution in which Jo ate *less than five* cookies and renormalizing the resulting distribution.</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>The following table illustrates this for *Jo ate five cookies*, as well as two other utterances.</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>Here, $w$ (the world) is identified with a possible inference;</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>i.e., about how many cookies Jo actually ate.</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $w =$ <span class="pp">|</span> 5 <span class="pp">|</span> 6  <span class="pp">|</span>7 <span class="pp">|</span></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> - <span class="pp">|</span> - <span class="pp">|</span> - <span class="pp">|</span> - <span class="pp">|</span></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $u = \textit{Jo ate 5 cookies}$ <span class="pp">|</span> 1/3 <span class="pp">|</span> 1/3 <span class="pp">|</span> 1/3 <span class="pp">|</span></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $u = \textit{Jo ate 6 cookies}$ <span class="pp">|</span> 0 <span class="pp">|</span> 1/2 <span class="pp">|</span> 1/2 <span class="pp">|</span></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $u = \textit{Jo ate 7 cookies}$ <span class="pp">|</span> 0 <span class="pp">|</span> 0 <span class="pp">|</span> 1 <span class="pp">|</span></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>Thus if *Jo ate five cookies* is uttered, $L_{0}$ assigns a probability of 1/3 to each of the possible inferences compatible with the utterance's lower-bounded literal meaning.</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The pragmatic speaker $S_{1}$</span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>Here is the pragmatic speaker model again, reformulated (i) as a proportionality statement, and (ii) by moving the cost term into the denominator:</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>P_{S_{1}}(u ∣ w) ∝ \frac{P_{L_{0}}(w ∣ u)^{α}}{e^{α × C(u)}}</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>For the purposes of the example, let's assume that the rationality parameter $α = 4$, and that the cost $C(u)$ of an utterance is constant across utterances.</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>Let's further assume that the speaker is only considering the utterances listed in the following table;</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>i.e., its prior distribution---given the constant cost function---is uniform over these alternatives.</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>Then, we  obtain the following distributions over *utterances* for three possible worlds corresponding to the inference which the speaker intends to communicate.</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $w =$ <span class="pp">|</span> 5 <span class="pp">|</span> 6  <span class="pp">|</span>7 <span class="pp">|</span></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> - <span class="pp">|</span> - <span class="pp">|</span> - <span class="pp">|</span> - <span class="pp">|</span></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $u = \textit{Jo ate 5 cookies}$ <span class="pp">|</span> 1 <span class="pp">|</span> 0.16 <span class="pp">|</span> 0.01 <span class="pp">|</span></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $u = \textit{Jo ate 6 cookies}$ <span class="pp">|</span> 0 <span class="pp">|</span> 0.84 <span class="pp">|</span> 0.06 <span class="pp">|</span></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $u = \textit{Jo ate 7 cookies}$ <span class="pp">|</span> 0 <span class="pp">|</span> 0 <span class="pp">|</span> 0.93 <span class="pp">|</span></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>Thus if $S_{1}$ wishes to convey that Jo ate exactly five cookies, it chooses the first utterance with a probability of 1.</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>This is because the literal listener assigns 5 cookies a probability of 0 if one of the other two stronger sentences is uttered.</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>Meanwhile, if it wishes to convey that Jo ate exactly six cookies, it chooses the first utterance with probability $\frac{(1/3)^4}{(1/3)^4 + (1/2)^4} ≈ 0.16$ and the second utterance with probability $\frac{(1/2)^4}{(1/3)^4 + (1/2)^4} ≈ 0.84$.</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>Crucially, we see that probabilities are normalized within *columns* of the table, rather than rows, as is the case for the listener models.</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The pragmatic listener $L_{1}$</span></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>Finally, recall that the pragmatic listener model uses the pragmatic speaker model as a representation of the likelihood of some utterance, given an intended inference.</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>P_{L_{1}}(w ∣ u) ∝ P_{S_{1}}(u ∣ w) × P (w)</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>Given that there is a uniform prior distribution over numbers of cookies, we may obtain probability distributions for the same three utterances by taking the table in the previous subsection and renormalizing its probabilities within rows.</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $w =$ <span class="pp">|</span> 5 <span class="pp">|</span> 6  <span class="pp">|</span>7 <span class="pp">|</span></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> - <span class="pp">|</span> - <span class="pp">|</span> - <span class="pp">|</span> - <span class="pp">|</span></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $u = \textit{Jo ate 5 cookies}$ <span class="pp">|</span> 0.85 <span class="pp">|</span> 0.14 <span class="pp">|</span> 0.01 <span class="pp">|</span></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $u = \textit{Jo ate 6 cookies}$ <span class="pp">|</span> 0 <span class="pp">|</span> 0.93 <span class="pp">|</span> 0.07 <span class="pp">|</span></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $u = \textit{Jo ate 7 cookies}$ <span class="pp">|</span> 0 <span class="pp">|</span> 0 <span class="pp">|</span> 1 <span class="pp">|</span></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>Note that if we had a non-uniform prior over numbers of cookies---e.g., if 6 is more probable than 5 (classic Jo)---we can simply multiply the entries of this table by their prior probabilities and renormalize them within rows once again.</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a><span class="fu">### RSA discussion</span></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>As noted earlier, RSA models come with a very appealing feature:</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>that they provide a modular separation between semantic and pragmatic concerns.</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>In particular, the $L_{0}$ model can be seen as instantiating a semantic analysis of some utterance (which is, ideally, provided by some external theory of the semantics of utterances), while the $L_{1}$ model can be seen as instantiating a pragmatic theory that is built up from the semantic theory in a fairly deterministic way (once, e.g., cost parameters, rationality parameters, and prior distributions over utterance alternatives are fixed).</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>Indeed, such a separation can be methodologically useful, since it allows one to *test* particular semantic theories in the face of human inference data that arises from pragmatic (as well as other) factors (see, e.g., @waldon_modeling_2020 for discussion)</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Challenges</span></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>There is a particular set of challenges for RSA models, as they are typically stated, that we aim to address in this course.</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>Namely, it is not super obvious what role the Montagovian notion of semantic compositionality can play.</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>Note that the account of the literal listener $L_{0}$ must come "from outside":</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>RSA models are typically defined on top of analyses of sentence meaning, as opposed to the meanings of basic expressions, though the latter are presumably implicated in deriving the former.</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>Thus there are certain questions about semantic compositionality which such models don't address:</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How may the semantics of individual expressions be studied in tandem with their pragmatic effects? How should such pragmatic effects be formally encoded in lexical meaning representations?</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How do pragmatic effects *compose*, in order to yield the global pragmatic effects associated with entire utterances? </span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>One of the aims of this course is to provide a framework in which the pragmatic effects of individual expressions may be stated and composed, and then tested against human inference data.</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>