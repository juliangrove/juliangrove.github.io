[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site contains materials the course Introduction to Semantics given by Julian Grove at the University of Florida during the Fall 2025 semester.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#about-the-instructor",
    "href": "about.html#about-the-instructor",
    "title": "About",
    "section": "About the instructor",
    "text": "About the instructor\nJulian Grove is an Assistant Professor in the Linguistics Department. Long story short, he is interested in combining computational tools from Bayesian data analysis and programming language theory and bringing them to semantics. You can find out more about his research interests by checking out his papers, slides, or perhaps even his cv.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#about-the-site",
    "href": "about.html#about-the-site",
    "title": "About",
    "section": "About the site",
    "text": "About the site\nThe site itself is built using Quarto. The source files for this site are available on github at juliangrove/intro-semantics.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#license",
    "href": "about.html#license",
    "title": "About",
    "section": "License ",
    "text": "License \nIntroduction to Semantics by Julian Grove is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. Based on materials at https://github.com/juliangrove/intro-semantics.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "diagnosing_inference/entailments_conv_impl.html",
    "href": "diagnosing_inference/entailments_conv_impl.html",
    "title": "Entailments and presuppositions versus conversational implicatures",
    "section": "",
    "text": "The most prominent difference between entailments and presuppositions on the one hand, and conversational implicatures on the other, is that the latter tend to be defeasible, while the former tend not to be. Conversational implicatures are defined as arising from the maxims, after all—so you can always just claim that you weren’t actually following them!\n\nThe non-defeasability of entailments\n\n#Jo has at least three siblings, {and, but} she doesn’t have at least two siblings.\n#Jo is Bo’s sibling, {and, but} Jo and Bo aren’t siblings.\n#No one came to class, {and, but} it’s not the case that Julian didn’t come to class.\n\nEach of these examples seems to be weird and contradictory. That’s because an entailment of the first clause in each example is negated in the second clause. That is, Jo has at least three siblings entails Jo has at least two siblings; Jo is Bo’s sibling entails Jo and Bo are siblings; and no one came to class entails Julian didn’t come to class.\n\n\nThe non-defeasibility of presuppositions\nLike entailments, presuppositions also appear to be non-defeasible.\n\n#Jo stopped smoking, {and, but} she didn’t used to smoke.\n#Jo loves that it’s raining, {and, but} it’s not raining.\n#Bo failed the assignment, {and, but} there isn’t an assignment.\n#Bo failed the assignment again, {and, but} he hasn’t failed it before.\n\n\n\nThe defeasibility of conversational implicatures\n\nJo did some of the readings. In fact, she did all of them.\nI want something with lots of caffeine. In fact, I want a matcha latte.\nContext: Jo is writing a letter of recommendation for Bo for a linguistics instructor position.  Jo: Bo is very punctual. He has great handwriting. He kicked my ass in Hungry Hungry Hippos. Last, but not least, he’d be a great instructor in your linguistics program!\n\nThese examples illustrate that conversational implicatures, unlike entailments, may be canceled. In (8), the implicature that Jo did not do all of the readings is canceled; in (9), the ignorance implicature is canceled; and in (10), the implicature that Bo should not be hired as a linguistics instructor that arises when Jo flouts relation is canceled.",
    "crumbs": [
      "Diagnosing inference",
      "Entailments and presuppositions versus conversational implicatures"
    ]
  },
  {
    "objectID": "diagnosing_inference/overview.html",
    "href": "diagnosing_inference/overview.html",
    "title": "Overview",
    "section": "",
    "text": "The data we tend to be interested in in semantics is inference judgment data. We can view an inference as being a pair of expressions of the language (or utterances of such expressions)—\\(E_{1}\\) and \\(E_{2}\\)—where \\(E_{2}\\) is generally a declarative sentence. In that case, we can ask if using \\(E_{1}\\) triggers an inference that \\(E_{2}\\) is true. If it does, then we say that \\(E_{2}\\) is an inference of \\(E_{1}\\) (or of an utterance of \\(E_{1}\\)).\nThere are four kinds of inference that we’d like to be able to classify. These are:\n\nentailments\nconversational implicatures\npresuppositions\nconventional implicatures\n\nEach of these kinds of inference can itself be viewed as a relation between expressions, or utterances of expressions. For example, we can say that sentence 1 entails sentence 2, or that it conversationally implicates it. Strictly speaking, it is really an utterance of a sentence that would trigger a conversational implicature—one wherefrom it is assumed that the person making the utterance is following certain conversational principles—but linguists often say that a sentence triggers a conversational implicature as a shorthand to mean that utterances of the sentence tend to give rise to the implicature.\nLikewise, expressions of different syntactic categories can presuppose certain sentences, or they can conventionally implicate them.",
    "crumbs": [
      "Diagnosing inference",
      "Overview"
    ]
  },
  {
    "objectID": "convention/defining_convention.html",
    "href": "convention/defining_convention.html",
    "title": "Defining convention",
    "section": "",
    "text": "Lewis’s goal in Lewis (1975) is to understand what is involved when a community of language users knows a language. Because he views language use in a community as the adoption by that community of a particular convention, he lays out a number of criteria that he thinks conventions, in general, ought to satisfy. According to Lewis, any given habit or practice that persists within some community is a convention just in case the following hold:\n\nEveryone conforms, i.e., participates in the habit.\nEveryone believes the other members of the community conform.\nThe other members of the community conforming provides a reason to conform.\nPeople generally prefer for the other members of the community to conform, rather than not conform.\nThe habit is optional or arbitrary, in the sense that it is selected from among some set of available alternative possible habits.\nThere is common knowledge within the community of the above criteria. That is, each community member knows (in some sense) about the convention, expects others to know about it, and expects others to have similar expectations.1\n\nHere, Lewis is engaged in what’s called “conceptual analysis”: he takes an ordinary English expression (convention), which English speakers regularly use with, perhaps at best, an implicit understanding of how it is used, and he attempts to elaborate necessary and sufficient conditions for its application. That is, Lewis takes conditions 1 through 6 to define the term convention. They all have to hold (they are necessary), and only they have to hold (they are sufficient), in order for something to be a convention. Why does he do this? Well in part, because he thinks the criteria in 1 - 6 isolate and shed light on an interesting array of phenomena that exist among our species—driving on the right vs. left of the road (depending on where one lives), greeting people in a particular way, knocking on doors to request entry, and a zillion other things; and in part, because as a philosopher of language, he thinks one of these kinds of phenomena—linguistic conventions—is particularly interesting. Does convention, when understood as abbreviating the conditions 1 through 6, have the same meaning as the ordinary English word convention? Maybe, maybe not. It doesn’t really matter very much for our purposes, since we just want a basic way of narrowing in on the phenomenon we’re studying (language, communication, meaning, etc.), along with, perhaps, what important features it has in common with certain other practices that people engage in.\nOne of the candidate conventions we looked at was teeth-brushing. I’ll go through the criteria here to see how well they apply to this particular practice.\n\n\n\nIt’s true that not everyone conforms (or always conforms), but not conforming is often noticed or frowned upon—something which at least appears to be true of genuine conventions, in general.\nWith certain exceptions, people expect each other to conform; this case doesn’t seem out of the ordinary for established conventions.\nIt is perhaps a little bit tricky to determine whether others brushing their teeth provides a reason for one to brush one’s own teeth. On the one hand, the main reasons people brush their teeth are to help prevent cavities and to not have bad breath. On the other hand, not having bad breath—while (or because) people believe it makes interactions with other people more pleasant—is often expected. In fact, there might exist a standard in societies in which teeth-brushing is common, according to which people shouldn’t be perceived as having bad breath; further, this standard may be reinforced when people actually do brush their teeth: the more people engage in the practice, the more social pressure there is to conform. If we entertain this possibility, it appears that other people brushing their teeth can provide a reason for one to brush one’s own teeth.\nThere appears to be a general preference that people brush their teeth. For one, people are often concerned about others’ hygiene because it affects their own experience (e.g., they don’t want to smell someone else’s bad breath). But also—insofar as standards of hygiene and cleanliness are arbitrary and culturally variable—I suspect people want to not have wasted their effort when they brush their own teeth. They might want their action to be justified, in part, by the existence of the standard. (Just think of how many people might stop brushing their teeth if the people around them stopped—probably at least a few?)\nWhether or not teeth-brushing is optional or arbitrary may also seem tricky to determine. On the one hand, it might not be clear what other practice would allow people to keep their teeth clean; on the other hand, it is a bit clearer what alternative practice might allow members of a community to adhere to a communal standard: the practice of not brushing your teeth. That is, we would just have to change the standard, and suddenly, people would be relieved of the social pressure to brush one’s teeth with which it coincides.\nInsofar as the above observations are accurate, they appear to be common knowledge. Common enough, at least, that you might expect to hear a reminder if you go to the dentist.\n\nI think there’s a good argument that teeth-brushing is (sometimes) a convention: it’s a convention when the community in which it is done adopts a standard (e.g., having healthy teeth, not being perceived as having bad breath)—one that teeth-brushing helps people meet. There might, of course, be other reasons to brush your teeth than the fact that it’s a convention.",
    "crumbs": [
      "Convention",
      "Defining convention"
    ]
  },
  {
    "objectID": "convention/defining_convention.html#footnotes",
    "href": "convention/defining_convention.html#footnotes",
    "title": "Defining convention",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Including the expectation that others have similar expectations!↩︎",
    "crumbs": [
      "Convention",
      "Defining convention"
    ]
  },
  {
    "objectID": "convention/assignment.html",
    "href": "convention/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "Due on Friday, September 5th.\n\nPart 1\nPick one of the other practices that were brought up in class, and give an analysis of whether or not it meets criteria 1-5 outlined in Defining convention. To remind you, these practices were:\n\nKnocking on the door before entering a room\nLooking both ways before crossing the street\nUsing an umbrella when it’s raining\nMaking WiFi free available to customers at coffee shops\n\nAlternatively, pick your own practice to analyze, and do the same. Whatever practice you choose need not be well-known, and we need not generally be consciously aware of its existence. It just needs to be something that people do within some community with which you’re familiar. (Please don’t use one of the ones that Lewis (1975) mentions, e.g., driving on a particular side of the road in a particular country.) You might use the discussion in these notes of ‘brushing your teeth’ as a model. Note—following those notes—that some aspects of the practice might appear to be conventional, while other aspects might seem less so. If you notice that this is true of the practice you analyze, please point it out.\nPlease consider and provide a response to the following question: is your analysis of the practice you’ve chosen consistent with your own intuitions about whether or not the practice is a convention?\n\n\nPart 2\nConsider the following state of affairs. Jo utters the sentence in (1) after her laptop freezes.\n\nThis damn computer is so slow!\n\nThere are a number of inferences you might draw based on this utterance. For example:\n\nThere is a computer.\nThe computer exceeds some degree of slowness, perhaps salient in the context of utterance, which is particularly high.\nJo is frustrated by her computer.\n\nThe last inference—that Jo is frustrated—appears to be triggered by her use of the word damn. In general, such words (called expressives) give rise to inferences of this kind, e.g., about an attitude held by the person who utters them; in this case, we draw an inference about Jo’s attitude toward her laptop.\nThis use of the word damn, which causes the frustration inference, appears, in some sense to mean that Jo is frustrated. Do you think that this notion of meaning is meaning\\(_{N}\\) or meaning\\(_{NN}\\)? That is, which of these relates utterances of (1)—or sentences containing the word damn, more generally—to inferences about frustration? Explain why, using the criteria for meaning\\(_{NN}\\) that Grice discusses.\n\n\nPart 3\nConsider the following different state of affairs. Bo maintains two forms of the verb be that he uses in the antecedents of counterfactual conditional sentences. For example, sometimes he utters sentences like (2)\n\nIf I was buying ice cream, I would probably get chocolate.\n\nwhile at other times he utters sentences like (3).\n\nIf I were buying ice cream, I would probably get chocolate.\n\nIn the first set of cases (exemplified by (2)), he inflects be as was when the subject is I, he, she, or a singular noun phrase, and as were when the subject is we, you, they, or a plural noun phrase. Meanwhile, in the second set of cases, (exemplified by (3)), he inflects be as were regardless of the person or number of the subject.\nImportantly, he tends to use these forms of be in this way in somewhat different social contexts. He is more likely to utter sentences like (2) (be -&gt; was) when he is with his friends or family or buying something at the grocery store (i.e., “informal” settings); while he is more likely to utter sentences like (3) (be -&gt; were) when he is answering a question in class or writing a paper (i.e., “formal” settings). In both cases, he appears to be signaling, in one way or another, something about his identity as a speaker of English and his relationship to his interlocutors. In the first set of contexts, he is not trying to speak “correctly” according to a received standard about spoken or written English, while in the second set of contexts, it is important to him that his English sound “correct”, and that his interlocutors draw the inference that he speaks English according to a particular formal standard.\nDo you think that the inference about Bo’s “correct” use of English in these contexts, i.e., when he uses were in cases in which he might otherwise use was is related to meaning\\(_{N}\\) or meaning\\(_{NN}\\)? In other words, in which sense of mean does Bo’s use of the form of the verb be mean that he uses English “correctly”? In the sense relevant to natural meaning, or in the sense relevant to non-natural meaning? Again, explain why, making reference to the criteria outlined by Grice.\n\n\n\n\n\n\n\nReferences\n\nLewis, David K. 1975. “Languages and Language.” In Arguing about Language, edited by Darragh Byrne and Max Kölbel. Arguing about Philosophy. New York: Routledge.",
    "crumbs": [
      "Convention",
      "Assignment"
    ]
  },
  {
    "objectID": "models/english_examples.html",
    "href": "models/english_examples.html",
    "title": "An English model and example",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\]\n\n\nThe model\nLet’s now say a bit more about the model, \\(\\mathcal{M}_{\\textit{Eng.}}\\).\n\n\\(\\mathcal{M}_{\\textit{Eng.}} = ⟨D_{\\textit{Eng.}}, \\{\\True, \\False\\}, I_{\\textit{Eng.}}⟩\\)\n\nThe domain, \\(D_{\\textit{Eng.}}\\), we can take to be the set \\(\\{\\ct{z}, \\ct{b}\\}\\):\n\\[D_{\\textit{Eng.}} = \\{\\ct{z}, \\ct{b}\\}\\]\nLet us now also say a thing or two about the interpretation function, \\(I_{\\textit{Eng.}}\\). Consistent with our grammatical rules, we will:\n\ninterpret noun phrases as elements of the domain,\ninterpret verb phrases as characteristic functions of sets,\ninterpret auxiliaries as functions from characteristic functions to characteristic functions,\nand interpret connectives as functions from pairs of characteristic functions to characteristic functions.\n\nHere we go:\n\n\\(I_{\\textit{Eng.}}(\\textit{Ziggy}) = \\ct{z}\\)  \\(I_{\\textit{Eng.}}(\\textit{Bella}) = \\ct{b}\\)  \\(I_{\\textit{Eng.}}(\\textit{runs}) = \\{\\ct{z}, \\ct{b}\\}_{CF}\\)  \\(I_{\\textit{Eng.}}(\\textit{run}) = \\{\\ct{z}, \\ct{b}\\}_{CF}\\)  \\(I_{\\textit{Eng.}}(\\textit{jumps}) = \\{\\ct{z}\\}_{CF}\\)  \\(I_{\\textit{Eng.}}(\\textit{jump}) = \\{\\ct{z}\\}_{CF}\\)  \\(I_{\\textit{Eng.}}(\\textit{doesn't}) = (λf.(λx.¬f(x)))\\)  \\(I_{\\textit{Eng.}}(\\textit{and}) = (λ⟨f, g⟩.(λx.f(x) = g(x) = \\True))\\)\n\nRecall the notation \\((·)_{CF}\\), which we use to form characteristic functions out of sets. Given any subset (\\(S\\)) of the domain \\(D\\):\n\\[S_{CF} ≝ (λx.x ∈ S)\\]\nThat is, a characteristic function of such a subset \\(D\\) is a function whose domain is \\(D\\) itself—\\(\\{\\ct{z}, \\ct{b}\\}\\) above—and whose codomain is the set of truth values. Further, this function gives back \\(\\True\\) on any element of the domain which is a member of \\(S\\) (otherwise, it gives back \\(\\False\\)).\nAlso recall the “negation” function \\(¬ : \\{\\True, \\False\\} → \\{\\True, \\False\\}\\). This function just flips \\(\\True\\) to \\(\\False\\) and \\(\\False\\) to \\(\\True\\):\n\\[¬\\True = \\False\\] \\[¬\\False = \\True\\]\nAlright. It is time to apply our lexicon definition and grammatical rules from earlier—together with this model definition—to an example expression of English.\n\n\nDeriving an example\nHere’s a derivation of the expression \\(\\textit{Ziggy doesn't jump and runs}\\). (Note that you may need to scroll right to see everything.)\n\\[\\begin{prooftree}\n\\AxiomC{$⟨\\textit{Ziggy}, I_{\\textit{Eng.}}(Ziggy)⟩ ⊢ np$}\n\\AxiomC{$⟨\\textit{doesn't}, I_{\\textit{Eng.}}(doesn't)⟩ ⊢ aux$}\n\\AxiomC{$⟨\\textit{jump}, I_{\\textit{Eng.}}(jump)⟩ ⊢ bvp$}\n\\RightLabel{Rule 2}\\BinaryInfC{$⟨\\textit{doesn't jump}, I_{\\textit{Eng.}}(doesn't)(I_{\\textit{Eng.}}(jump))⟩ ⊢ vp$}\n\\AxiomC{$⟨\\textit{and}, I_{\\textit{Eng.}}(and)⟩ ⊢ con$}\n\\AxiomC{$⟨\\textit{runs}, I_{\\textit{Eng.}}(runs)⟩ ⊢ vp$}\n\\RightLabel{Rule 3}\\TrinaryInfC{$⟨\\textit{doesn't jump and runs}, I_{\\textit{Eng.}}(and)(I_{\\textit{Eng.}}(doesn't)(I_{\\textit{Eng.}}(jump)), I_{\\textit{Eng.}}(runs))⟩ ⊢ vp$}\n\\RightLabel{Rule 1}\\BinaryInfC{$⟨\\textit{Ziggy doesn't jump and runs}, I_{\\textit{Eng.}}(and)(I_{\\textit{Eng.}}(doesn't)(I_{\\textit{Eng.}}(jump)), I_{\\textit{Eng.}}(runs))(I_{\\textit{Eng.}}(Ziggy))⟩ ⊢ s$}\n\\end{prooftree}\\]\n\n\nSimplifying the result\nNow, all we have to do is substitute the cases in the definition of \\(I_{\\textit{Eng.}}\\) in (2) for their occurrences in the last line of this derivation. If we do this we get:\n\\[(λ⟨f, g⟩.(λx.f(x) = g(x) = \\True))((λf.(λx.¬f(x)))(\\{\\ct{z}\\}_{CF}), \\{\\ct{z}, \\ct{b}\\}_{CF})(\\ct{z})\\]\nThis is a monster of a metalanguage expression! Fear not: it is quite straightforward to reduce it into something simple, as long as we are careful. One thing we need to be careful about is where all of the parentheses are!\nNote that this expression can be divided up into three main parts. First, there is a function:\n\\[(λ⟨f, g⟩.(λx.f(x) = g(x) = \\True))\\]\nThat’s the first part. The second part is the function’s first argument, which is a pair:\n\\[⟨(λf.(λx.¬f(x)))(\\{\\ct{z}\\}_{CF}), \\{\\ct{z}, \\ct{b}\\}_{CF}⟩\\]\nAnd the third part is the function’s second argument, which is just Ziggy:\n\\[\\ct{z}\\]\nImportantly, the first argument of the function—the pair—is somewhat complex: it itself can be reduced into something simpler! Let’s talk about that.\n\n\nSimplifying the first argument\nThe first argument of the main expression above\n\\[⟨(λf.(λx.¬f(x)))(\\{\\ct{z}\\}_{CF}), \\{\\ct{z}, \\ct{b}\\}_{CF}⟩\\]\nis a pair, so it has two components. The first component of this pair is itself a function applied to an argument; specifically, it is the interpretation of doesn’t applied to the interpretation of jump:\n\\[(λf.(λx.¬f(x)))(\\{\\ct{z}\\}_{CF})\\]\nLet us simplify this expression by first getting rid of the \\(λf\\) at the beginning of the function and substituting the argument, \\(\\{\\ct{z}\\}_{CF}\\), for all occurrences of \\(f\\) that appear inside of the function. Indeed, there is only one occurrence, and we get:\n\\[(λx.¬\\{\\ct{z}\\}_{CF}(x))\\]\nGreat! If we put this result back into the pair we started with, we end up with the following pair, which is equivalent to the one we needed to simplify:\n\\[⟨(λx.¬\\{\\ct{z}\\}_{CF}(x)), \\{\\ct{z}, \\ct{b}\\}_{CF}⟩\\]\n\n\nSimplifying the main function applied to the first argument\nIf we apply the main function to its (now simplified) first argument, we obtain:\n\\[(λ⟨f, g⟩.(λx.f(x) = g(x) = \\True))((λx.¬\\{\\ct{z}\\}_{CF}(x)), \\{\\ct{z}, \\ct{b}\\}_{CF})\\]\nThis complex metalanguage expression may be reduced as well. In particular, we should substitute\n\\[(λx.¬\\{\\ct{z}\\}_{CF}(x))\\]\nin for the \\(f\\) component of the function’s pair-shaped argument; and we should substitute\n\\[\\{\\ct{z}, \\ct{b}\\}_{CF}\\]\nfor the \\(g\\) component of this argument. As before, the substitution should occur inside the function itself, after we get rid of the \\(λ⟨f, g⟩\\). If we do this complex substitution, we end up with\n\\[(λx.(λx.¬\\{\\ct{z}\\}_{CF}(x))(x) = \\{\\ct{z}, \\ct{b}\\}_{CF}(x) = \\True)\\]\nHaving performed this substitution, we end up with another expression that can be simplified—specifically, the one to the left of the first equals sign:\n\\[(λx.¬\\{\\ct{z}\\}_{CF}(x))(x)\\]\nThis, too, is a function—\\((λx.¬\\{\\ct{z}\\}_{CF}(x))\\)—applied to an argument—\\(x\\). In this case, we have to get rid of the \\(λx\\) and—perhaps, unintuitively—substitute \\(x\\) for the \\(x\\) inside the function! No problem. If we do that, we get\n\\[¬\\{\\ct{z}\\}_{CF}(x)\\]\nIf we now try to recover the original function applied to its first argument—that is by replacing \\((λx.¬\\{\\ct{z}\\}_{CF}(x))(x)\\) with its simplified version—we get\n\\[(λx.¬\\{\\ct{z}\\}_{CF}(x) = \\{\\ct{z}, \\ct{b}\\}_{CF}(x) = \\True)\\]\nIt’s much easier to see now that this is just a characteristic function of some set: it is the characteristic function of the set of elements of the domain (\\(\\{\\ct{z}, \\ct{b}\\}\\)) such that it is true that that element is not in the set \\(\\{\\ct{z}\\}\\) and it is also true that element is in the set \\(\\{\\ct{z}, \\ct{b}\\}\\). That’s just the characteristic function of the set containing \\(\\ct{b}\\)—\\(\\{\\ct{b}\\}_{CF}\\)!\nSince this is the result of applying the main function to its first argument, we now have to apply the main function—already applied to its first argument—to its second argument.\n\n\nSimplifying the main function (already applied to its first argument) applied to its second argument\nNow that we have the result of applying the main function to its first argument, we can recover the original function applied to both its arguments by replacing the relevant part with the result we got. If we do this, we get\n\\[(λx.¬\\{\\ct{z}\\}_{CF}(x) = \\{\\ct{z}, \\ct{b}\\}_{CF}(x) = \\True)(\\ct{z})\\]\nThus, all we have remaining is one more substitution. In this case, we need to get rid of the \\(λx\\) and substitute \\(\\ct{z}\\) inside the function, getting\n\\[¬\\{\\ct{z}\\}_{CF}(\\ct{z}) = \\{\\ct{z}, \\ct{b}\\}_{CF}(\\ct{z}) = \\True\\]\nThis whole expression evaluates to \\(\\True\\) just in case \\(¬\\{\\ct{z}\\}_{CF}(\\ct{z})\\) and \\(\\{\\ct{z}, \\ct{b}\\}_{CF}(\\ct{z})\\) are both true. Well… because \\(\\{\\ct{z}\\}_{CF}(\\ct{z})\\) is \\(\\True\\), we have that \\(¬\\{\\ct{z}\\}_{CF}(\\ct{z})\\) is \\(\\False\\). So, the meaning of Ziggy doesn’t jump and runs in \\(\\mathcal{M}_{\\textit{Eng.}}\\) is \\(\\False = \\True = \\True\\). Otherwise, known as \\(\\False\\)!\n\n\nFinal thoughts\nIt is worth going back up to the model definition and inspecting the meanings we provided for jump and runs. Hopefully, you can convince yourself that Ziggy doesn’t run and jumps really should be false—specifically, because Ziggy runs. I think it is then worth reflecting on the fact that we actually got this intuitive result in the end, simply by applying the rules of our grammar—carefully, but ultimately in a completely deterministic and mindless way—and then simplifying the resulting metalanguage expression.",
    "crumbs": [
      "Models",
      "An English model and example"
    ]
  },
  {
    "objectID": "models/ana_syntax.html",
    "href": "models/ana_syntax.html",
    "title": "Arabic numeral arithmetic: syntax",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\]\n\nAs our first case study, we will look at the language of Arabic numeral arithmetic (‘\\(L_{ANA}\\)’, for short). This language has expressions like \\(⌜5⌝\\), \\(⌜(1 + 2)⌝\\), \\(⌜(37 = 43)⌝\\), \\(⌜(3 × 5) = 15⌝\\), and so on.\nNote that I’m using the corner brackets (\\(⌜\\) and \\(⌝\\)), here, in order to distinguish object language expressions (i.e., of Arabic numeral arithmetic) from actual numbers, like \\(1\\) and \\(2\\). The first are just symbols, currently meaningless, while the second are elements of the set \\(ℕ\\) of natural numbers. It’s important to keep these two distinct: while the former are things in the language—the set that we are trying to define and give an interpretation to—the latter will be elements of the domain of the model for the language.\n\\(L_{ANA}\\) has a few different kinds of expressions which are useful to distinguish. First, it has numerals. These are strings like \\(⌜1⌝\\), \\(⌜2⌝\\), \\(⌜57⌝\\), and so on. Second, it has complex expressions. These are strings containing the symbols for the addition and multiplication operators, like \\(⌜(1 + 2)⌝\\), \\(⌜((3 × 4) + 7)⌝\\), and so on. Finally, it has equations. These are strings containing the “equals” symbol, like \\(⌜37 = (5 × 6)⌝\\).\nThe syntax of \\(L_{ANA}\\) can be described by the rules in (2) (based on the alphabet in (1)). Note that we have four categories total: \\(atom\\), \\(n\\), \\(np\\), and \\(s\\).\nFirst, the alphabet:\n\n\\(Σ_{ANA} = \\{⌜0⌝, ..., ⌜9⌝, ⌜(⌝, ⌜)⌝, ⌜+⌝, ⌜×⌝\\}\\)\n\nNow, for the syntax:\n\nRule 1. \\(atom = \\{⌜0⌝, ..., ⌜9⌝\\}\\).  Rule 2. \\(atom ⊆ n\\).  Rule 3. If \\(x, y ∈ n\\), then \\(x^{⌢}y ∈ n\\).  Rule 4. \\(n ⊆ np\\).  Rule 5. If \\(x, y ∈ np\\), then \\(⌜(⌝^{⌢}x^{⌢}⌜+⌝^{⌢}y^{⌢}⌜)⌝, ⌜(⌝^{⌢}x^{⌢}⌜×⌝^{⌢}y^{⌢}⌜)⌝ ∈ np\\).  Rule 6. If \\(x, y ∈ np\\), then \\(x^{⌢}⌜{=}⌝^{⌢}y ∈ s\\).  Rule 7. Closure clause: nothing else is in \\(n\\), \\(np\\), or \\(s\\).\n\nIn words, these rules say (respectively):\n\nthat the atoms are \\(⌜1⌝\\) through \\(⌜9⌝\\);\nthat any atoms are also numerals;\nthat you can always take two numerals and concatenate them to make another numeral;\nthat any numerals are also noun phrases;\nthat you can always take two noun phrases and concatenate them around the addition or multiplication symbol (with parentheses) to make another noun phrase;\nthat you can always take two noun phrases and concatenate them around the equals symbol to make a sentence;\nand finally (the closure clause), that those things exhaust what you’re allowed to do—nothing else is in the language!\n\nWe can define the language of Arabic numeral arithmetic itself as follows:\n\n\\(L_{ANA} = s ∪ np\\):\n\nSo, it’s just the set of sentences unioned with the set of noun phrases; in plain English, something is in the language if and only if it is a sentence or a noun phrase.",
    "crumbs": [
      "Models",
      "Arabic numeral arithmetic: syntax"
    ]
  },
  {
    "objectID": "models/overview.html",
    "href": "models/overview.html",
    "title": "Overview",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\]\n\nOne of the central technical notions that we’ll rely on in this course is that of a model. A model is, in short, a way of associating the most basic expressions of whatever language we are studying with interpretations. It can be useful to think of a model as encoding a way that things could be, or—in the parlance of possible world semantics—a “possible world”. For example, if we are assigning meanings to a language that has the verb runs as one of its basic expressions, then we might associate this verb with a set—specifically, the set of people (or other entities) that run. In one model, Jo and Bo might be the ones who run (so that the set of people who run is \\(\\{\\ct{j}, \\ct{b}\\}\\)); in another, the interpretation of runs might be the set containing Bo and Mo (\\(\\{\\ct{b}, \\ct{m}\\}\\)); in yet another, maybe nobody runs (\\(\\varnothing\\)); and so on.\nTechnically, a model \\(\\mathcal{M}\\) is a structure that consists of a pair of a domain \\(D\\) and an interpretation function \\(I\\)—something which takes atoms (i.e., the most basic expressions, whatever those are) of the language onto elements of a set \\(M\\) (‘\\(M\\)’ for “meanings”):\n\nDefinition:  a model \\(\\mathcal{M}\\) is a tuple \\(⟨D, \\{\\True, \\False\\}, I⟩\\) of a non-empty set \\(D\\), the set of truth values \\(\\True\\) and \\(\\False\\), and a function \\(I : atom → M\\).\n\nIn (1), \\(atom\\) is just whatever the set of atoms is chosen to be.\nExactly how \\(M\\) is chosen will vary by case—but in general, it is something “constructed” out of \\(D\\), the domain. For example, it might be \\(D\\) itself; or it might be \\(D ∪ 2^{D}\\) (the set containing as members both all of the elements of \\(D\\) and all of the subsets of \\(D\\)); or it might be something else more complicated.\nFurther, the way the set \\(atom\\) is chosen will also vary by case. For example, if we are defining a language \\(L\\) over some alphaet \\(Σ\\), \\(atom\\) might be chosen to be \\(Σ\\) itself—or perhaps some subset of \\(Σ\\). The way we decide what \\(atom\\) is depends on how we decide we want to construct the interpretations of more complex expressions of the language out of the meanings of more basic expressions.",
    "crumbs": [
      "Models",
      "Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to Intro to Semantics!1\nI’m Julian Grove - I’m an Assistant Professor in the Linguistics Department (where this course is offered).\nHere is my contact info:\nCheck out the About page for more info about me and this site.\nHere are some facts about this course:",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#birds-eye-view",
    "href": "index.html#birds-eye-view",
    "title": "Syllabus",
    "section": "Bird’s-eye view",
    "text": "Bird’s-eye view\nThe goals of this course are to acquaint you with the study of meaning in natural language; specifically, to equip you with a certain vocabulary of concepts, as well as a certain collection of theoretical and methodological tools. This vocabulary and these tools will allow you to:\n\ncollect linguistic data and determine whether and how it is relevant to meaning;\nformulate and test hypotheses and theoretical claims about linguistic meaning; and\napproach questions about how linguistic meaning is situated within the broader scientific study of natural language and the human mind.\n\nThe first few weeks of the course will be more-or-less stage setting. Natural language semantics is a young field whose basic aims and guiding principles are somewhat in flux; so it will be useful to have this part of the course be dedicated to making these aims and principles fairly explicit. After these introductory weeks, we will get deep into the practice of doing semantics by focusing on a variety of empirical phenomena. The main acts will be verbs and their arguments, coordination, modification, anaphora, and quantification.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#attendance-and-class-participation",
    "href": "index.html#attendance-and-class-participation",
    "title": "Syllabus",
    "section": "Attendance and class participation",
    "text": "Attendance and class participation\nCome to class! Attendance won’t be graded, but you’re likely to do better in the course if you come than if you don’t.\nWhen you’re here, participate! You’ll get more out of the class if you do, and it’ll be more fun.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#assignments-and-grading",
    "href": "index.html#assignments-and-grading",
    "title": "Syllabus",
    "section": "Assignments and grading",
    "text": "Assignments and grading\n\nGrading\nYour work in this course will consist of nine (9) short written assignments—sometimes based on assigned readings—as well as an in-person final exam. Each written assignments will be a collection of responses to questions: some of these responses will have a mini-essay format, and some will be more like answers to problem sets (it’ll be clear in any given case what the expectation is). Each assignment will be available on this site, at the end of the set of notes to which the assignment corresponds. Please turn in a physical copy of your responses at the beginning of the class on which the assignment is due.\nThe written assignments will be graded on a 5-point scale, using the following rubric:\n\n5: you thoroughly engage with each question on the assignment. This means that you justify each of your responses, as well point out its implications, if appropriate. For example, if the truth of your response is likely to have any unintuitive consequences, you should note and explain these. Doing this well will sometimes be challenging and require you to think a lot.\nIf the assignment has questions requiring the application of analytical techniques that we have introduced in the course, “thorough engagement”, for current purposes, means applying these techniques in the right way.\n4: you thoroughly engage with most, but not all, of the questions.\n3: you thoroughly engage with only a small part of the assignment and give shallow or cursory answers to most of it.\n2: you give shallow or cursory answers to all of the questions on the assignment.\n1: you barely do the assignment.\n0: you don’t even barely do the assignment.\n\nThe written assignments will account for 90% of your grade, and the final exam, for 10%. The final will take place from 10am-12pm on December 10th (place TBD). It will be formatted similarly to the assignments.\nThe following grading scale will be used to determine course grades:\n\n\n\n93.5 ≤ A ≤ 100\n89.5 ≤ A- &lt; 93.5\n\n\n\n86.5 ≤ B+ &lt; 89.5\n83.5 ≤ B &lt; 86.5\n79.5 ≤ B- &lt; 83.5\n\n\n76.5 ≤ C+ &lt; 79.5\n74.5 ≤ C &lt; 76.5\n69.5 ≤ C &lt; 74.5\n\n\n\n\n\nCollaboration (including with chatbots)\nWe’ll talk about this in class. Basically:\n\nPlease actually do the readings yourself.\nPlease complete assignments yourself.\nIf you worked with someone else in the class (or a chatbot) on an assignment, please indicate that you did. If you used a chatbot, please include a note with your assignment describing how you used it.\n\n\n\nMissed or late work\nLate work will not be accepted unless you’ve arranged with me beforehand. If you need an extension, ask for one—just do it prior to the due date.\n\n\nExtra credit\nIf you like, you can get extra credit, accounting for up to an additional 2% of your course grade, in one of two ways:\n\nYou can earn up to two credits by participating in linguistics studies through the Linguistics Department’s SONA account (so, one percentage point of your grade per credit). This document has more information; so does this video. If you go this route, please participate in the studies before November 22.\nYou can read and summarize the main arguments of a (short) semantics paper, i.e., published in a journal, collection, or conference proceedings. Here are some paper suggestions (which I will continue to add to during the semester), but note that you can also summarize a paper of your choosing, so long as you consult with me about it first.\n\nGlass (2025)\nKarttunen (1971)\nKlecha (2014)\nSteinert-Threlkeld et al. (2023)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#zulip",
    "href": "index.html#zulip",
    "title": "Syllabus",
    "section": "Zulip",
    "text": "Zulip\nThere is a Zulip site for this course at uf-semantics-2025.zulipchat.com. We’ll use it to host discussions of the course material. You can post (and answer!) questions there. I will chime in, as well.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#university-honesty-policy",
    "href": "index.html#university-honesty-policy",
    "title": "Syllabus",
    "section": "University Honesty Policy",
    "text": "University Honesty Policy\nUF students are bound by The Honor Pledge which states, “We, the members of the University of Florida community, pledge to hold ourselves and our peers to the highest standards of honor and integrity by abiding by the Honor Code.” On all work submitted for credit by students at the University of Florida, the following pledge is either required or implied: “On my honor, I have neither given nor received unauthorized aid in doing this assignment.” The Honor Code (policy.ufl.edu/regulation/4-040/) specifies a number of behaviors that are in violation of this code and the possible sanctions. Furthermore, you are obligated to report any condition that facilitates academic misconduct to appropriate personnel. If you have any questions or concerns, please consult with the instructor.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#counseling-and-wellness-center",
    "href": "index.html#counseling-and-wellness-center",
    "title": "Syllabus",
    "section": "Counseling and Wellness Center",
    "text": "Counseling and Wellness Center\nContact information for the Counseling and Wellness Center:\n\ncounseling.ufl.edu\nPhone: 352.392.1575\n\nContact information for the University Police Department:\n\npolice.ufl.edu\nPhone: 352.392.1111",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#accommodations-for-students-with-disabilities",
    "href": "index.html#accommodations-for-students-with-disabilities",
    "title": "Syllabus",
    "section": "Accommodations for Students with Disabilities",
    "text": "Accommodations for Students with Disabilities\nStudents with disabilities requesting accommodations should first register with the Disability Resource Center (352.392.8565, disability.ufl.edu) by providing appropriate documentation. Once registered, students will receive an accommodation letter which must be presented to the instructor when requesting accommodation. Students with disabilities should follow this procedure as early as possible in the semester.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#academic-polices-and-resources",
    "href": "index.html#academic-polices-and-resources",
    "title": "Syllabus",
    "section": "Academic polices and resources",
    "text": "Academic polices and resources\nAvailable here:\n\nsyllabus.ufl.edu/syllabus-policy/uf-syllabus-policy-links/",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#calendar",
    "href": "index.html#calendar",
    "title": "Syllabus",
    "section": "Calendar",
    "text": "Calendar\n\n\n\nDate\nTopic\nReading\nAssignment\n\n\n\n\nFri., Aug. 22\nIntroduction\nFrankfurt (1986); Hicks et al. (2024)\n\n\n\nMon., Aug. 25\nConvention\nLewis (1975) (pp. 3–10)\n\n\n\nWed., Aug. 27\nConvention\nGrice (1957)\nView\n\n\nFri., Aug. 29\nSpeech acts\nBach (2006b) (pp. 147–155)\n\n\n\nWed., Sept. 3\nSpeech acts\n\n\n\n\nFri., Sept. 5\nSpeaker meaning\nGrice (1975)\n\n\n\nMon., Sept. 8\nSpeaker meaning\n\n\n\n\nWed., Sept 10\nDiagnosing inference\nWinter (2016) (Ch. 2, pp. 12–16)\n\n\n\nFri., Sept. 12\nDiagnosing inference\nCoppock and Champollion (2025) (Ch. 1, pp. 13–41; Ch. 8, pp. 315–322)\n\n\n\nMon., Sept. 15\nSets and set theory\n\nView\n\n\nWed., Sept. 17\nFunctions, relations, and languages\n\n\n\n\nFri., Sept. 19\nModels\nWinter (2016) (Ch. 2, pp. 17–27)\n\n\n\nMon., Sept. 22\nArabic numeral arithmetic\n\nView\n\n\nWed., Sept. 24\nArabic numeral arithmetic\n\n\n\n\nFri., Sept. 26\nIntransitive verbs\nWinter (2016) (Ch. 3, pp. 44–64)\n\n\n\nMon., Sept. 29\nIntransitive verbs\nWinter (2016) (Ch. 3, pp. 64-72)\n\n\n\nWed., Oct. 1\nWhat should go into a theory of meaning?\nBach (2006a); Szabó (2020) (§§3–4)\n\n\n\nFri., Oct. 3\nLambda notation and types\n\n\n\n\nMon., Oct. 6\nLambda notation and types\n\nView\n\n\nWed., Oct. 8\nTransitive verbs\n\n\n\n\nFri., Oct. 10\nTransitive verbs\n\n\n\n\nMon., Oct. 13\nApplicative categorial grammar\nJurafsky and Martin (2025) (Appendix E)\n\n\n\nWed., Oct. 15\nApplicative categorial grammar\n\nTBD\n\n\nFri., Oct. 17\nGrammars and models\n\n\n\n\nMon., Oct. 20\nVerbs and arguments redux\n\n\n\n\nWed., Oct. 22\nVerbs and arguments redux\n\n\n\n\nFri., Oct. 24\nCoordinators\nWinter (2016) (Ch. 3, pp. 74–80)\n\n\n\nMon., Oct. 27\nCoordinators\n\nTBD\n\n\nWed., Oct. 29\nAdjectives\nKennedy (2012)\n\n\n\nFri., Oct. 31\nAdjectives\n\n\n\n\nMon., Nov. 3\nAdjectives\n\nTBD\n\n\nWed., Nov. 5\nPronouns\n\n\n\n\nFri., Nov. 7\nPronouns\n\n\n\n\nMon., Nov. 10\nPronouns\n\nTBD\n\n\nWed., Nov. 12\nQuantifiers\nWinter (2016) (Ch. 4, pp. 99–108)\n\n\n\nFri., Nov. 14\nQuantifiers\n\n\n\n\nMon., Nov. 17\nQuantifiers\n\nTBD\n\n\nWed., Nov. 19\nNegative polarity items\n\n\n\n\nFri., Nov. 21\nQuantifiers and binding\n\n\n\n\nMon., Dec. 1\nQuantifiers and binding\n\n\n\n\nWed., Dec. 3\nQuantifiers and binding",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nListed as both LIN4803 and LIN6804 at one.uf.edu.↩︎",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "formal_preliminaries/sets.html",
    "href": "formal_preliminaries/sets.html",
    "title": "Sets",
    "section": "",
    "text": "What’s a set?\nA set is, intuitively speaking, a collection of objects, which we call the elements or members of the set. Any talk about sets is understood in the context of some universe of discourse. All sets inhabit this universe, as do any of the objects which we may consider to be members of a set. We may, for example, want to have things like words or natural numbers inhabit this universe. When analyzing the semantics of English, we may include other things too, like entities or individuals.\nFor any set \\(A\\) and any object \\(a\\) in the universe, either one of the following two things is true. \\[\\begin{align*}\na ∈ A \\tag{$a$ is a member of $A$}\\\\\na ∉ A \\tag{$a$ is not a member of $A$}\n\\end{align*}\\] Moreover, sets are defined by their members. That means that, for every \\(a\\) in the universe, if \\(a ∈ A\\) if and only if \\(a ∈ B\\), then \\(A\\) and \\(B\\) are the same set. This is the principle of extensionality for sets.\nWe will generally help ourselves to two kinds of notation for specifying sets: list notation and predicate (or “set-comprehension”, or “set-builder”) notation. In list notation, we define a set by enumerating its elements between curly braces. For example, \\[\\{1, 2, 3, George Washington\\}\\] is just the set \\(A\\) such that \\(1 ∈ A\\), \\(2 ∈ A\\), \\(3 ∈ A\\), and nothing else is \\(∈ A\\). Because of extensionality, \\(\\{1, 2, 3, GW\\}\\) is the same set as \\(\\{1, 1, 2, 3, GW\\}\\) is the same set as \\(\\{3, GW, 2, 2, 1\\}\\), etc. Using predicate notation, we can specify the very same set as \\[\\{x ∣ x = 1 \\text{ or } x = 2 \\text{ or } x = 3 \\text{ or } x = GW\\}\\] In general, in writing ‘\\(\\{a\\) \\(|\\) \\(φ\\}\\)’, \\(a\\) will be an expression with some number of variables occurring in it (things like \\(x\\), \\(y\\), \\(z\\), and so on), and \\(φ\\) will be a sentence expressing a condition on the values of these variables. Any number of the variables in \\(a\\), and generally only these, can occur in \\(φ\\). In the example above, \\(a\\) just was the variable \\(x\\), and the condition expressed was that the value of \\(x\\) be either 1, 2, 3, or George.\nBut we could also use predicate notation, as in \\[\\{x + 1 ∣ x = 1 \\text{ or } x = 2 \\text{ or } x = 3\\}\\] where \\(a\\) is the complex expression ‘\\(x +\n1\\)’, to specify the set \\(\\{2, 3, 4\\}\\).\nImportantly, there is an empty set, which we’ll write ‘\\(\\varnothing\\)’. It has no members. As a result, \\(a ∉ \\varnothing\\) for any \\(a\\) in the universe. The empty set can also be specified in list notation as `\\(\\{\\}\\)’.\n\n\nSet-forming operations\nGiven any two sets \\(A\\) and \\(B\\), we can use the following set-forming operations. \\[\\begin{align*}\n&a ∈ A \\cup B &\\textit{if $a ∈ A$ or $a ∈ B$}\\tag{union}\\\\\n&a ∈ A \\cap B &\\textit{if $a ∈ A$ and $a ∈ B$}\\tag{intersection}\\\\\n&a ∈ A - B &\\textit{if $a ∈ A$ and $a ∉ B$}\\tag{set difference}\n\\end{align*}\\] In addition, we say one set \\(A\\) is a subset of another set \\(B\\) \\[A ⊆ B\\] just in case every \\(a ∈ A\\) is also \\(∈ B\\). If, in addition, there is some \\(b ∈ B\\) such that \\(b ∉ A\\), we may write \\[A ⊂ B\\] (\\(A\\) is a proper subset of \\(B\\)) to indicate this. Note that because of extensionality, \\(A = B\\) just in case both \\(A ⊆ B\\) and \\(B ⊆ A\\).\nWe may sometimes refer to sets using notation for generalized intersection and union. If \\(S\\) is a set all of whose members are also sets, we write \\[⋃ S\\] to mean \\(\\{x\\) \\(|\\) \\(x ∈ A\\) for some \\(A ∈ S\\}\\); that is, we squash \\(S\\) into the union of all of its members.\nFor example, let’s define \\(ℕ = \\{0, 1, 2, ...\\}\\) (i.e., the set of “natural numbers”). Then, we can define the set \\(\\{\\{x\\} ∣ x ∈ ℕ\\}\\)—this is just the set of sets with only one member, and whose single member is one of the natural numbers; i.e., \\(\\{\\{0\\}, \\{1\\}, \\{3\\}, ...\\}\\). Now, we can define the set \\(⋃\\{\\{x\\} ∣ x ∈ ℕ\\}\\)—this set is just \\(ℕ\\)!\nLikewise, we write \\[⋂ S\\] to mean \\(\\{x\\) \\(|\\) \\(x ∈ A\\) for every \\(A ∈ S\\}\\); that is, we squash \\(S\\) into the intersection of all of its members. If we did this for the set defined above, i.e., \\(⋃\\{\\{x\\} ∣ x ∈ ℕ\\}\\), we’d get back the empty set (\\(\\varnothing\\))—none of the sets in the original set has any members in common!\n\n\nPowerset\nGiven a set \\(A\\), we may take its powerset, which is often written ‘\\(2^A\\)’ The powerset of \\(A\\) is defined as \\[2^A = \\{B ∣ B ⊆ A\\}\\] That is, it is the set of subsets of \\(A\\).\nThe power set of \\(ℕ\\), for example is \\(\\{\\{1\\}, \\{7, 223423, 2\\}, ... \\text{all the ways of putting natural numbers into sets}\\}\\). Indeed, the set \\(\\{\\{x\\} ∣ x ∈ ℕ\\}\\) that we talked about above is a subset of the powerset of \\(ℕ\\). (Why?)",
    "crumbs": [
      "Formal preliminaries",
      "Sets"
    ]
  },
  {
    "objectID": "formal_preliminaries/products.html",
    "href": "formal_preliminaries/products.html",
    "title": "Products",
    "section": "",
    "text": "Given two sets \\(A\\) and \\(B\\), we take their binary Cartesian product \\[(A × B)\\] to be the set of pairs of elements \\(⟨a, b⟩\\), where \\(a ∈ A\\) and \\(b ∈ B\\). We call \\(a\\) the first component (or projection) of such a pair and \\(b\\) the second component. Given a pair \\(p\\), we will sometimes write ‘\\(p_{1}\\)’ to refer to its first component and ‘\\(p_{2}\\)’ to refer to its second component. In particular: \\[⟨x, y⟩_{1} = x\\] \\[⟨x, y⟩_{2} = y\\]\nThe following statement is also true: \\[⟨x_{1}, x_{2}⟩ = x\\] That is, if you take the first and second components of a pair and pair them back up, you haven’t done anything!\nWe can generalize binary products to \\(n\\)-ary products by considering the former to correspond to the special case where \\(n = 2\\). For the general case \\[A₁ × ... × A_{n-1} × Aₙ\\] we can encode this product as \\[(...(A₁ × ... × A_{n-1}) × Aₙ)\\] For example, \\(A × B × C\\) is just \\(((A × B) × C)\\); its members are, therefore, pairs \\(⟨⟨a, b⟩, c⟩\\), which is just how we can encode 3-tuples \\(⟨a, b, c⟩\\).",
    "crumbs": [
      "Formal preliminaries",
      "Products"
    ]
  },
  {
    "objectID": "formal_preliminaries/relations.html",
    "href": "formal_preliminaries/relations.html",
    "title": "Relations",
    "section": "",
    "text": "If we have a collection of sets \\(A_{1}\\) through \\(A_{n}\\), then we can take an \\(n\\)-ary relation on those sets. An \\(n\\)-ary relation on the sets \\(A_{1}, ..., A_{n}\\) is a set \\(R\\) such that \\(R ⊆ A_{1} × ... × A_{n}\\). In the special case of a binary relation on two sets \\(A\\) and \\(B\\), \\(R\\) is a set of pairs \\(⟨a, b⟩\\), such that \\(a ∈ A\\) and \\(b ∈ B\\). That is, \\(R ⊆ A × B\\). For \\(n\\)-ary relations \\(R\\), if the \\(n\\)-tuple \\(⟨a₁, ..., aₙ⟩\\) is a member of \\(R\\), we can indicate this by writing \\[R(a₁, ..., aₙ)\\]\n\n\n\n\n\n\n\n\n\nAn important operation on relations is relation composition. Given a relation \\(R_{1} ⊂ A × B\\) and a relation \\(R_{2} ⊆ B × C\\), their composition \\(R_{2} ∘ R_{1} ⊆ A × C\\) is defined as \\[R_{2} ∘ R_{1} = \\{⟨x, y⟩ ∣ \\text{there is some $z$ such that } ⟨x, z⟩ ∈ R_{1} \\text{ and } ⟨z, y⟩ ∈ R_{2}\\}\\] That is, \\(R_{2} ∘ R_{1}\\) is gotten by relating an element of \\(A\\) to an element of \\(C\\) just in case there is some element of \\(B\\) related to the first element by \\(R_{1}\\) and to the second element by \\(R_{2}\\).",
    "crumbs": [
      "Formal preliminaries",
      "Relations"
    ]
  },
  {
    "objectID": "speaker_meaning/applying_maxims.html",
    "href": "speaker_meaning/applying_maxims.html",
    "title": "Applying the maxims",
    "section": "",
    "text": "Grice (1975) gives several examples of inferences that appear to result from the maxims, given particular utterances. These inferences are called conversational implicatures. Here, I’ll give a few of my own.",
    "crumbs": [
      "Speaker meaning",
      "Applying the maxims"
    ]
  },
  {
    "objectID": "speaker_meaning/applying_maxims.html#footnotes",
    "href": "speaker_meaning/applying_maxims.html#footnotes",
    "title": "Applying the maxims",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n When I say that Jo did all of the readings is stronger than Jo did some of the readings, what I mean is that the first sentence entails the second sentence, but the second does not entail the first. Viewing things from the other direction, one can say that Jo did some of the readings is weaker than Jo did all of the readings. ↩︎",
    "crumbs": [
      "Speaker meaning",
      "Applying the maxims"
    ]
  },
  {
    "objectID": "speaker_meaning/maxims.html",
    "href": "speaker_meaning/maxims.html",
    "title": "The Gricean maxims",
    "section": "",
    "text": "Since our general goal is to give an account of conventional meaning—the type of meaning that users of a language associate with expressions by convention, in a way that allows them to deploy those expressions to communicate in real conversational settings—it would be useful to first get a handle on some general conversational principles about how expressions with certain meanings are deployed. Having observed inferences in real-life language use, we can hopefully then work out which of those inferences are related to conventional meaning and which are related to general (and perhaps, to some extent, non-linguistic) social conventions about conversation itself.\nWith this aim partly in mind, Grice (1975) proposes a principle that he takes to regulate real-life conversation: the Cooperative Principle.\n\nThe Cooperative Principle:  “Make your conversational contribution, such as is required, at the stage at which it occurs, by the accepted purpose or direction of the talk exchange in which you are engaged” (p. 45).\n\n(Will do 😅!) More seriously, Grice thinks that this principle should be seen as implicitly regulating what people decide to say to one another, and he tries to give the principle a little bit of heft by saying what maxims people appear to follow in order to adhere to it.\nHe proposes four general maxims:\n\nQuality\nQuantity\nRelation\nManner\n\n\nQuality\nThe maxim of Quality has two parts: first, don’t say things that you think are false; and second, only say things that you have enough evidence for.\nGrice doesn’t say much about what, in general, constitutes “enough evidence”—presumably this is meant to be understood as somewhat context-dependent. For example, if you’re a surgeon in an operating room, the standard of evidence for certain kinds of assertions would likely be pretty high (“The tibial plateau is misaligned by three degrees.”), whereas if you’re a contestant on Jeopardy, it’s generally likely to be lower. Indeed (as is the case for all of Grice’s maxims), there are many details to fill in here! But we can still get a certain amount of reasoning off the ground about why certain utterances mean what they do, even with while leaving a lot underspecified.\n\n\nQuantity\nThe maxim of Quantity is also stated by Grice as having two parts: first, be as informative as necessary; second, don’t overdo it!\nAgain, Grice leaves out crucial information about how this maxim ought to be evaluated—what counts as “as informative as necessary” or “overdo[ing] it”? As we saw, it’s possible to give a more systematic account of what counts as too little or too much information if we have access to the notion of a question under discussion (QUD; Roberts (2012)). For example, if the question is how many children does John have?, an answer which is under-informative might be John has between three and five children, while an answer that might be over-informative might be John has exactly three children, and they all go to the same high school. Intriguingly, it is perhaps also possible for an answer to both under- and over-informative, e.g., John has between three and five children, and they all go to the same high school. The answer which is “just right”, given the question, might then be John has exactly three children.\nImportant to note is that this addition really only works if we have two things. First, we need a way of knowing when some question is the QUD of a particular conversation or discourse; and second, we need a definition of when a given statement answers a question, either partially or completely. The problem of identifying QUDs is a matter of ongoing research in both computational and experimental pragmatics—it’s hard (though language users might generally seem to solve it fairly effortlessly).\nMeanwhile, the problem of defining when a statement answers a question is perhaps somewhat easier, insofar as there are several theories of question meanings on the market, and any of them might in principle tell us what the possible complete and partial answers to a QUD are, once one has been identified.\n\n\nRelation\nThe maxim of Relation is stated as follows: be relevant.\nThat is, don’t be like zombie kid. This maxim can also be made a little more systematic by taking it to constraint the relationship between statements made in some conversation and the QUD of the conversation: it seems to say that one should make statements that answer the QUD, or at least partially answer it.\n\n\nManner\nThe maxim of Manner is taken by Grice to dictate that one should be clear, unambiguous, brief, and orderly.\nWhether or not a particular utterance adheres to this maxim might seem to be somewhat in the eye of the beholder. For example, what counts as “brief”—should we count the number of segments, syllables, words, constituent phrases used (does it depend?)? Ideally, such criteria would be open to empirical investigation.\n\n\n\n\n\n\n\nReferences\n\nGrice, H. P. 1975. “Logic and Conversation.” In Syntax and Semantics, edited by Peter Cole and Jerry L. Morgan, 3, Speech Acts:41–58. New York: Academic Press.\n\n\nRoberts, Craige. 2012. “Information Structure: Towards an Integrated Formal Theory of Pragmatics.” Semantics and Pragmatics 5 (December): 6:1–69. https://doi.org/10.3765/sp.5.6.",
    "crumbs": [
      "Speaker meaning",
      "The Gricean maxims"
    ]
  },
  {
    "objectID": "formal_preliminaries/assignment.html",
    "href": "formal_preliminaries/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "Due on Monday, September 29th.\n\nPart 1\nTaken from Partee, Meulen, and Wall (1990).\nConsider the following sets:\n\\[S_{1} = \\{\\{\\varnothing\\}, \\{A\\}, A\\}\\] \\[S_{2} = A\\] \\[S_{3} = \\{A\\}\\] \\[S_{4} = \\{\\{A\\}\\}\\] \\[S_{5} = \\{\\{A\\}, A\\}\\] \\[S_{6} = \\varnothing\\] \\[S_{7} = \\{\\varnothing\\}\\] \\[S_{8} = \\{\\{\\varnothing\\}\\}\\] \\[S_{9} = \\{\\varnothing, \\{\\varnothing\\}\\}\\]\n\nOf the sets \\(S_{1}\\) through \\(S_{9}\\), which are members of \\(S_{1}\\)?\nWhich are subsets of \\(S_{1}\\)?\nWhich are members of \\(S_{9}\\)?\nWhich are subsets of \\(S_{9}\\)?\nWhich are members of \\(S_{4}\\)?\nWhich are subsets of \\(S_{4}\\)?\n\n\n\nPart 2\nShow that \\(A ∩ B ⊆ A\\) for any arbitrary sets \\(A\\) and \\(B\\).\n\n\nPart 3\nShow that if \\(A ⊆ B\\), then \\(A = A ∩ B\\) for any arbitrary sets \\(A\\) and \\(B\\).\n\n\n\n\n\n\n\nReferences\n\nPartee, Barbara H., Alice ter Meulen, and Robert E. Wall. 1990. Mathematical Methods in Linguistics. Vol. 30. Studies in Linguistics and Philosophy. Dordrecht: Kluwer Academic Publishers.",
    "crumbs": [
      "Formal preliminaries",
      "Assignment"
    ]
  },
  {
    "objectID": "formal_preliminaries/languages.html",
    "href": "formal_preliminaries/languages.html",
    "title": "Languages",
    "section": "",
    "text": "We write `\\(A^n\\)’ to refer to the result of taking the product of the set \\(A\\) with itself \\(n\\) times. As a matter of convention, we will adopt the following equivalences. \\[\\begin{align*}\nA^0 &=_{def} \\{\\varnothing\\}\\\\\nA^1 &=_{def} A\\\\\nA^n &=_{def} A^{n-1} × A\n\\end{align*}\\] To define a language, we first define a set \\(Σ\\), which we call the alphabet, words, or lexicon of the language. These can be anything, in principle (phonetic forms, sets of syntactic features, blades of grass, etc.). Then, a language over the alphabet \\(Σ\\) is some set \\(L\\) such that \\[L ⊆ ⋃ \\{Σ^{i} ∣ i ∈ ℕ\\}\\] Thus \\(L\\) is a set of \\(n\\)-tuples of words from \\(Σ\\) of any length \\(n\\). When we talk about languages, we will refer to tuples as strings and adopt the convention of writing, e.g., the string ⟨ the, dog, is, friendly ⟩ simply as the dog is friendly (or as “the dog is friendly” when I’m writing on the chalkboard). Note that \\(L\\) may include \\(\\varnothing\\), which we will regard as the empty string (the string of length 0), which we write ‘\\(ϵ\\)’ in this context.",
    "crumbs": [
      "Formal preliminaries",
      "Languages"
    ]
  },
  {
    "objectID": "formal_preliminaries/functions.html",
    "href": "formal_preliminaries/functions.html",
    "title": "Functions",
    "section": "",
    "text": "A function from a set \\(A\\) to a set \\(B\\) is a map from elements of \\(A\\) to elements of \\(B\\) which pairs each element of \\(A\\) with exactly one element of \\(B\\). If \\(f\\) is such a function, we write \\[f : A → B\\] to indicate this. We call \\(A\\) the domain of the function, and we call \\(B\\) the codomain of the function. If \\(a ∈ A\\), then we write \\[f(a)\\] to pick out the unique \\(b ∈ B\\) that \\(f\\) pairs \\(a\\) up with. In case \\(f(a) = b\\) (for some \\(a\\) and \\(b\\)), we call \\(a\\) the argument of the function \\(f\\), and we call \\(b\\) the value of the function \\(f\\) on \\(a\\).\nKind of like with relations, the definition of (unary) functions can be generalized to that for n-ary functions by considering the latter to be a map from the product of \\(n\\) sets \\(A₁, ..., Aₙ\\) to a set \\(B\\). If \\(f\\) is such an \\(n\\)-ary function, we write \\[f : A_1 × ... × A_n → B\\] to indicate this. Given \\(n\\) arguments \\(a₁ ∈ A₁, ..., aₙ ∈ Aₙ\\), we write \\[f(a_1, ..., a_n)\\] to pick out out the \\(b ∈ B\\) that \\(f\\) maps \\(a₁, ..., aₙ\\) to.\n\n\n\n\n\n\n\n\n\n\n\n\nImportantly, there is a corresponding notion of function composition. We need only consider functions from \\(A\\) to \\(B\\) a type of relation on \\(A\\) and \\(B\\)—one that relations every element in \\(A\\) to exactly one element in \\(B\\). Given functions \\(f : A → B\\) and \\(g : B → C\\), their composition \\(g ∘ f : A → C\\) is such that \\((g ∘ f)(x) = g(f(x))\\), for any \\(x ∈ A\\).",
    "crumbs": [
      "Formal preliminaries",
      "Functions"
    ]
  },
  {
    "objectID": "formal_preliminaries/examples.html",
    "href": "formal_preliminaries/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of proofs of some basic facts about sets.\n\nFirst example\nShow that \\(A ⊆ A ∪ B\\).\nThe definition of subset says that this holds if everything in \\(A\\) is also in \\(A ∪ B\\). The definition of union says that any \\(x ∈ A ∪ B\\) if \\(x ∈ A\\) or \\(x ∈ B\\). Thus any \\(x ∈ A\\) is such that \\(x ∈ A ∪ B\\), as needed.\n\n\nSecond example\nLet \\(A, B ⊆ C\\), and let’s introduce the notation ‘\\(¬X\\)’ for \\(C - X\\).\nShow that \\(¬(A ∪ B) ⊆ ¬A ∩ ¬B\\).\nAssume \\(x ∈ ¬(A ∪ B)\\) (for arbitrary \\(x\\)); then, our goal is show that \\(x\\) must be in \\(¬A ∩ ¬B\\). By definition, \\(x ∈ C\\), but \\(x ∉ A ∪ B\\). Hence, \\(x ∉ A\\) (or else, we would have that \\(x ∈ A ∪ B\\)). But because, by hypothesis, \\(x ∈ C\\), we now have that \\(x ∈ ¬A\\). A similar argument applies to \\(¬B\\)! Thus \\(x ∈ ¬A ∩ ¬B\\). Since \\(x\\) is arbitrary, the argument extends to any element of \\(¬(A ∪ B)\\), as needed to show that the two sets are in a subset relation.",
    "crumbs": [
      "Formal preliminaries",
      "Examples"
    ]
  },
  {
    "objectID": "models/assignment.html",
    "href": "models/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\]\n\nDue on Monday, October 13th.\n\n\n\n\n\n\n\n\nPart 1\nThere is a planet orbiting Proxima Centauri that has a population of organisms which are very human-like. As it happens, a group within the larger population uses a system a lot like Arabic numeral arithmetic to perform arithmetic calculations, except that the syntax and semantics are slightly different. This group uses a base-7 number system, and the atomic numerals have different meanings than they do in Arabic numeral arithmetic. We can call the language they use for arithmetic calculations \\(L_{NANA}\\) (not Arabic numeral arithmetic).\nTo be more precise, for this group within the population which lives on the planet orbiting Proxima Centauri, the set of atomic numerals is the following one:\n\\[atom = \\{⌜1⌝, ⌜2⌝, ⌜3⌝, ⌜4⌝, ⌜5⌝, ⌜6⌝, ⌜7⌝\\}\\]\nMeanwhile, the interpretation function of the model employed by this group, \\(I_{NANA}\\), is the following one:\n\\[I_{NANA}(⌜1⌝) = 6\\] \\[I_{NANA}(⌜2⌝) = 5\\] \\[I_{NANA}(⌜3⌝) = 4\\] \\[I_{NANA}(⌜4⌝) = 3\\] \\[I_{NANA}(⌜5⌝) = 2\\] \\[I_{NANA}(⌜6⌝) = 1\\] \\[I_{NANA}(⌜7⌝) = 0\\]\nThe full model of \\(L_{NANA}\\) employed by this group is thus:\n\\[\\mathcal{M}_{NANA} = ⟨ℕ, \\{\\True, \\False\\}, I_{NANA}⟩\\]\nOne more thing. Instead of the Rule 3 which we defined in Arabic numeral arithmetic: semantics, these guys use the following alternative Rule 3:\n\\[\\begin{prooftree}\n\\AxiomC{$⟨x, ⟦x⟧_{\\mathcal{M}_{ANA}}⟩ ⊢ n$}\n\\AxiomC{$⟨y, ⟦y⟧_{\\mathcal{M}_{ANA}}⟩ ⊢ n$}\n\\RightLabel{Rule 3}\\BinaryInfC{$⟨x^{⌢}y, ⟦x⟧_{\\mathcal{M}_{ANA}} * 7^{length(y)} + ⟦y⟧_{\\mathcal{M}_{ANA}}⟩ ⊢ n$}\n\\end{prooftree}\\]\nThe rules of for generating strings of \\(L_{NANA}\\) and their interpretations are otherwise identical to those for generating strings of \\(L_{ANA}\\) and their interpretations.\nShow that \\(⌜((53 + 6) × 5) = 24⌝\\) is a sentence (i.e., an \\(s\\)) of \\(L_{NANA}\\), and show what its interpretation in \\(\\mathcal{M}_{NANA}\\) is.\n\n\nPart 2\nUsing the English grammar fragment defined in A first approximation of English in conjunction with the model defined in An English model and example, show that the string in (1) is a sentence (i.e., an \\(s\\)), and show what its interpretation is.\n\nBella doesn’t jump and run.\n\n\n\nPart 3\nAs you know, English does not only have sentences like (1). It also has sentences like (2).\n\nBella doesn’t jump and Bella doesn’t run.\n\nLet us propose that in addition to the word and, English has another word, and2, which is pronounced exactly the same way as and is pronounced (i.e., [æːnd]), but which coordinates sentences instead of verb phrases. Thus and is the variant of the coordinator that appears in (1), while and2 is the variant of the coordinator that appears in (2).\nProvide a lexical entry for the variant of the coordinator which appears in (2)—something that can be added to the lexicon given in A first approximation of English. Give a derivation showing that (2) is a sentence with a meaning, using this lexical entry. Does your lexical entry ensure that (1) and (2) entail each other? If so, how does it ensure this?",
    "crumbs": [
      "Models",
      "Assignment"
    ]
  },
  {
    "objectID": "models/ana_semantics.html",
    "href": "models/ana_semantics.html",
    "title": "Arabic numeral arithmetic: semantics",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\]\nWhat we would like to do is figure out a way of associating elements of \\(L_{ANA}\\) with meanings. In order to do this, we have to make use of a model—that is, one of the things that we introduced here.\nWhich model, exactly, should we be using to interpret \\(L_{ANA}\\)? We’ll follow the standard way of doing things and say that the model of \\(L_{ANA}\\) is the one defined in (1), which has as its domain the set \\(ℕ\\) of natural numbers.\nThat is, \\(D\\) is defined to be \\(ℕ\\)—i.e., the set \\(\\{0, 1, 2, ...\\}\\).\nWhat can we say about the model’s interpretation function, \\(I_{ANA}\\)? Well, because we defined \\(atom\\) (here) to be the set \\(\\{⌜0⌝, ..., ⌜9⌝\\}\\), we know that this set is the domain of \\(I_{ANA}\\): it is the job of the model’s interpretation function to map things from this set onto interpretations of some kind.\nWhat about the codomain of \\(I_{ANA}\\)? Since we’re having \\(I_{ANA}\\) map elements from the set \\(\\{⌜0⌝, ..., ⌜9⌝\\}\\), we should choose the codomain of \\(I_{ANA}\\)—the set it maps these symbols to—to be \\(ℕ\\): the interpretation of any of the symbols \\(⌜0⌝\\) through \\(⌜9⌝\\) is just a natural number!\nSpecifically, we’ll define \\(I_{ANA}\\) as in (2).\nHopefully, none of these choices is particularly surprising! Still, it’s important to point out that—while it looks a bit like we are using the interpretation function to simply “remove the corner brackets (\\(⌜, ⌝\\))”—that’s just due to the fact that we are employing the very language that we are interpreting as part of our metalanguage.1 Said another way: there is nothing special about the symbol \\(⌜1⌝\\) that makes it a particularly good candidate for being mapped onto the number 1. The choice reflects how we use the language \\(L_{ANA}\\), but it is arbitrary nonetheless—a genuine Lewis-style convention.\nAlright. The time has come to define our interpretation function (or “interpretation brackets”, “denotation function”, what have you) \\(⟦·⟧_{\\mathcal{M}_{ANA}}\\) over the entirety of \\(L_{ANA}\\). We will show how to do this using two different presentations.2 One presentation uses English and provides the rules mostly as conditional if then statements. The other presentation is a bit more formally systematic: it provides formal rules for constructing proofs. Importantly, both presentations say exactly the same thing.",
    "crumbs": [
      "Models",
      "Arabic numeral arithmetic: semantics"
    ]
  },
  {
    "objectID": "models/ana_semantics.html#footnotes",
    "href": "models/ana_semantics.html#footnotes",
    "title": "Arabic numeral arithmetic: semantics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Recall that our metalanguage is just the language that we’re using to conduct Introduction to Semantics (LIN4803/LIN6804). It consists of our natural language idiolects (as spoken and written) and a little bit of set-theoretic notation, as well as certain other mathematical notation. I’m assuming our idiolects (as written) make Arabic numeral arithmetic available.↩︎\n After all, according to Keenan and Moss (2016), “if you can’t say something two ways, you can’t say it at all”. Not sure if this is what they had in mind.↩︎",
    "crumbs": [
      "Models",
      "Arabic numeral arithmetic: semantics"
    ]
  },
  {
    "objectID": "models/english.html",
    "href": "models/english.html",
    "title": "A first approximation of English",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\]\n\nAs our second case study, let’s begin developing a syntax and semantics for a small fragment of English. Recall that we are regarding English—just like Arabic numeral arithmetic—as a language (in the formal sense discussed here). Thus one of the things we would like to do is define a particular set of strings—the “grammatical” strings of (some small fragment of) English. We can call this set (i.e., language) ‘\\(L_{\\textit{Eng.}}\\)’.\nAs usual, to define a language, we need an alphabet. For now, let’s define our alphabet to be the one in (1).\n\n\\(Σ_{\\textit{Eng.}} = \\{\\textit{Ziggy}, \\textit{Bella}, \\textit{runs}, \\textit{run}, \\textit{jumps}, \\textit{jump}, \\textit{doesn't}, \\textit{and}\\}\\)\n\nConveniently, the alphabet can also act as our set of atomic expressions—so that when we define the interpretation function \\(I_{\\textit{Eng.}}\\), we can say that the domain of this function is simply the set \\(Σ_{\\textit{Eng.}}\\).\nWithout saying anything more for now about our model for English—\\(\\mathcal{M}_{\\textit{Eng.}}\\)—let’s say a little about the lexicon. Let us work with the lexicon in (2).\n\n\\(⟨\\textit{Ziggy}, I_{\\textit{Eng.}}(Ziggy)⟩ ⊢ np\\)  \\(⟨\\textit{Bella}, I_{\\textit{Eng.}}(Bella)⟩ ⊢ np\\)  \\(⟨\\textit{runs}, I_{\\textit{Eng.}}(runs)⟩ ⊢ vp\\)  \\(⟨\\textit{run}, I_{\\textit{Eng.}}(runs)⟩ ⊢ bvp\\)  \\(⟨\\textit{jumps}, I_{\\textit{Eng.}}(jumps)⟩ ⊢ vp\\)  \\(⟨\\textit{jump}, I_{\\textit{Eng.}}(jump)⟩ ⊢ bvp\\)  \\(⟨\\textit{doesn't}, I_{\\textit{Eng.}}(doesn't)⟩ ⊢ aux\\)  \\(⟨\\textit{and}, I_{\\textit{Eng.}}(and)⟩ ⊢ con\\) \n\nThat is: \\(\\textit{Ziggy}\\) and \\(\\textit{Bella}\\) are our noun phrases; \\(\\textit{runs}\\) and \\(\\textit{jumps}\\) are our basic verb phrases; \\(\\textit{run}\\) and \\(\\textit{jump}\\) are our basic bare verb phrases; \\(\\textit{doesn't}\\) is our one auxiliary; and \\(\\textit{and}\\) is our one connective (for now).\nLet us now provide our full set of grammatical rules. These rules will allow us, all at once, (i) to prove that particular strings are expressions of English, and (ii) to show, for any of these expressions, what its meaning is.\n\n\\[\\begin{prooftree}\n\\AxiomC{$⟨x, ⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ np$}\n\\AxiomC{$⟨y, ⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ vp$}\n\\RightLabel{Rule 1}\\BinaryInfC{$⟨x^{⌢}y, ⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}}(⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}})⟩ ⊢ s$}\n\\end{prooftree}\\]\n\n\\[\\begin{prooftree}\n\\AxiomC{$⟨x, ⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ aux$}\n\\AxiomC{$⟨y, ⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ bvp$}\n\\RightLabel{Rule 2}\\BinaryInfC{$⟨x^{⌢}y, ⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}}(⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}})⟩ ⊢ vp$}\n\\end{prooftree}\\]\n\n\\[\\begin{prooftree}\n\\AxiomC{$⟨x, ⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ bvp$}\n\\AxiomC{$⟨y, ⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ con$}\n\\AxiomC{$⟨z, ⟦z⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ bvp$}\n\\RightLabel{Rule 3}\\TrinaryInfC{$⟨x^{⌢}y^{⌢}z, ⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}}(⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}}, ⟦z⟧_{\\mathcal{M}_{\\textit{Eng.}}})⟩ ⊢ bvp$}\n\\end{prooftree}\\]\n\n\\[\\begin{prooftree}\n\\AxiomC{$⟨x, ⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ vp$}\n\\AxiomC{$⟨y, ⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ con$}\n\\AxiomC{$⟨z, ⟦z⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ vp$}\n\\RightLabel{Rule 4}\\TrinaryInfC{$⟨x^{⌢}y^{⌢}z, ⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}}(⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}}, ⟦z⟧_{\\mathcal{M}_{\\textit{Eng.}}})⟩ ⊢ vp$}\n\\end{prooftree}\\]",
    "crumbs": [
      "Models",
      "A first approximation of English"
    ]
  },
  {
    "objectID": "models/ana_examples.html",
    "href": "models/ana_examples.html",
    "title": "Arabic numeral arithmetic: examples",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\]\n\nIt’s example time.\n\nExample 1\nFor the first example let’s show that \\(⌜537⌝\\) is an \\(np\\) (and that it has a particular interpretation in \\(\\mathcal{M}_{ANA}\\)).\n\\[\\begin{prooftree}\n\\AxiomC{$⟨⌜5⌝, I_{ANA}(⌜5⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⟨⌜5⌝, I_{ANA}(⌜5⌝)⟩ ⊢ n$}\n\\AxiomC{$⟨⌜3⌝, I_{ANA}(⌜3⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⟨⌜3⌝, I_{ANA}(⌜3⌝)⟩ ⊢ n$}\n\\AxiomC{$⟨⌜7⌝, I_{ANA}(⌜7⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⟨⌜7⌝, I_{ANA}(⌜7⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 3}\\BinaryInfC{$⟨⌜37⌝, I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 3}\\BinaryInfC{$⟨⌜537⌝, I_{ANA}(⌜5⌝) * 10^{length(⌜37⌝)} + I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 4}\\UnaryInfC{$⟨⌜537⌝, I_{ANA}(⌜5⌝) * 10^{length(⌜37⌝)} + I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝)⟩ ⊢ np$}\n\\end{prooftree}\\]\nWhat’s proved is the statement in the very last line of this derivation: that the string \\(⌜537⌝\\) has the interpretation \\[I_{ANA}(⌜5⌝) * 10^{length(⌜37⌝)} + I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝)\\] and the category \\(np\\). Since we know that\n\n\\(I_{ANA}(⌜5⌝) = 5\\),\n\\(I_{ANA}(⌜3⌝) = 3\\), and\n\\(I_{ANA}(⌜7⌝) = 7\\)\n\nas well as what \\(length(⌜37⌝)\\) and \\(length(⌜7⌝)\\) are—\\(2\\) and \\(1\\), respectively—the very last line of this proof/derivation can be rewritten as\n\\[5 * 10^{2} + 3 * 10^{1} + 7\\]\nwhich, of course, is \\(537\\).\n\n\nExample 2\nFor the second example, we’ll show that \\(⌜((37 + 1) × 0) = 5⌝\\) is an \\(s\\), and also what its interpretation in \\(\\mathcal{M}_{ANA}\\) is.\n\\[\\begin{prooftree}\n\\AxiomC{$⟨⌜3⌝, I_{ANA}(⌜3⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⟨⌜3⌝, I_{ANA}(⌜3⌝)⟩ ⊢ n$}\n\\AxiomC{$⟨⌜7⌝, I_{ANA}(⌜7⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⟨⌜7⌝, I_{ANA}(⌜7⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 3}\\BinaryInfC{$⟨⌜37⌝, I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 4}\\UnaryInfC{$⟨⌜37⌝, I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝)⟩ ⊢ np$}\n\\AxiomC{$⟨⌜1⌝, I_{ANA}(⌜1⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⌜1⌝, I_{ANA}(⌜1⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 4}\\UnaryInfC{$⌜1⌝, I_{ANA}(⌜1⌝)⟩ ⊢ np$}\n\\RightLabel{Rule 5}\\BinaryInfC{$⟨⌜(37 + 1)⌝, I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝) + I_{ANA}(⌜1⌝)⟩ ⊢ np$}\n\\AxiomC{$⟨⌜0⌝, I_{ANA}(⌜0⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⟨⌜0⌝, I_{ANA}(⌜0⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 4}\\UnaryInfC{$⟨⌜0⌝, I_{ANA}(⌜0⌝)⟩ ⊢ np$}\n\\RightLabel{Rule 5}\\BinaryInfC{$⟨⌜((37 + 1) × 0)⌝, (I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝) + I_{ANA}(⌜1⌝)) * I_{ANA}(⌜0⌝)⟩ ⊢ np$}\n\\AxiomC{$⟨⌜5⌝, I_{ANA}(⌜5⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⟨⌜5⌝, I_{ANA}(⌜5⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 4}\\UnaryInfC{$⟨⌜5⌝, I_{ANA}(⌜5⌝)⟩ ⊢ np$}\n\\RightLabel{Rule 6}\\BinaryInfC{$⟨⌜((37 + 1) × 0) = 5⌝, (I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝) + I_{ANA}(⌜1⌝)) * I_{ANA}(⌜0⌝) = I_{ANA}(⌜5⌝)⟩ ⊢ s$}\n\\end{prooftree}\\]\nIndeed, we’ve proved that \\(⌜((37 + 1) × 0 = 5⌝\\) is an \\(s\\). What we’ve also shown is that the interpretation of this \\(s\\) is \\(\\True\\) just in case\n\\[(I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝) + I_{ANA}(⌜1⌝)) * I_{ANA}(⌜0⌝)\\]\nis equal to \\(I_{ANA}(⌜5⌝)\\).\nIn fact, since we know that \\[I_{ANA}(⌜0⌝) = 0\\] and that multiplying anything by \\(0\\) gives you \\(0\\), we can determine that\n\\[(I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝) + I_{ANA}(⌜1⌝)) * I_{ANA}(⌜0⌝)\\]\nis equal to \\(0\\) without even worrying about what\n\\[I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝) + I_{ANA}(⌜1⌝)\\]\nevaluates to. Though, if we were to evaluate it, we’d of course find that it evaluates to \\(38\\).\nSince \\(I_{ANA}(⌜5⌝)\\) is \\(5\\), the interpretation of this sentence is \\(\\True\\) if and only if \\(0\\) and \\(5\\) are the same number. They’re not, so the interpretation is \\(\\False\\).",
    "crumbs": [
      "Models",
      "Arabic numeral arithmetic: examples"
    ]
  },
  {
    "objectID": "convention/defining_meaning.html",
    "href": "convention/defining_meaning.html",
    "title": "Defining meaning",
    "section": "",
    "text": "Like Lewis (1975), Grice (1957) is engaged in conceptual analysis, but of the English word ‘meaning’. To help pump our intuitions about the distinct ways this word is used, he provides examples analogous to the following:\n\nThis rainy weather means we’ll probably be taxiing for a while.\nJo’s gesture means that she wants you to walk over.\n“Bo devoured the beans” means that Bo ate the beans aggressively.\n\nThe differences among the uses of mean in (1-3) may not be obvious at a single glance, but Grice introduces a number of tests that seem to tease these uses apart.\nWe’ll go through these again now, starting with what I’ll call the contraction test.\n\n\nThis test coordinates the original sentence with a sentence that contradicts the clause selected by mean in the original.\n\n#This rainy weather means we’ll probably be taxiing for a while, but in fact, we won’t be taxiing for a while.\nJo’s gesture means that she wants you to walk over, but in fact, she doesn’t want you to walk over.\n“Bo devoured the beans” means that Bo ate the beans aggressively, but in fact, Bo didn’t eat the beans aggressively.\n\nI’ve marked (1) with a ‘#’ in order to indicate that it is perceived as odd; in particular, it appears to be a contradiction: the use of mean seems to imply the truth of the clause that it selects (we’ll probably be taxiing for a while), while the coordinated clause contradicts it. Meanwhile, (2) and (3) don’t coincide with similar oddness, which suggests that the coordinated clause does not result in a contradiction. (You might feel that the second sentence of (2) is unexpected, but this is likely due to the fact that it describes an odd state of affairs—one in which Jo made the gesture accidentally or as a form of trickery—rather than because it is an odd sentence.)\n\n\n\nWhat I’ll call the passive test involves passivizing the verb mean from the original sentences. The verb is also placed inside the subject of a pseudo-cleft construction, though other tests that involve passivizing and pseud-clefting on their own would suggest that it is really the passive that distinguishes the different uses of mean.1\n\n#What is meant by this rainy weather is that we’ll be taxiing for a while.\nWhat is meant by Jo’s gesture is that she wants you to walk over.\nWhat is meant by “Bo devoured the beans” is that Bo ate the beans aggressively.\n\nHere again, (1) pulls apart from (2) and (3).\n\n\n\nLast, but not least, is what I’ll call the agentivity test. This test modifies the original sentences by combining mean with an agentive subject (and putting the original subject into a by-phrase).\n\n#Someone meant by this rainy weather that we’ll probably be taxiing for a while.\nJo meant by her gesture that she wants you to walk over.\nSomeone meant by “Bo devoured the beans” that Bo ate the beans aggressively.\n\nHey check it out! (1) again pulls apart from (2) and (3).",
    "crumbs": [
      "Convention",
      "Defining meaning"
    ]
  },
  {
    "objectID": "convention/defining_meaning.html#footnotes",
    "href": "convention/defining_meaning.html#footnotes",
    "title": "Defining meaning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Pseudo-cleft constructions are phrases like what Jo bought was apples (an identificational pseudo-cleft) or what Jo bought was tasty (a predicational pseudo-cleft). They generally have the form ‘free relative clause + copula + post-copular phrase’. ↩︎\n Though, see the paper—this is my attempt to paraphrase Grice’s definition.↩︎",
    "crumbs": [
      "Convention",
      "Defining meaning"
    ]
  },
  {
    "objectID": "diagnosing_inference/assignment.html",
    "href": "diagnosing_inference/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "Due on Monday, September 22nd.\n\nPart 1\nConsider the sentences in (1) and (2).\n\nIf it’s raining outside, I’ll bring my raincoat.\nJo: What did you think of the movie?  Bo: It was interesting.\n\nEach of these examples can be understood as involving a conversational implicature. (1) appears to give rise to the implicature that the speaker doesn’t know if it’s raining outside. Meanwhile, (2) seems to imply that Bo may have found the movie less than fantastic. Assuming these implicatures are intended, what conversational maxims are involved in generating them? How are the conversational maxims deployed in each case—go through the reasoning involved (kind of like I did in Applying the maxims). For either one, are there other principles, besides the four conversational maxims, that might be in play?\n\n\nPart 2\nRecall where I mentioned that the sentence in (3) appears to give rise to a scalar implicature, while Bo’s utterance in (4) appears to give rise to an ignorance implicature. The reasoning that is typically hypothesized to generate both implicatures is similar; for example, both the Quantity and Quality maxims are invoked.\n\nJo did some of the readings.\nJo: What do you want?  Bo: Something with a lot of caffeine.\n\nWhat difference(s) between these examples might help explain why you observe only an ignorance implicature in (4)? How does this difference affect the reasoning involved in deriving the implicatures?\n\n\nPart 3\nConsider the examples in (5) and (6).\n\nJo proved that her trunk was empty.  ↝ Jo’s trunk was empty.\nBo doesn’t like his sandwich.  ↝ Bo dislikes his sandwich.\n\nWhat is the status of each of these inferences? That is, is it an entailment, a conversational implicature, a presupposition, a conventional implicature, or not obviously any of these?",
    "crumbs": [
      "Diagnosing inference",
      "Assignment"
    ]
  },
  {
    "objectID": "diagnosing_inference/conventional_implicatures_presuppositions.html",
    "href": "diagnosing_inference/conventional_implicatures_presuppositions.html",
    "title": "Conventional implicatures versus presuppositions",
    "section": "",
    "text": "Disappearing presuppositions\nOne well-studied feature of presuppositions is that, despite the fact that they tend to project, they can also be canceled (or sometimes, modified) under certain circumstances. For example, consider the following cases, in which sentences that normally display presuppositions are made the consequent clauses of certain conditional sentences:\n\nIf Bo has a dog, he brought the dog to his place of work.\nIf Jo is actually good at Go, she loves that she’s good at Go.\n\nIn (1), a sentence that entails there is a dog—the usual presupposition of the consequent clause—is the antecedent of the conditional. In (2), the sentence which is usually the presupposition of the consequent clause is made the antecedent of the conditional. In each case, the entire conditionals themselves don’t appear to have the relevant presuppositions: (1), taken as a whole, does not imply that there is a dog; nor does (2) imply that Jo is good at Go.\nWe obtain a similar pattern if we conjoin the antecedent sentences of these conditionals with the corresponding consequent sentences, and then negate the coordinate structure that result:\n\nIt’s not true that Bo has a dog and that he brought the dog to his place of work.\nIt’s not true that Jo is good at Go and that she loves that she’s good at Go.\n\n\n\nConventional implicatures project\nLike presuppositions, conventional implicatures show projection behavior when subjected to family-of-sentence tests. Consider the example in (4), which features a conventional implicature triggered by a non-restrictive relative clause.\n\nThe class is taught by Julian, who enjoys syntax.\n\nThis sentence implies that Julian enjoys syntax. Like a presupposition, this inference appears to project under negation:\n\nThe class isn’t taught by Julian, who enjoys syntax.\n\nSame with questioning the sentence and placing it into the antecedent of a conditional, which you can check.\nWhat’s notable about conventional implicatures is that if you try to cancel them using means similar to those described above, the result is a bit odd:\n\n#If Julian enjoys syntax, then the class is taught by Julian, who enjoys syntax.\n\nThis sounds weird, suggesting that the conventional implicature is forced to project; that is, what may make this conditional sentence odd is that it triggers the inference that Julian enjoys syntax—due to the non-restrictive relative clause—while at the same time, the fact that Julian enjoys syntax is the antecedent the conditional may trigger an ignorance implicature, i.e., that the utterer doesn’t know whether or not Julian enjoys syntax. So, on the one hand, the utterer believes Julian enjoys syntax (i.e., the conventional implicature), while on the other hand, they are conversationally implicating that they don’t know whether or not he does. (Or something along those lines :).)\nThe point here is that this apparent difference between presuppositions and conventional implicatures can provide a kind of test that helps distinguish between them. For a given candidate presupposition or conventional implicature \\(S\\), ask yourself, “Can I place \\(S\\) into the consequent of a conditional sentence in the way described above, so that it is canceled?” If so, you may have a presupposition. If not (because the result is weird), you may have a conventional implicature.",
    "crumbs": [
      "Diagnosing inference",
      "Conventional implicatures versus presuppositions"
    ]
  },
  {
    "objectID": "diagnosing_inference/presuppositions_entailments.html",
    "href": "diagnosing_inference/presuppositions_entailments.html",
    "title": "Presuppositions versus entailments",
    "section": "",
    "text": "Entailments and presuppositions are typically distinguished by family-of -sentence tests (Chierchia and McConnell-Ginet 1990). Given a sentence which appears to have some entailment or presupposition, one may negate the sentence, turn it into a polar question, or place it into the antecedent of a conditional. If the inference projects (or survives), then we have evidence that it is a presupposition; if it does not, then we have evidence that it is an entailment.\nFor example, both the clause-selecting verbs love and demonstrate appear to give rise to the inference that the clause they select is true;\n\n\nJo demonstrated that she’s good at Go. \nJo loves that she’s good at Go.\n\n\nbut the following examples illustrate that these inferences behave differently under negation.\n\n\nJo didn’t demonstrate that she’s good at Go. \nJo doesn’t love that she’s good at Go.\n\n\nLikewise, under questioning:\n\n\nDid Jo demonstrate that she’s good at Go?\nDoes Jo love that she’s good at Go?\n\n\nSimilarly, when we place the original sentences into the antecedent of a larger conditional sentence:\n\n\nIf Jo demonstrated that she’s good at Go, she’ll have to join the Go league. \nIf Jo loves that she’s good at Go, she’ll have to join the Go league.\n\n\nIn each case, the inference that the clause selected by love is true survives, while the inference that the clause selected by demonstrate is true does not.\nWe observe the same pattern when exchanging an indefinite determiner in some sentence with a definite determiner, suggesting that when an indefinite determiner trigger an entailment, an indefinite determiner triggers a corresponding presupposition.\n\n\nBo brought a dog to his place of work. \nBo brought the dog to his place of work.\n\n\nIn both of these sentences, we observe the inference that there is a dog. If we negate both sentences,\n\n\nBo didn’t bring a dog to his place of work. \nBo didn’t bring the dog to his place of work.\n\n\nwe observe that the inference persists only when there is a definite determiner. Analogous results obtain when we question the original sentences or place them into the antecedents of conditional sentences.\n\n\nDid Bo bring a dog to his place of work? \nDid Bo bring the dog to his place of work?\n\n\nIf Bo brought a dog to his place of work, the boss will be mad. \nIf Bo brought the dog to his place of work, the boss will be mad.",
    "crumbs": [
      "Diagnosing inference",
      "Presuppositions versus entailments"
    ]
  },
  {
    "objectID": "diagnosing_inference/presuppositions_entailments.html#footnotes",
    "href": "diagnosing_inference/presuppositions_entailments.html#footnotes",
    "title": "Presuppositions versus entailments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n What are these factors? If I had a nickel for every time I’ve wondered this, I would be a rich man.↩︎",
    "crumbs": [
      "Diagnosing inference",
      "Presuppositions versus entailments"
    ]
  }
]