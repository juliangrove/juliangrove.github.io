[
  {
    "objectID": "verbs/ditransitives.html",
    "href": "verbs/ditransitives.html",
    "title": "Ditransitive verbs",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\true{\\ct{T}}\n\\def\\false{\\ct{F}}\n\\]\n\nLet’s consider the two sentences in (1), which, arguably, entail each other.\n\n\nPo showed noodles to Tigress. \nPo showed Tigress noodles.\n\n\nTo capture this mutual entailment, we might assign the ditransitive verb showed two distinct lexical entries, as in (2).\n\n\\(⟨\\textit{showed}_{1}, (λx.(λy.(λz.\\ct{show}(z, x, y))))⟩ ⊢ (((np\\backslash s)/pp)/np)\\)  \\(⟨\\textit{showed}_{2}, (λy.(λx.(λz.\\ct{show}(z, x, y))))⟩ ⊢ (((np\\backslash s)/np)/np)\\)\n\nWe can also assign a lexical entry to the preposition to, according to which it is interpreted as the identity function \\((λx.x)\\).\n\n\\(⟨\\textit{to}, (λx.x)⟩ ⊢ (pp/np)\\)\n\nAccording to the first of these lexical entries, the verb showed combines with an object noun phrase; then it combines with a prepositional phrase; and then it combines with a subject noun phrase, yielding, e.g., (1-a). According to the second, it combines with an indirect-object noun phrase; then a direct-object noun phrase; and then the subject, yielding, e.g., (1-b).\nA derivation of the (1-a) sentence would go:\n\\[\\begin{prooftree}\n\\AxiomC{\\(⟨\\textit{Po}, \\ct{p}⟩ ⊢ np⟩\\)}\n\\AxiomC{\\(⟨\\textit{showed}_{1}, (λx.(λy.(λz.\\ct{show}(z, x, y))))⟩ ⊢ (((np\\backslash s)/pp)/np)\\)}\n\\AxiomC{\\(⟨\\textit{noodles}, \\ct{n}⟩ ⊢ np\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(⟨\\textit{showed\\(_{1}\\) noodles}, (λx.(λy.(λz.\\ct{show}(z, x, y))))(\\ct{n})⟩ ⊢ ((np\\backslash s)/pp)\\)}\n\\AxiomC{\\(⟨\\textit{to}, (λx.x)⟩ ⊢ (pp/np)\\)}\n\\AxiomC{\\(\\textit{Tigress}, \\ct{ti} ⟩ ⊢ np\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(⟨\\textit{to Tigress}, (λx.x)(\\ct{ti})⟩ ⊢ pp\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(⟨\\textit{showed\\(_{1}\\) noodles to Tigress}, (λx.(λy.(λz.\\ct{show}(z, x, y))))(\\ct{n})((λx.x)(\\ct{ti}))⟩ ⊢ (np\\backslash s)\\)}\n\\RightLabel{\\(\\backslash\\)}\\BinaryInfC{\\(⟨\\textit{Po showed\\(_{1}\\) noodles to Tigress}, (λx.(λy.(λz.\\ct{show}(z, x, y))))(\\ct{n})((λx.x)(\\ct{ti}))(\\ct{p})⟩ ⊢ s\\)}\n\\end{prooftree}\\]\nAt the end of the day, we end up with the following truth value as the meaning of this sentence:\n\\[(λx.(λy.(λz.\\ct{show}(z, x, y))))(\\ct{n})((λx.x)(\\ct{ti}))(\\ct{p})\\] \\[⇒ (λy.(λz.\\ct{show}(z, \\ct{n}, y))))((λx.x)(\\ct{ti}))(\\ct{p})\\] \\[⇒ (λz.\\ct{show}(z, \\ct{n}, (λx.x)(\\ct{ti})))(\\ct{p})\\] \\[⇒ \\ct{show}(\\ct{p}, \\ct{n}, (λx.x)(\\ct{ti}))\\] \\[⇒ \\ct{show}(\\ct{p}, \\ct{n}, \\ct{ti})\\]\n(It can be helpful to check and make sure you understand each line as the above metalanguage expression is evaluated.)\nMeanwhile, a derivation of (1-b) would go:\n\\[\\begin{prooftree}\n\\AxiomC{\\(⟨\\textit{Po}, \\ct{p}⟩ ⊢ np⟩\\)}\n\\AxiomC{\\(⟨\\textit{showed}_{2}, (λy.(λx.(λz.\\ct{show}(z, x, y))))⟩ ⊢ (((np\\backslash s)/np)/np)\\)}\n\\AxiomC{\\(⟨\\textit{Tigress}, \\ct{ti}⟩ ⊢ np\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(⟨\\textit{showed\\(_{2}\\) Tigress}, (λy.(λx.(λz.\\ct{show}(z, x, y))))(\\ct{ti})⟩ ⊢ ((np\\backslash s)/np)\\)}\n\\AxiomC{\\(⟨\\textit{noodles}, \\ct{n} ⟩ ⊢ np\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(⟨\\textit{showed\\(_{2}\\) Tigress noodles}, (λy.(λx.(λz.\\ct{show}(z, x, y))))(\\ct{ti})(\\ct{n})⟩ ⊢ (np\\backslash s)\\)}\n\\RightLabel{\\(\\backslash\\)}\\BinaryInfC{\\(⟨\\textit{Po showed\\(_{2}\\) Tigress noodles}, (λy.(λx.(λz.\\ct{show}(z, x, y))))(\\ct{ti})(\\ct{n})(\\ct{p})⟩ ⊢ s\\)}\n\\end{prooftree}\\]\nIf we evaluate the result, we get:\n\\[(λy.(λx.(λz.\\ct{show}(z, x, y))))(\\ct{ti})(\\ct{n})(\\ct{p})\\] \\[⇒ (λx.(λz.\\ct{show}(z, x, \\ct{ti})))(\\ct{n})(\\ct{p})\\] \\[⇒ (λz.\\ct{show}(z, \\ct{n}, \\ct{ti}))(\\ct{p})\\] \\[⇒ \\ct{show}(\\ct{p}, \\ct{n}, \\ct{ti})\\]\n(Again, check each line! :)\nThankfully, the resulting interpretation is the same in both cases. Why “thankfully”? Well, because as we saw, the two sentences entail each other!\nAnd importantly, our analysis derives this result precisely because of how we designed the lexical enries for prepositional-dative show and double-object dative show. In particular, how we designed their meanings.",
    "crumbs": [
      "Verbs",
      "Ditransitive verbs"
    ]
  },
  {
    "objectID": "verbs/transitives.html",
    "href": "verbs/transitives.html",
    "title": "Transitive verbs",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\true{\\ct{T}}\n\\def\\false{\\ct{F}}\n\\]\n\nTransitive verbs like punches increase the number of syntactic and semantic arguments of intransitives by one. To analyze the sentence in (1),\n\nTigress punches Po.\n\nwe can assign a lexical entry like the one in (2) to this verb.\n\n\\(⟨\\textit{punches}, (λx.(λy.\\ct{punch}(y, x)))⟩ ⊢ ((np\\backslash s)/np)\\)\n\nIn this case, we have a function of the following type:\n\\[(λx.(λy.\\ct{punch}(y, x))) : D_{e} → D_{e} → \\{\\true, \\false\\}\\]\nThis function is defined in terms of a relation on entities, \\(\\ct{punch}\\). That is, given two entities \\(x\\) and \\(y\\), \\(\\ct{punch}(y, x)\\) is some truth value.\nThen, given the lexical entry for Tigress where it denotes \\(\\ct{ti}\\), we can derive the meaning and syntactic category of the sentence in (1):\n\\[\\begin{prooftree}\n\\AxiomC{\\(⟨\\textit{Tigress}, \\ct{ti}⟩ ⊢ np\\)}\n\\AxiomC{\\(⟨\\textit{punches}, (λx.(λy.\\ct{punch}(y, x)))⟩ ⊢ (np\\backslash s)/np\\)}\n\\AxiomC{\\(⟨\\textit{Po}, \\ct{p}⟩ ⊢ np\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(⟨\\textit{punches Po}, (λx.(λy.\\ct{punch}(y, x)))(\\ct{p})⟩ ⊢ (np\\backslash s)\\)}\n\\RightLabel{\\(\\backslash\\)}\\BinaryInfC{\\(⟨\\textit{Tigress punches Po}, (λx.(λy.\\ct{punch}(y, x)))(\\ct{p})(\\ct{ti})⟩ ⊢ s\\)}\n\\end{prooftree}\\]\nSince \\((λx.(λy.\\ct{punch}(y, x)))\\) takes two entities in order, we can get a function takes one entity by applying it first to \\(\\ct{p}\\):\n\\[(λx.(λy.\\ct{punch}(y, x)))(\\ct{p})\\] \\[⇒ (λy.\\ct{punch}(y, \\ct{p}))\\]\nThis function is itself a characteristic function of a set of entities in \\(D_{e}\\). As a result, if we apply it to two entities, we get a truth value:\n\\[(λx.(λy.\\ct{punch}(y, x)))(\\ct{p})(\\ct{ti})\\] \\[⇒ (λy.\\ct{punch}(x, \\ct{p}))(\\ct{ti})\\] \\[⇒ \\ct{punch}(\\ct{ti}, \\ct{p})\\]\nThis result, \\(\\ct{punch}(\\ct{ti}, \\ct{p})\\), will be \\(\\true\\) or \\(\\false\\), depending on the model.",
    "crumbs": [
      "Verbs",
      "Transitive verbs"
    ]
  },
  {
    "objectID": "verbs/assignment.html",
    "href": "verbs/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\true{\\ct{T}}\n\\def\\false{\\ct{F}}\n\\]\n\nDue on Wednesday, November 5th.\n\nPart 1\nThis exercise is basically just like Part 1 of the previous assignment. I want you to have a chance to practice a bit more figuring out what syntactic category and semantic type something has, based on a description of its distribution. Importantly, none of the expression behaviors I will describe here necessarily correspond to anything you actually see in English or any other language. The point of the exercise is for you to get comfortable translating the informal description in English into a formal description of an expression’s syntactic category and semantic type.\nAgain, I will describe the behavior of an expression in English—what it can combine with, and what it makes as a result—and again, I want you to:\n\nsay what its syntactic category is in “parentheses” notation;\nsay what its syntactic category is in “tree” notation;\nsay what its semantic type is, assuming the category-type correspondence discussed in Syntactic categories of the previous module. As before, you can use either the usual notation or the “tree” notation for semantic types here.\n\n\nThis expression takes a preposition to its left, and then a ditransitive verb to its right, to form a noun phrase.\nThis expression combines with a coordinator of verb phrases to its right (e.g., and), and then a preposition to its left, to form a sentence.\nThis expression combines with an expression \\(X\\) to its left, where \\(X\\) is something that takes a coordinator of sentences on its left to form a verb phrase, and together with \\(X\\), it forms a preposition.\n\n\n\nPart 2\nEnglish has a syntactic voice distinction between active verbs and passive verbs. This means that in addition to sentences like (1), where we see the transitive verb ate,\n\nTigress ate noodles.\n\nit is also possible to form sentences like (2), in which the very same verb has been passivized.\n\nNoodles were eaten.\n\nTo analyze sentences with passive verbs in them, let’s assume that we have another base syntactic category:\n\\[v_{\\textit{pass.}}\\]\nAssume that this base category is associated with the following semantic type:\n\\[(e → t)\\]\nThus semantically, \\(v_{\\textit{pass.}}\\) is just like the syntactic category \\((np\\backslash s)\\) of verb phrases!\nFurther, let’s assume that ate has the following lexical entry:\n\\[⟨\\textit{ate}, (λx.(λy.\\ct{eat}(y, x)))⟩ ⊢ ((np\\backslash s)/np)\\]\nIt takes two semantic arguments, one corresponding to the direct object and one corresponding to the subject, and it gives back \\(\\true\\) if the second argument eats the first argument (and it gives back \\(\\false\\), otherwise).\nTo analyze the passive verb eaten, let’s assume that it has the following lexical entry:\n\\[⟨\\textit{eaten}, (λx.\\{y ∣ \\ct{eat}(y, x)\\} ≠ \\varnothing)⟩ ⊢ v_{\\textit{pass.}}\\]\nIn words, the interpretation of eaten is a characteristic function of some set of entities. Specifically, it is the characteristic function of the set of entities such that the set of things that eat them is not the empty set.\nProvide a lexical entry for the auxiliary verb were that allows you to derive the sentence in (2).\n\n\nPart 3\nAssume, as usual, that Tigress is a noun phrase whose interpretation is \\(\\ct{ti}\\), and that noodles is a noun phrase whose interpretation is \\(\\ct{n}\\). Provide a derivation of the sentence in (1). Use your lexical entry for were to provide a derivation for the sentence in (2). Do the meanings you obtain for the two sentences help explain the fact that (1) entails (2)?",
    "crumbs": [
      "Verbs",
      "Assignment"
    ]
  },
  {
    "objectID": "diagnosing_inference/presuppositions_entailments.html",
    "href": "diagnosing_inference/presuppositions_entailments.html",
    "title": "Presuppositions versus entailments",
    "section": "",
    "text": "Entailments and presuppositions are typically distinguished by family-of -sentence tests (Chierchia and McConnell-Ginet 1990). Given a sentence which appears to have some entailment or presupposition, one may negate the sentence, turn it into a polar question, or place it into the antecedent of a conditional. If the inference projects (or survives), then we have evidence that it is a presupposition; if it does not, then we have evidence that it is an entailment.\nFor example, both the clause-selecting verbs love and demonstrate appear to give rise to the inference that the clause they select is true;\n\n\nJo demonstrated that she’s good at Go. \nJo loves that she’s good at Go.\n\n\nbut the following examples illustrate that these inferences behave differently under negation.\n\n\nJo didn’t demonstrate that she’s good at Go. \nJo doesn’t love that she’s good at Go.\n\n\nLikewise, under questioning:\n\n\nDid Jo demonstrate that she’s good at Go?\nDoes Jo love that she’s good at Go?\n\n\nSimilarly, when we place the original sentences into the antecedent of a larger conditional sentence:\n\n\nIf Jo demonstrated that she’s good at Go, she’ll have to join the Go league. \nIf Jo loves that she’s good at Go, she’ll have to join the Go league.\n\n\nIn each case, the inference that the clause selected by love is true survives, while the inference that the clause selected by demonstrate is true does not.\nWe observe the same pattern when exchanging an indefinite determiner in some sentence with a definite determiner, suggesting that when an indefinite determiner trigger an entailment, an indefinite determiner triggers a corresponding presupposition.\n\n\nBo brought a dog to his place of work. \nBo brought the dog to his place of work.\n\n\nIn both of these sentences, we observe the inference that there is a dog. If we negate both sentences,\n\n\nBo didn’t bring a dog to his place of work. \nBo didn’t bring the dog to his place of work.\n\n\nwe observe that the inference persists only when there is a definite determiner. Analogous results obtain when we question the original sentences or place them into the antecedents of conditional sentences.\n\n\nDid Bo bring a dog to his place of work? \nDid Bo bring the dog to his place of work?\n\n\nIf Bo brought a dog to his place of work, the boss will be mad. \nIf Bo brought the dog to his place of work, the boss will be mad.",
    "crumbs": [
      "Diagnosing inference",
      "Presuppositions versus entailments"
    ]
  },
  {
    "objectID": "diagnosing_inference/presuppositions_entailments.html#footnotes",
    "href": "diagnosing_inference/presuppositions_entailments.html#footnotes",
    "title": "Presuppositions versus entailments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n What are these factors? If I had a nickel for every time I’ve wondered this, I would be a rich man.↩︎",
    "crumbs": [
      "Diagnosing inference",
      "Presuppositions versus entailments"
    ]
  },
  {
    "objectID": "diagnosing_inference/conventional_implicatures_presuppositions.html",
    "href": "diagnosing_inference/conventional_implicatures_presuppositions.html",
    "title": "Conventional implicatures versus presuppositions",
    "section": "",
    "text": "Disappearing presuppositions\nOne well-studied feature of presuppositions is that, despite the fact that they tend to project, they can also be canceled (or sometimes, modified) under certain circumstances. For example, consider the following cases, in which sentences that normally display presuppositions are made the consequent clauses of certain conditional sentences:\n\nIf Bo has a dog, he brought the dog to his place of work.\nIf Jo is actually good at Go, she loves that she’s good at Go.\n\nIn (1), a sentence that entails there is a dog—the usual presupposition of the consequent clause—is the antecedent of the conditional. In (2), the sentence which is usually the presupposition of the consequent clause is made the antecedent of the conditional. In each case, the entire conditionals themselves don’t appear to have the relevant presuppositions: (1), taken as a whole, does not imply that there is a dog; nor does (2) imply that Jo is good at Go.\nWe obtain a similar pattern if we conjoin the antecedent sentences of these conditionals with the corresponding consequent sentences, and then negate the coordinate structure that result:\n\nIt’s not true that Bo has a dog and that he brought the dog to his place of work.\nIt’s not true that Jo is good at Go and that she loves that she’s good at Go.\n\n\n\nConventional implicatures project\nLike presuppositions, conventional implicatures show projection behavior when subjected to family-of-sentence tests. Consider the example in (4), which features a conventional implicature triggered by a non-restrictive relative clause.\n\nThe class is taught by Julian, who enjoys syntax.\n\nThis sentence implies that Julian enjoys syntax. Like a presupposition, this inference appears to project under negation:\n\nThe class isn’t taught by Julian, who enjoys syntax.\n\nSame with questioning the sentence and placing it into the antecedent of a conditional, which you can check.\nWhat’s notable about conventional implicatures is that if you try to cancel them using means similar to those described above, the result is a bit odd:\n\n#If Julian enjoys syntax, then the class is taught by Julian, who enjoys syntax.\n\nThis sounds weird, suggesting that the conventional implicature is forced to project; that is, what may make this conditional sentence odd is that it triggers the inference that Julian enjoys syntax—due to the non-restrictive relative clause—while at the same time, the fact that Julian enjoys syntax is the antecedent the conditional may trigger an ignorance implicature, i.e., that the utterer doesn’t know whether or not Julian enjoys syntax. So, on the one hand, the utterer believes Julian enjoys syntax (i.e., the conventional implicature), while on the other hand, they are conversationally implicating that they don’t know whether or not he does. (Or something along those lines :).)\nThe point here is that this apparent difference between presuppositions and conventional implicatures can provide a kind of test that helps distinguish between them. For a given candidate presupposition or conventional implicature \\(S\\), ask yourself, “Can I place \\(S\\) into the consequent of a conditional sentence in the way described above, so that it is canceled?” If so, you may have a presupposition. If not (because the result is weird), you may have a conventional implicature.",
    "crumbs": [
      "Diagnosing inference",
      "Conventional implicatures versus presuppositions"
    ]
  },
  {
    "objectID": "diagnosing_inference/assignment.html",
    "href": "diagnosing_inference/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "Due on Monday, September 22nd.\n\nPart 1\nConsider the sentences in (1) and (2).\n\nIf it’s raining outside, I’ll bring my raincoat.\nJo: What did you think of the movie?  Bo: It was interesting.\n\nEach of these examples can be understood as involving a conversational implicature. (1) appears to give rise to the implicature that the speaker doesn’t know if it’s raining outside. Meanwhile, (2) seems to imply that Bo may have found the movie less than fantastic. Assuming these implicatures are intended, what conversational maxims are involved in generating them? How are the conversational maxims deployed in each case—go through the reasoning involved (kind of like I did in Applying the maxims). For either one, are there other principles, besides the four conversational maxims, that might be in play?\n\n\nPart 2\nRecall where I mentioned that the sentence in (3) appears to give rise to a scalar implicature, while Bo’s utterance in (4) appears to give rise to an ignorance implicature. The reasoning that is typically hypothesized to generate both implicatures is similar; for example, both the Quantity and Quality maxims are invoked.\n\nJo did some of the readings.\nJo: What do you want?  Bo: Something with a lot of caffeine.\n\nWhat difference(s) between these examples might help explain why you observe only an ignorance implicature in (4)? How does this difference affect the reasoning involved in deriving the implicatures?\n\n\nPart 3\nConsider the examples in (5) and (6).\n\nJo proved that her trunk was empty.  ↝ Jo’s trunk was empty.\nBo doesn’t like his sandwich.  ↝ Bo dislikes his sandwich.\n\nWhat is the status of each of these inferences? That is, is it an entailment, a conversational implicature, a presupposition, a conventional implicature, or not obviously any of these?",
    "crumbs": [
      "Diagnosing inference",
      "Assignment"
    ]
  },
  {
    "objectID": "applicative_cg/sem_types.html",
    "href": "applicative_cg/sem_types.html",
    "title": "Semantic types",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\true{\\ct{T}}\n\\def\\false{\\ct{F}}\n\\]\n\nLet’s start with semantic types.\nWe’ll have two atomic types: \\(e\\) and \\(t\\). Recall that \\(e\\) is the type of entities, or elements of the domain \\(D\\), for whatever model \\(\\mathcal{M}\\) we happen to be using to interpret English. One way we can state this correspondence between types and what they mean is by saying that \\(D_{e}\\)—the domain of the type \\(e\\)—is just the domain \\(D\\) of the model.\nCrucially, \\(e\\) is the semantic type of noun phrases like Ziggy and Bella. This just amounts to saying that noun phrases take their interpretations from the domain of the type \\(e\\): noun phrases are interpreted in \\(D_{e}\\), the domain of the model.\nLikewise for prepositional phrases like to Ziggy and to Bella. Instead of saying that the preposition to itself has any interesting semantic function, we’ve been saying that it doesn’t modify the meaning of the noun phrase that it takes as its syntactic complement: it takes that meaning in as an argument and spits it right back out. As a result, the whole prepositional phrase has semantic type \\(e\\), just like its noun phrase complement.\nMeanwhile, \\(t\\) is the type of truth values, \\(\\true\\) and \\(\\false\\). In turn, the domain of type \\(t\\), \\(D_{t}\\), is just the set \\(\\{\\true, \\false\\}\\). Finally, \\(t\\) is the semantic type of sentences like Ziggy meows and Bella runs—expressions that can be true or false in some model.\nThus so far we have the following correspondence:\n\n\n\nSyntactic category\nSemantic type\nDomain\n\n\n\n\n\\(np\\)\n\\(e\\)\n\\(D_{e} = D\\)\n\n\n\\(pp\\)\n\\(e\\)\n\\(D_{e} = D\\)\n\n\n\\(s\\)\n\\(t\\)\n\\(D_{t} = \\{\\true, \\false\\}\\)\n\n\n\nBut we would like to be able to have semantic types not just for noun phrases and sentences, but for any syntactic category that an expression might possibly have. So let’s use our two atomic types \\(e\\) and \\(t\\) to generate a whole collection of types that we can pull from.\nHere is how we can do this. In addition to \\(e\\) and \\(t\\), let’s have a rule for forming new types—the one in (1).\n\nIf \\(α\\) and \\(β\\) are types, then \\((α → β)\\) is also a type.\n\nFor example, since \\(e\\) and \\(t\\) are types, then by (1), so is \\((e → t)\\). But now that \\((e → t)\\) is a type, (1) may apply again: we can use \\(e\\), \\(t\\), and \\((e → t)\\) to form a bunch more types, like \\((e → (e → t))\\), \\(((e → t) → t)\\), \\((e → (e → (e → t)))\\), and an infinite number more!\n\nDomains for arrow types\nWhat about the domains associated with these types—that is, types with arrows in them? To pair arrow types up with the domains they correspond to, we’ll use the rule in (2), where \\(x\\) and \\(y\\) can be any types at all.\n\n\\(D_{(x → y)} = \\{f\\ |\\ \\text{$f$ is a function from $D_{x}$ to $D_{y}$}\\}\\)\n\nThus the domain of an arrow type is a set of functions. Specifically, it is the set of functions from the domain of the type on the left-hand side of the arrow onto the domain of the type on the right-hand side of the arrow.\n\nThree examples\nLet’s look at a few examples of how the rule (2) applies in practice, for three different semantic types: \\((e → t)\\), \\((e → (e → t))\\), and \\(((e → t) → t)\\).\n\nFirst example\nLet’s start with \\((e → t)\\). By (2), \\(D_{(e → t)}\\) is the following set of functions:\n\\[D_{(e → t)} = \\{f\\ |\\ \\text{$f$ is a function from $D_{e}$ to $D_{t}$}\\}\\]\nThis set is the set of functions from the domain \\(D\\) of the model to the set of truth values (\\(\\{\\true, \\false\\}\\)). These functions are just characteristic functions of sets of elements of the domain.\nFor example, if the domain of the model just has the two elements \\(\\ct{z}\\) and \\(\\ct{b}\\)\n\\[D = \\{\\ct{z}, \\ct{b}\\}\\]\nthen there are a total of four functions in the set \\(D_{(e → t)}\\). One of the functions in this set is\n\\[(λx.\\{\\ct{z}\\}_{CF}(x))\\]\n—the function that gives back \\(\\true\\) when its argument is \\(\\ct{z}\\), and which gives back \\(\\false\\) when its argument is \\(\\ct{b}\\). Another function in this set is\n\\[(λx.\\{\\ct{b}\\}_{CF}(x))\\]\n—the function that gives back \\(\\false\\) when its argument is \\(\\ct{z}\\), and which gives back \\(\\true\\) when its argument is \\(\\ct{b}\\). Another one is\n\\[(λx.\\true)\\]\nwhich gives back \\(\\true\\) for all entities (it can therefore also be written as \\((λx.\\{\\ct{z}, \\ct{b}\\}_{CF}(x))\\)). And finally, the function\n\\[(λx.\\false)\\]\nwhich gives back \\(\\false\\) for all entities (and which can therefore also be written as \\((λx.\\varnothing_{CF})\\)).\n\n\nSecond example\nLet’s consider the domain of the type \\((e → (e → t)))\\)—that is, \\(D_{(e → (e → t))}\\). Following the definition in (2), we have\n\\[D_{(e → (e → t))} = \\{f\\ |\\ \\text{$f$ is a function from $D_{e}$ to $D_{(e → t)}$}\\}\\]\nIn other words, a function in the set \\(D_{(e → (e → t))}\\) maps entities onto functions in the set \\(D_{(e → t)}\\).\nBut remember that we just said what \\(D_{(e → t)}\\) is: it’s the set which has four functions in it. Thus a function in \\(D_{(e → (e → t))}\\) maps any entity onto one of these four functions.\nAn example of such a function is\n\\[(λx.(λy.\\{\\ct{z}\\}_{CF}(y)))\\]\nThis function maps either \\(\\ct{z}\\) or \\(\\ct{b}\\) onto the function \\((λy.\\{\\ct{z}\\}_{CF}(y))\\)—this result is just the first function described above!\nAnother example is the function\n\\[(λx.(λy.(x = y)))\\]\nThis function maps \\(\\ct{z}\\) onto the function \\((λy.\\{\\ct{z}\\}_{CF}(y))\\), while it maps \\(\\ct{b}\\) onto the function \\((λy.\\{\\ct{b}\\}_{CF}(y))\\). (Why is this true?)\n\n\n\nThird example\nLet’s now consider the domain of the type \\(((e → t) → t)\\)—that is, \\(D_{((e → t) → t)}\\). Again, following the definition in (2), we have\n\\[D_{((e → t) → t)} = \\{f\\ |\\ \\text{$f$ is a function from $D_{(e → t)}$ to $D_{t}$}\\}\\]\nThat is, a function in \\(D_{((e → t) → t)}\\) maps any of the functions in \\(D_{(e → t)}\\) to the set \\(\\{\\true, \\false\\}\\). Put differently, any function in this set is certain type of characteristic function! But instead of being a characteristic function of a subset of the domain of the model (\\(D_{e}\\)), it is a characteristic function of a subset of \\(D_{(e → t)}\\). Using our running example, where \\(D_{(e → t)}\\) has as elements four different functions, a function in \\(D_{((e → t) → t)}\\) can be said to characterize a set containing any of these four.\nHere is an example of such a function:\n\\[(λf.f(\\ct{b}))\\]\nThis function takes as its argument one of the functions in \\(D_{(e → t)}\\). It then applies this function—the one it took as its argument—to Bella and just gives back the truth value which results.\nIf we apply this function to \\((λx.\\{\\ct{z}, \\ct{b}\\}_{CF}(x))\\), for example, we get:\n\\[(λf.f(\\ct{b}))(λx.\\{\\ct{z}, \\ct{b}\\}_{CF}(x))\\] \\[⇒ (λx.\\{\\ct{z}, \\ct{b}\\}_{CF}(x))(\\ct{b})\\] \\[⇒ \\{\\ct{z}, \\ct{b}\\}_{CF}(\\ct{b})\\] \\[⇒ \\true\\]",
    "crumbs": [
      "Applicative categorial grammar",
      "Semantic types"
    ]
  },
  {
    "objectID": "applicative_cg/assignment.html",
    "href": "applicative_cg/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\true{\\ct{T}}\n\\def\\false{\\ct{F}}\n\\]\n\nDue on Wednesday, October 29th.\n\nPart 1\nIn the following exercise, I will describe the behavior of an expression in English—that is, what it can combine with, and what it makes as a result. Based on my description, I want you to:\n\nsay what its syntactic category is in “parentheses” notation;\nsay what its syntactic category is in “tree” notation;\nsay what its semantic type is, assuming the category-type correspondence discussed in Syntactic categories. You can use either the usual notation or the “tree” notation for semantic types in this part.\n\n\nThis expression takes a transitive verb to its left, and then a noun phrase to its left, to form a sentence.\nThis expression combines with a preposition to its right, and then a verb phrase to its left, to form a transitive verb.\nThis expression combines with an expression \\(X\\) to its right, where \\(X\\) is something that takes a preposition on its left to form a verb phrase, and together with \\(X\\), it forms a preposition.\n\n\n\nPart 2\nRecall that in A first approximation of English of the Models module, we assigned the connective and the category \\(con\\) in order to account for sentences like (1).\n\nZiggy runs and jumps.\n\nProvide a new lexical entry for and that uses the syntactic category system defined in Syntactic categories. Provide lexical entries for runs and jumps (including their meanings), and give a derivation of the sentence in (1).\n\n\nPart 3\nConsider the following sentence, which features an adjective in predicate position.\n\nZiggy is fast.\n\nAssume that there is a new base category for adjective phrases—call it \\(ap\\)—which is the syntactic category of the adjective fast.\nWhat semantic type do you think this category should be associated with? Keep in mind that this semantic type doesn’t have to be \\(e\\) or \\(t\\)—it can be something more complex!\nWhat syntactic category should the copula is have? Given this syntactic category, what is the semantic type of is?",
    "crumbs": [
      "Applicative categorial grammar",
      "Assignment"
    ]
  },
  {
    "objectID": "convention/defining_meaning.html",
    "href": "convention/defining_meaning.html",
    "title": "Defining meaning",
    "section": "",
    "text": "Like Lewis (1975), Grice (1957) is engaged in conceptual analysis, but of the English word ‘meaning’. To help pump our intuitions about the distinct ways this word is used, he provides examples analogous to the following:\n\nThis rainy weather means we’ll probably be taxiing for a while.\nJo’s gesture means that she wants you to walk over.\n“Bo devoured the beans” means that Bo ate the beans aggressively.\n\nThe differences among the uses of mean in (1-3) may not be obvious at a single glance, but Grice introduces a number of tests that seem to tease these uses apart.\nWe’ll go through these again now, starting with what I’ll call the contraction test.\n\n\nThis test coordinates the original sentence with a sentence that contradicts the clause selected by mean in the original.\n\n#This rainy weather means we’ll probably be taxiing for a while, but in fact, we won’t be taxiing for a while.\nJo’s gesture means that she wants you to walk over, but in fact, she doesn’t want you to walk over.\n“Bo devoured the beans” means that Bo ate the beans aggressively, but in fact, Bo didn’t eat the beans aggressively.\n\nI’ve marked (1) with a ‘#’ in order to indicate that it is perceived as odd; in particular, it appears to be a contradiction: the use of mean seems to imply the truth of the clause that it selects (we’ll probably be taxiing for a while), while the coordinated clause contradicts it. Meanwhile, (2) and (3) don’t coincide with similar oddness, which suggests that the coordinated clause does not result in a contradiction. (You might feel that the second sentence of (2) is unexpected, but this is likely due to the fact that it describes an odd state of affairs—one in which Jo made the gesture accidentally or as a form of trickery—rather than because it is an odd sentence.)\n\n\n\nWhat I’ll call the passive test involves passivizing the verb mean from the original sentences. The verb is also placed inside the subject of a pseudo-cleft construction, though other tests that involve passivizing and pseud-clefting on their own would suggest that it is really the passive that distinguishes the different uses of mean.1\n\n#What is meant by this rainy weather is that we’ll be taxiing for a while.\nWhat is meant by Jo’s gesture is that she wants you to walk over.\nWhat is meant by “Bo devoured the beans” is that Bo ate the beans aggressively.\n\nHere again, (1) pulls apart from (2) and (3).\n\n\n\nLast, but not least, is what I’ll call the agentivity test. This test modifies the original sentences by combining mean with an agentive subject (and putting the original subject into a by-phrase).\n\n#Someone meant by this rainy weather that we’ll probably be taxiing for a while.\nJo meant by her gesture that she wants you to walk over.\nSomeone meant by “Bo devoured the beans” that Bo ate the beans aggressively.\n\nHey check it out! (1) again pulls apart from (2) and (3).",
    "crumbs": [
      "Convention",
      "Defining meaning"
    ]
  },
  {
    "objectID": "convention/defining_meaning.html#footnotes",
    "href": "convention/defining_meaning.html#footnotes",
    "title": "Defining meaning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Pseudo-cleft constructions are phrases like what Jo bought was apples (an identificational pseudo-cleft) or what Jo bought was tasty (a predicational pseudo-cleft). They generally have the form ‘free relative clause + copula + post-copular phrase’. ↩︎\n Though, see the paper—this is my attempt to paraphrase Grice’s definition.↩︎",
    "crumbs": [
      "Convention",
      "Defining meaning"
    ]
  },
  {
    "objectID": "models/ana_examples.html",
    "href": "models/ana_examples.html",
    "title": "Arabic numeral arithmetic: examples",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\]\n\nIt’s example time.\n\nExample 1\nFor the first example let’s show that \\(⌜537⌝\\) is an \\(np\\) (and that it has a particular interpretation in \\(\\mathcal{M}_{ANA}\\)).\n\\[\\begin{prooftree}\n\\AxiomC{$⟨⌜5⌝, I_{ANA}(⌜5⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⟨⌜5⌝, I_{ANA}(⌜5⌝)⟩ ⊢ n$}\n\\AxiomC{$⟨⌜3⌝, I_{ANA}(⌜3⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⟨⌜3⌝, I_{ANA}(⌜3⌝)⟩ ⊢ n$}\n\\AxiomC{$⟨⌜7⌝, I_{ANA}(⌜7⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⟨⌜7⌝, I_{ANA}(⌜7⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 3}\\BinaryInfC{$⟨⌜37⌝, I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 3}\\BinaryInfC{$⟨⌜537⌝, I_{ANA}(⌜5⌝) * 10^{length(⌜37⌝)} + I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 4}\\UnaryInfC{$⟨⌜537⌝, I_{ANA}(⌜5⌝) * 10^{length(⌜37⌝)} + I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝)⟩ ⊢ np$}\n\\end{prooftree}\\]\nWhat’s proved is the statement in the very last line of this derivation: that the string \\(⌜537⌝\\) has the interpretation \\[I_{ANA}(⌜5⌝) * 10^{length(⌜37⌝)} + I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝)\\] and the category \\(np\\). Since we know that\n\n\\(I_{ANA}(⌜5⌝) = 5\\),\n\\(I_{ANA}(⌜3⌝) = 3\\), and\n\\(I_{ANA}(⌜7⌝) = 7\\)\n\nas well as what \\(length(⌜37⌝)\\) and \\(length(⌜7⌝)\\) are—\\(2\\) and \\(1\\), respectively—the very last line of this proof/derivation can be rewritten as\n\\[5 * 10^{2} + 3 * 10^{1} + 7\\]\nwhich, of course, is \\(537\\).\n\n\nExample 2\nFor the second example, we’ll show that \\(⌜((37 + 1) × 0) = 5⌝\\) is an \\(s\\), and also what its interpretation in \\(\\mathcal{M}_{ANA}\\) is.\n\\[\\begin{prooftree}\n\\AxiomC{$⟨⌜3⌝, I_{ANA}(⌜3⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⟨⌜3⌝, I_{ANA}(⌜3⌝)⟩ ⊢ n$}\n\\AxiomC{$⟨⌜7⌝, I_{ANA}(⌜7⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⟨⌜7⌝, I_{ANA}(⌜7⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 3}\\BinaryInfC{$⟨⌜37⌝, I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 4}\\UnaryInfC{$⟨⌜37⌝, I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝)⟩ ⊢ np$}\n\\AxiomC{$⟨⌜1⌝, I_{ANA}(⌜1⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⌜1⌝, I_{ANA}(⌜1⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 4}\\UnaryInfC{$⌜1⌝, I_{ANA}(⌜1⌝)⟩ ⊢ np$}\n\\RightLabel{Rule 5}\\BinaryInfC{$⟨⌜(37 + 1)⌝, I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝) + I_{ANA}(⌜1⌝)⟩ ⊢ np$}\n\\AxiomC{$⟨⌜0⌝, I_{ANA}(⌜0⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⟨⌜0⌝, I_{ANA}(⌜0⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 4}\\UnaryInfC{$⟨⌜0⌝, I_{ANA}(⌜0⌝)⟩ ⊢ np$}\n\\RightLabel{Rule 5}\\BinaryInfC{$⟨⌜((37 + 1) × 0)⌝, (I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝) + I_{ANA}(⌜1⌝)) * I_{ANA}(⌜0⌝)⟩ ⊢ np$}\n\\AxiomC{$⟨⌜5⌝, I_{ANA}(⌜5⌝)⟩ ⊢ atom$}\n\\RightLabel{Rule 2}\\UnaryInfC{$⟨⌜5⌝, I_{ANA}(⌜5⌝)⟩ ⊢ n$}\n\\RightLabel{Rule 4}\\UnaryInfC{$⟨⌜5⌝, I_{ANA}(⌜5⌝)⟩ ⊢ np$}\n\\RightLabel{Rule 6}\\BinaryInfC{$⟨⌜((37 + 1) × 0) = 5⌝, (I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝) + I_{ANA}(⌜1⌝)) * I_{ANA}(⌜0⌝) = I_{ANA}(⌜5⌝)⟩ ⊢ s$}\n\\end{prooftree}\\]\nIndeed, we’ve proved that \\(⌜((37 + 1) × 0 = 5⌝\\) is an \\(s\\). What we’ve also shown is that the interpretation of this \\(s\\) is \\(\\True\\) just in case\n\\[(I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝) + I_{ANA}(⌜1⌝)) * I_{ANA}(⌜0⌝)\\]\nis equal to \\(I_{ANA}(⌜5⌝)\\).\nIn fact, since we know that \\[I_{ANA}(⌜0⌝) = 0\\] and that multiplying anything by \\(0\\) gives you \\(0\\), we can determine that\n\\[(I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝) + I_{ANA}(⌜1⌝)) * I_{ANA}(⌜0⌝)\\]\nis equal to \\(0\\) without even worrying about what\n\\[I_{ANA}(⌜3⌝) * 10^{length(⌜7⌝)} + I_{ANA}(⌜7⌝) + I_{ANA}(⌜1⌝)\\]\nevaluates to. Though, if we were to evaluate it, we’d of course find that it evaluates to \\(38\\).\nSince \\(I_{ANA}(⌜5⌝)\\) is \\(5\\), the interpretation of this sentence is \\(\\True\\) if and only if \\(0\\) and \\(5\\) are the same number. They’re not, so the interpretation is \\(\\False\\).",
    "crumbs": [
      "Models",
      "Arabic numeral arithmetic: examples"
    ]
  },
  {
    "objectID": "models/english.html",
    "href": "models/english.html",
    "title": "A first approximation of English",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\]\n\nAs our second case study, let’s begin developing a syntax and semantics for a small fragment of English. Recall that we are regarding English—just like Arabic numeral arithmetic—as a language (in the formal sense discussed here). Thus one of the things we would like to do is define a particular set of strings—the “grammatical” strings of (some small fragment of) English. We can call this set (i.e., language) ‘\\(L_{\\textit{Eng.}}\\)’.\nAs usual, to define a language, we need an alphabet. For now, let’s define our alphabet to be the one in (1).\n\n\\(Σ_{\\textit{Eng.}} = \\{\\textit{Ziggy}, \\textit{Bella}, \\textit{runs}, \\textit{run}, \\textit{jumps}, \\textit{jump}, \\textit{doesn't}, \\textit{and}\\}\\)\n\nConveniently, the alphabet can also act as our set of atomic expressions—so that when we define the interpretation function \\(I_{\\textit{Eng.}}\\), we can say that the domain of this function is simply the set \\(Σ_{\\textit{Eng.}}\\).\nWithout saying anything more for now about our model for English—\\(\\mathcal{M}_{\\textit{Eng.}}\\)—let’s say a little about the lexicon. Let us work with the lexicon in (2).\n\n\\(⟨\\textit{Ziggy}, I_{\\textit{Eng.}}(Ziggy)⟩ ⊢ np\\)  \\(⟨\\textit{Bella}, I_{\\textit{Eng.}}(Bella)⟩ ⊢ np\\)  \\(⟨\\textit{runs}, I_{\\textit{Eng.}}(runs)⟩ ⊢ vp\\)  \\(⟨\\textit{run}, I_{\\textit{Eng.}}(runs)⟩ ⊢ bvp\\)  \\(⟨\\textit{jumps}, I_{\\textit{Eng.}}(jumps)⟩ ⊢ vp\\)  \\(⟨\\textit{jump}, I_{\\textit{Eng.}}(jump)⟩ ⊢ bvp\\)  \\(⟨\\textit{doesn't}, I_{\\textit{Eng.}}(doesn't)⟩ ⊢ aux\\)  \\(⟨\\textit{and}, I_{\\textit{Eng.}}(and)⟩ ⊢ con\\) \n\nThat is: \\(\\textit{Ziggy}\\) and \\(\\textit{Bella}\\) are our noun phrases; \\(\\textit{runs}\\) and \\(\\textit{jumps}\\) are our basic verb phrases; \\(\\textit{run}\\) and \\(\\textit{jump}\\) are our basic bare verb phrases; \\(\\textit{doesn't}\\) is our one auxiliary; and \\(\\textit{and}\\) is our one connective (for now).\nLet us now provide our full set of grammatical rules. These rules will allow us, all at once, (i) to prove that particular strings are expressions of English, and (ii) to show, for any of these expressions, what its meaning is.\n\n\\[\\begin{prooftree}\n\\AxiomC{$⟨x, ⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ np$}\n\\AxiomC{$⟨y, ⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ vp$}\n\\RightLabel{Rule 1}\\BinaryInfC{$⟨x^{⌢}y, ⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}}(⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}})⟩ ⊢ s$}\n\\end{prooftree}\\]\n\n\\[\\begin{prooftree}\n\\AxiomC{$⟨x, ⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ aux$}\n\\AxiomC{$⟨y, ⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ bvp$}\n\\RightLabel{Rule 2}\\BinaryInfC{$⟨x^{⌢}y, ⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}}(⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}})⟩ ⊢ vp$}\n\\end{prooftree}\\]\n\n\\[\\begin{prooftree}\n\\AxiomC{$⟨x, ⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ bvp$}\n\\AxiomC{$⟨y, ⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ con$}\n\\AxiomC{$⟨z, ⟦z⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ bvp$}\n\\RightLabel{Rule 3}\\TrinaryInfC{$⟨x^{⌢}y^{⌢}z, ⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}}(⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}}, ⟦z⟧_{\\mathcal{M}_{\\textit{Eng.}}})⟩ ⊢ bvp$}\n\\end{prooftree}\\]\n\n\\[\\begin{prooftree}\n\\AxiomC{$⟨x, ⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ vp$}\n\\AxiomC{$⟨y, ⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ con$}\n\\AxiomC{$⟨z, ⟦z⟧_{\\mathcal{M}_{\\textit{Eng.}}}⟩ ⊢ vp$}\n\\RightLabel{Rule 4}\\TrinaryInfC{$⟨x^{⌢}y^{⌢}z, ⟦y⟧_{\\mathcal{M}_{\\textit{Eng.}}}(⟦x⟧_{\\mathcal{M}_{\\textit{Eng.}}}, ⟦z⟧_{\\mathcal{M}_{\\textit{Eng.}}})⟩ ⊢ vp$}\n\\end{prooftree}\\]",
    "crumbs": [
      "Models",
      "A first approximation of English"
    ]
  },
  {
    "objectID": "models/ana_semantics.html",
    "href": "models/ana_semantics.html",
    "title": "Arabic numeral arithmetic: semantics",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\]\nWhat we would like to do is figure out a way of associating elements of \\(L_{ANA}\\) with meanings. In order to do this, we have to make use of a model—that is, one of the things that we introduced here.\nWhich model, exactly, should we be using to interpret \\(L_{ANA}\\)? We’ll follow the standard way of doing things and say that the model of \\(L_{ANA}\\) is the one defined in (1), which has as its domain the set \\(ℕ\\) of natural numbers.\nThat is, \\(D\\) is defined to be \\(ℕ\\)—i.e., the set \\(\\{0, 1, 2, ...\\}\\).\nWhat can we say about the model’s interpretation function, \\(I_{ANA}\\)? Well, because we defined \\(atom\\) (here) to be the set \\(\\{⌜0⌝, ..., ⌜9⌝\\}\\), we know that this set is the domain of \\(I_{ANA}\\): it is the job of the model’s interpretation function to map things from this set onto interpretations of some kind.\nWhat about the codomain of \\(I_{ANA}\\)? Since we’re having \\(I_{ANA}\\) map elements from the set \\(\\{⌜0⌝, ..., ⌜9⌝\\}\\), we should choose the codomain of \\(I_{ANA}\\)—the set it maps these symbols to—to be \\(ℕ\\): the interpretation of any of the symbols \\(⌜0⌝\\) through \\(⌜9⌝\\) is just a natural number!\nSpecifically, we’ll define \\(I_{ANA}\\) as in (2).\nHopefully, none of these choices is particularly surprising! Still, it’s important to point out that—while it looks a bit like we are using the interpretation function to simply “remove the corner brackets (\\(⌜, ⌝\\))”—that’s just due to the fact that we are employing the very language that we are interpreting as part of our metalanguage.1 Said another way: there is nothing special about the symbol \\(⌜1⌝\\) that makes it a particularly good candidate for being mapped onto the number 1. The choice reflects how we use the language \\(L_{ANA}\\), but it is arbitrary nonetheless—a genuine Lewis-style convention.\nAlright. The time has come to define our interpretation function (or “interpretation brackets”, “denotation function”, what have you) \\(⟦·⟧_{\\mathcal{M}_{ANA}}\\) over the entirety of \\(L_{ANA}\\). We will show how to do this using two different presentations.2 One presentation uses English and provides the rules mostly as conditional if then statements. The other presentation is a bit more formally systematic: it provides formal rules for constructing proofs. Importantly, both presentations say exactly the same thing.",
    "crumbs": [
      "Models",
      "Arabic numeral arithmetic: semantics"
    ]
  },
  {
    "objectID": "models/ana_semantics.html#footnotes",
    "href": "models/ana_semantics.html#footnotes",
    "title": "Arabic numeral arithmetic: semantics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Recall that our metalanguage is just the language that we’re using to conduct Introduction to Semantics (LIN4803/LIN6804). It consists of our natural language idiolects (as spoken and written) and a little bit of set-theoretic notation, as well as certain other mathematical notation. I’m assuming our idiolects (as written) make Arabic numeral arithmetic available.↩︎\n After all, according to Keenan and Moss (2016), “if you can’t say something two ways, you can’t say it at all”. Not sure if this is what they had in mind.↩︎",
    "crumbs": [
      "Models",
      "Arabic numeral arithmetic: semantics"
    ]
  },
  {
    "objectID": "models/assignment.html",
    "href": "models/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\]\n\nDue on Monday, October 13th.\n\n\n\n\n\n\n\n\nPart 1\nThere is a planet orbiting Proxima Centauri that has a population of organisms which are very human-like. As it happens, a group within the larger population uses a system a lot like Arabic numeral arithmetic to perform arithmetic calculations, except that the syntax and semantics are slightly different. This group uses a base-7 number system, and the atomic numerals have different meanings than they do in Arabic numeral arithmetic. We can call the language they use for arithmetic calculations \\(L_{NANA}\\) (not Arabic numeral arithmetic).\nTo be more precise, for this group within the population which lives on the planet orbiting Proxima Centauri, the set of atomic numerals is the following one:\n\\[atom = \\{⌜1⌝, ⌜2⌝, ⌜3⌝, ⌜4⌝, ⌜5⌝, ⌜6⌝, ⌜7⌝\\}\\]\nMeanwhile, the interpretation function of the model employed by this group, \\(I_{NANA}\\), is the following one:\n\\[I_{NANA}(⌜1⌝) = 6\\] \\[I_{NANA}(⌜2⌝) = 5\\] \\[I_{NANA}(⌜3⌝) = 4\\] \\[I_{NANA}(⌜4⌝) = 3\\] \\[I_{NANA}(⌜5⌝) = 2\\] \\[I_{NANA}(⌜6⌝) = 1\\] \\[I_{NANA}(⌜7⌝) = 0\\]\nThe full model of \\(L_{NANA}\\) employed by this group is thus:\n\\[\\mathcal{M}_{NANA} = ⟨ℕ, \\{\\True, \\False\\}, I_{NANA}⟩\\]\nOne more thing. Instead of the Rule 3 which we defined in Arabic numeral arithmetic: semantics, these guys use the following alternative Rule 3:\n\\[\\begin{prooftree}\n\\AxiomC{$⟨x, ⟦x⟧_{\\mathcal{M}_{ANA}}⟩ ⊢ n$}\n\\AxiomC{$⟨y, ⟦y⟧_{\\mathcal{M}_{ANA}}⟩ ⊢ n$}\n\\RightLabel{Rule 3}\\BinaryInfC{$⟨x^{⌢}y, ⟦x⟧_{\\mathcal{M}_{ANA}} * 7^{length(y)} + ⟦y⟧_{\\mathcal{M}_{ANA}}⟩ ⊢ n$}\n\\end{prooftree}\\]\nThe rules of for generating strings of \\(L_{NANA}\\) and their interpretations are otherwise identical to those for generating strings of \\(L_{ANA}\\) and their interpretations.\nShow that \\(⌜((53 + 6) × 5) = 24⌝\\) is a sentence (i.e., an \\(s\\)) of \\(L_{NANA}\\), and show what its interpretation in \\(\\mathcal{M}_{NANA}\\) is.\n\n\nPart 2\nUsing the English grammar fragment defined in A first approximation of English in conjunction with the model defined in An English model and example, show that the string in (1) is a sentence (i.e., an \\(s\\)), and show what its interpretation is.\n\nBella doesn’t jump and run.\n\n\n\nPart 3\nAs you know, English does not only have sentences like (1). It also has sentences like (2).\n\nBella doesn’t jump and Bella doesn’t run.\n\nLet us propose that in addition to the word and, English has another word, and2, which is pronounced exactly the same way as and is pronounced (i.e., [æːnd]), but which coordinates sentences instead of verb phrases. Thus and is the variant of the coordinator that appears in (1), while and2 is the variant of the coordinator that appears in (2).\nProvide a lexical entry for the variant of the coordinator which appears in (2)—something that can be added to the lexicon given in A first approximation of English. Give a derivation showing that (2) is a sentence with a meaning, using this lexical entry. Does your lexical entry ensure that (1) and (2) entail each other? If so, how does it ensure this?",
    "crumbs": [
      "Models",
      "Assignment"
    ]
  },
  {
    "objectID": "coordinators/examples.html",
    "href": "coordinators/examples.html",
    "title": "Some examples",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\true{\\ct{T}}\n\\def\\false{\\ct{F}}\n\\]\n\nTo explain these facts, it would be useful to have a systematic account of the meanings of and and or. Let’s start by providing the sentence-coordinator meanings in terms of some lexical entries. For each lexical entry, we’ll annotate the word itself with a category name, just to keep coordinators of different categories distinct (e.g., we’ll call the sentence-level and \\(\\textit{and}_{s}\\) and the sentence-level or \\(\\textit{or}_{s}\\)).\n\nSentence coordinators\nLet’s start with sentences.\n\n\n\\(⟨\\textit{and}_{s}, (λa.(λb.a = b = \\true))⟩ ⊢ ((s\\backslash s)/s)\\)\n\\(⟨\\textit{or}_{s}, (λa.(λb.¬(a = b = \\false)))⟩ ⊢ ((s\\backslash s)/s)\\)\n\n\nThus for example, the sentence in (2)\n\nPo slept or Tigress slept.\n\nhas the following derivation:\n\\[\\begin{prooftree}\n\\AxiomC{\\(⟨\\textit{Po}, \\ct{p}⟩ ⊢ np\\)}\n\\AxiomC{\\(⟨\\textit{slept}, (λx.\\ct{sleep}(x))⟩ ⊢ (np\\backslash s)\\)}\n\\RightLabel{\\(\\backslash\\)}\\BinaryInfC{\\(⟨\\textit{Po slept}, (λx.\\ct{sleep}(x))(\\ct{p})⟩ ⊢  s\\)}\n\\AxiomC{\\(⟨\\textit{or}_{s}, (λa.(λb.¬(a = b = \\false)))⟩ ⊢ ((s\\backslash s)/s)\\)}\n\\AxiomC{\\(⟨\\textit{Tigress}, \\ct{ti}⟩ ⊢ np\\)}\n\\AxiomC{\\(⟨\\textit{slept}, (λx.\\ct{sleep}(x))⟩ ⊢ (np\\backslash s)\\)}\n\\RightLabel{\\(\\backslash\\)}\\BinaryInfC{\\(⟨\\textit{Tigress slept}, (λx.\\ct{sleep}(x))(\\ct{ti})⟩ ⊢  s\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(⟨\\textit{or\\(_{s}\\) Tigress slept}, (λa.(λb.¬(a = b = \\false)))((λx.\\ct{sleep}(x))(\\ct{ti}))⟩ ⊢  (s\\backslash s)\\)}\n\\RightLabel{\\(\\backslash\\)}\\BinaryInfC{\\(⟨\\textit{Po slept or\\(_{s}\\) Tigress slept}, (λa.(λb.¬(a = b = \\false)))((λx.\\ct{sleep}(x))(\\ct{ti}))((λx.\\ct{sleep}(x))(\\ct{p}))⟩ ⊢  s\\)}\n\\end{prooftree}\\]\nWe should then evaluate the meaning representation on the very last line. Given that the category of the resulting string is \\(s\\), the meaning should be a truth value.\nTaking things one step at a time—and evaluating the outermost expression before the innermost two—we obtain:\n\\[(λa.(λb.¬(a = b = \\false)))((λx.\\ct{sleep}(x))(\\ct{ti}))((λx.\\ct{sleep}(x))(\\ct{p}))\\] \\[⇒ (λb.¬((λx.\\ct{sleep}(x))(\\ct{ti}) = b = \\false))((λx.\\ct{sleep}(x))(\\ct{p}))\\] \\[⇒ ¬((λx.\\ct{sleep}(x))(\\ct{ti}) = (λx.\\ct{sleep}(x))(\\ct{p}) = \\false)\\] \\[⇒ ¬(\\ct{sleep}(\\ct{ti}) = (λx.\\ct{sleep}(x))(\\ct{p}) = \\false)\\] \\[⇒ ¬(\\ct{sleep}(\\ct{ti}) = \\ct{sleep}(\\ct{p}) = \\false)\\]\nAs we can see, the result is \\(\\true\\) just in case it is not false both that Tigress slept and that Po slept—that is, if at least one of them slept.\n\n\nVerb phrase coordinators\nWhat about the versions of and and or when they coordinate verb phrases? In this case, they should both have the same syntatic category:\n\\[(((np\\backslash s)\\backslash(np\\backslash s))/(np\\backslash s))\\]\nThis category can be rendered in tree notation as follows:\n        (/)\n        / \\\n       /   \\\n      /     \\\n    (\\)     (\\)\n    / \\     / \\\n   /   \\   np  s\n (\\)   (\\)\n / \\   / \\\nnp  s np  s\nBecause they have this category, \\(vp\\)-coordinating and and or should also have the following semantic type:\n\\[((e → t) → ((e → t) → (e → t)))\\]\nOr in tree notation:\n       ─&gt;\n      / \\\n     /   \\\n    /     \\\n   ─&gt;      ─&gt;\n  / \\     / \\\n e   t   /   \\\n        /     \\\n       ─&gt;      ─&gt;\n      / \\     / \\\n     e   t   e   t\nThus analogous to the \\(s\\)-coordinators from above, the meanings of \\(vp\\)-coordinators should take two arguments in order; but this time, both of these arguments are characteristic functions of sets of entities (functions of type \\((e → t)\\)). Furthermore, they should give back a new characteristic function.\nThe lexical entries in (3) accomplish this.\n\n\n\\(⟨\\textit{and}_{(np\\backslash s)}, (λf.(λg.(λx.f(x) = g(x) = \\true)))⟩ ⊢ (((np\\backslash s)\\backslash(np\\backslash s))/(np\\backslash s))\\)\n\\(⟨\\textit{or}_{(np\\backslash s)}, (λf.(λg.(λx.¬(f(x) = g(x) = \\false))))⟩ ⊢ (((np\\backslash s)\\backslash(np\\backslash s))/(np\\backslash s))\\)\n\n\nThus for example, the sentence in (4)\n\nPo slept or jumped.\n\nhas the following derivation:\n\\[\\begin{prooftree}\n\\AxiomC{\\(⟨\\textit{Po}, \\ct{p}⟩ ⊢ np\\)}\n\\AxiomC{\\(⟨\\textit{slept}, (λx.\\ct{sleep}(x))⟩ ⊢ (np\\backslash s)\\)}\n\\AxiomC{\\(⟨\\textit{or}_{(np\\backslash s)}, (λf.(λg.(λx.¬(f(x) = g(x) = \\false))))⟩ ⊢ (((np\\backslash s)\\backslash(np\\backslash s))/(np\\backslash s))\\)}\n\\AxiomC{\\(⟨\\textit{jumped}, (λx.\\ct{jump}(x))⟩ ⊢ (np\\backslash s)\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(⟨\\textit{or\\(_{(np\\backslash s)}\\) jumped}, (λf.(λg.(λx.¬(f(x) = g(x) = \\false))))((λx.\\ct{jump}(x)))⟩ ⊢  ((np\\backslash s)\\backslash(np\\backslash s))\\)}\n\\RightLabel{\\(\\backslash\\)}\\BinaryInfC{\\(⟨\\textit{slept or\\(_{(np\\backslash s)}\\) jumped}, (λf.(λg.(λx.¬(f(x) = g(x) = \\false))))((λx.\\ct{jump}(x)))((λx.\\ct{sleep}(x))⟩ ⊢ (np\\backslash s)\\)}\n\\RightLabel{\\(\\backslash\\)}\\BinaryInfC{\\(⟨\\textit{Po slept or\\(_{(np\\backslash s)}\\) jumped}, (λf.(λg.(λx.¬(f(x) = g(x) = \\false))))((λx.\\ct{jump}(x)))((λx.\\ct{sleep}(x)))(\\ct{p})⟩ ⊢ s\\)}\n\\end{prooftree}\\]\nAgain, we should evaluate the result:\n\\[(λf.(λg.(λx.¬(f(x) = g(x) = \\false))))((λx.\\ct{jump}(x)))((λx.\\ct{sleep}(x)))(\\ct{p})\\] \\[⇒ (λg.(λx.¬((λx.\\ct{jump}(x))(x) = g(x) = \\false)))((λx.\\ct{sleep}(x)))(\\ct{p})\\] \\[⇒ (λx.¬((λx.\\ct{jump}(x))(x) = (λx.\\ct{sleep}(x))(x) = \\false))(\\ct{p})\\] \\[⇒ ¬((λx.\\ct{jump}(x))(\\ct{p}) = (λx.\\ct{sleep}(x))(\\ct{p}) = \\false)\\] \\[⇒ ¬(\\ct{jump}(\\ct{p}) = (λx.\\ct{sleep}(x))(\\ct{p}) = \\false)\\] \\[⇒ ¬(\\ct{jump}(\\ct{p}) = \\ct{sleep}(\\ct{p}) = \\false)\\]\nIn this case, the resulting truth value is \\(\\true\\) just in case it isn’t false both that Po jumped and that Po slept—Po must have done at least one.\n\n\nTransitive verb coordinators\nWhat about when and and or coordinate transitive verbs? Transitive verbs have the syntactic category \\(((np\\backslash s)/np)\\), so the coordinators should have the following syntactic category, in this case:\n\\[((((np\\backslash s)/np)\\backslash((np\\backslash s)/np))/((np\\backslash s)/np))\\]\nOr in tree notation:\n               (/)\n               / \\\n              /   \\\n             /     \\\n            /       \\\n           /         \\\n          /           \\\n        (\\)           (/)\n        / \\           / \\\n       /   \\        (\\)  np\n      /     \\       / \\\n    (/)     (/)    np  s\n    / \\     / \\\n  (\\)  np (\\)  np\n  / \\     / \\\n np  s   np  s\nUsing the usual mapping from categories to types, we can see that the two coordinators should therefore have the following semantic type:\n\\[((e → (e → t)) → ((e → (e → t)) → (e → (e → t))))\\]\nOr in tree notation:\n        ─&gt;\n       / \\\n      /   \\\n     /     \\\n    /       \\\n   /         \\\n  ─&gt;          ─&gt;\n / \\         / \\\ne  ─&gt;       /   \\\n  / \\      /     \\\n e   t    ─&gt;      ─&gt;\n         / \\     / \\\n        e   ─&gt;  e   ─&gt;\n           / \\     / \\\n          e   t   e   t\n\nAgain, their meanings each take two arguments in order, but now both of the arguments are functions of type \\((e → (e → t))\\): they are functions that each take two entities (in order) and give back a truth value. The lexical entries in (5) assign the coordinators meanings that do this.\n\n\n\\(⟨\\textit{and}_{((np\\backslash s)/np)}, (λf.(λg.(λx.(λy.f(x)(y) = g(x)(y) = \\true)))⟩ ⊢ ((((np\\backslash s)/np)\\backslash((np\\backslash s)/np))/((np\\backslash s)/np))\\)\n\\(⟨\\textit{or}_{((np\\backslash s)/np)}, (λf.(λg.(λx.(λy.¬(f(x) = g(x) = \\false)))))⟩ ⊢ ((((np\\backslash s)/np)\\backslash((np\\backslash s)/np))/((np\\backslash s)/np))\\)\n\n\nThus for example, the sentence in (6)\n\nPo cooked or ate noodles.\n\nhas the following derivation:\n\\[\\begin{prooftree}\n\\AxiomC{\\(⟨\\textit{Po}, \\ct{p}⟩ ⊢ np\\)}\n\\AxiomC{\\(⟨\\textit{cooked}, (λx.(λy.\\ct{cook}(y, x)))⟩ ⊢ ((np\\backslash s)/np)\\)}\n\\AxiomC{\\(⟨\\textit{or}_{((np\\backslash s)/np)}, (λf.(λg.(λx.(λy.¬(f(x)(y) = g(x)(y) = \\false))))⟩ ⊢ ((((np\\backslash s)/np)\\backslash((np\\backslash s)/np))/((np\\backslash s)/np))\\)}\n\\AxiomC{\\(⟨\\textit{ate}, (λx.(λy.\\ct{eat}(y, x)))⟩ ⊢ ((np\\backslash s)/np)\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(⟨\\textit{or\\(_{((np\\backslash s)/np)}\\) ate}, (λf.(λg.(λx.(λy.¬(f(x)(y) = g(x)(y) = \\false)))))((λx.(λy.\\ct{eat}(y, x))))⟩ ⊢  (((np\\backslash s)/np)\\backslash((np\\backslash s)/np))\\)}\n\\RightLabel{\\(\\backslash\\)}\\BinaryInfC{\\(⟨\\textit{cooked or\\(_{((np\\backslash s)/np)}\\) ate}, (λf.(λg.(λx.(λy.¬(f(x)(y) = g(x)(y) = \\false)))))((λx.(λy.\\ct{eat}(y, x))))((λx.(λy.\\ct{cook}(y, x))))⟩ ⊢ ((np\\backslash s)/np)\\)}\n\\AxiomC{\\(⟨\\textit{noodles}, \\ct{n}⟩ ⊢ np\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(⟨\\textit{cooked or\\(_{((np\\backslash s)/np)}\\) ate noodles}, (λf.(λg.(λx.(λy.¬(f(x)(y) = g(x)(y) = \\false)))))((λx.(λy.\\ct{eat}(y, x))))((λx.(λy.\\ct{cook}(y, x))))(\\ct{n})⟩ ⊢ (np\\backslash s)\\)}\n\\RightLabel{\\(\\backslash\\)}\\BinaryInfC{\\(⟨\\textit{Po cooked or\\(_{((np\\backslash s)/np)}\\) ate noodles}, (λf.(λg.(λx.(λy.¬(f(x)(y) = g(x)(y) = \\false)))))((λx.(λy.\\ct{eat}(y, x))))((λx.(λy.\\ct{cook}(y, x))))(\\ct{n})(\\ct{p})⟩ ⊢ s\\)}\n\\end{prooftree}\\]\nThe resulting meaning representation is quite grisly! But as usual, we can tame it—we just have to be careful when we substitute arguments for the variables that stand for them inside of the relevant functions. Going again from the outside in, we can evaluate the result by substituting in the four arguments of the function denoted by or (i.e., corresponding to the two verbs, the object, and the subject, respectively):\n\\[(λf.(λg.(λx.(λy.¬(f(x)(y) = g(x)(y) = \\false)))))((λx.(λy.\\ct{eat}(y, x))))((λx.(λy.\\ct{cook}(y, x))))(\\ct{n})(\\ct{p})\\] \\[⇒ (λg.(λx.(λy.¬((λx.(λy.\\ct{eat}(y, x)))(x)(y) = g(x)(y) = \\false)))))((λx.(λy.\\ct{cook}(y, x))))(\\ct{n})(\\ct{p})\\] \\[⇒ (λx.(λy.¬((λx.(λy.\\ct{eat}(y, x)))(x)(y) = (λx.(λy.\\ct{cook}(y, x)))(x)(y) = \\false)))(\\ct{n})(\\ct{p})\\] \\[⇒ (λy.¬((λx.(λy.\\ct{eat}(y, x)))(\\ct{n})(y) = (λx.(λy.\\ct{cook}(y, x)))(\\ct{n})(y) = \\false))(\\ct{p})\\] \\[⇒ ¬((λx.(λy.\\ct{eat}(y, x)))(\\ct{n})(\\ct{p}) = (λx.(λy.\\ct{cook}(y, x)))(\\ct{n})(\\ct{p}) = \\false)\\]\nWe just have to evaluate the meanings of the two verbs themselves, each now applied to its two arguments. Let’s take care of one of these meanings at a time, starting with the meaning of ate:\n\\[¬((λx.(λy.\\ct{eat}(y, x)))(\\ct{n})(\\ct{p}) = (λx.(λy.\\ct{cook}(y, x)))(\\ct{n})(\\ct{p}) = \\false)\\] \\[⇒ ¬((λy.\\ct{eat}(y, \\ct{n}))(\\ct{p}) = (λx.(λy.\\ct{cook}(y, x)))(\\ct{n})(\\ct{p}) = \\false)\\] \\[⇒ ¬(\\ct{eat}(\\ct{p}, \\ct{n}) = (λx.(λy.\\ct{cook}(y, x)))(\\ct{n})(\\ct{p}) = \\false)\\]\nFinally, let’s take care of the meaning of cook, applied to its arguments:\n\\[¬(\\ct{eat}(\\ct{p}, \\ct{n}) = (λx.(λy.\\ct{cook}(y, x)))(\\ct{n})(\\ct{p}) = \\false)\\] \\[⇒ ¬(\\ct{eat}(\\ct{p}, \\ct{n}) = (λy.\\ct{cook}(y, \\ct{n}))(\\ct{p}) = \\false)\\] \\[⇒ ¬(\\ct{eat}(\\ct{p}, \\ct{n}) = \\ct{cook}(\\ct{p}, \\ct{n}) = \\false)\\]\nAs we can see, (6) is taken to have the value \\(\\true\\) just in case it isn’t false both that Po ate noodles and that Po cooked noodles—that is, one of them has to be true. Seems about right!",
    "crumbs": [
      "Coordinators",
      "Some examples"
    ]
  },
  {
    "objectID": "coordinators/overview.html",
    "href": "coordinators/overview.html",
    "title": "Overview",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\true{\\ct{T}}\n\\def\\false{\\ct{F}}\n\\]\n\nThese notes aim to get a better handle on the syntax and semantics of coordinators, like and and or. Something special about coordinators, at least in English, is that (at least, apparently) they can coordinate expressions of any category. For example, while we can coordinate sentences and verb phrases, as in (1)\n\n\nPo slept {and, or} Tigress jumped.\nPo slept {and, or} jumped.\n\n\nwe can also coordinate noun phrases, transitive verbs, prepositional phrases, and perhaps even other categories like prepositions, as in (2).\n\n\nPo {and, or} Tigress slept.\nTigress punched {and, or} helped Po.\nPo cooked noodles in the kitchen {and, or} for Tigress.\nPo cooked noodles inside {and, or} outside the kitchen.\n\n\nNot only is it possible to coordinate strings of various categories, but when we do, a systematic pattern of inference arises. Specifically: when we coordinate constituents smaller than sentences, what’s produced is a sentence which seems to mean the same thing as a sentence-level coordination. For example, the sentences in (2) appear to mean the same things as the sentences in (3), respectively, when the same coordinator occurs in both.\n\n\nPo slept {and, or} Tigress slept.\nTigress punched Po {and, or} Tigress helped Po.\nPo cooked noodles in the kitchen {and, or} Po cooked noodles for Tigress.\nPo cooked noodles inside the kitchen {and, or} Po cooked noodles outside the kitchen.\n\n\nIn each case, the (2) sentence entails the (3) sentence, and vice versa. How can we explain this?",
    "crumbs": [
      "Coordinators",
      "Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to Intro to Semantics!1\nI’m Julian Grove - I’m an Assistant Professor in the Linguistics Department (where this course is offered).\nHere is my contact info:\nCheck out the About page for more info about me and this site.\nHere are some facts about this course:",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#birds-eye-view",
    "href": "index.html#birds-eye-view",
    "title": "Syllabus",
    "section": "Bird’s-eye view",
    "text": "Bird’s-eye view\nThe goals of this course are to acquaint you with the study of meaning in natural language; specifically, to equip you with a certain vocabulary of concepts, as well as a certain collection of theoretical and methodological tools. This vocabulary and these tools will allow you to:\n\ncollect linguistic data and determine whether and how it is relevant to meaning;\nformulate and test hypotheses and theoretical claims about linguistic meaning; and\napproach questions about how linguistic meaning is situated within the broader scientific study of natural language and the human mind.\n\nThe first few weeks of the course will be more-or-less stage setting. Natural language semantics is a young field whose basic aims and guiding principles are somewhat in flux; so it will be useful to have this part of the course be dedicated to making these aims and principles fairly explicit. After these introductory weeks, we will get deep into the practice of doing semantics by focusing on a variety of empirical phenomena. The main acts will be verbs and their arguments, coordination, modification, anaphora, and quantification.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#attendance-and-class-participation",
    "href": "index.html#attendance-and-class-participation",
    "title": "Syllabus",
    "section": "Attendance and class participation",
    "text": "Attendance and class participation\nCome to class! Attendance won’t be graded, but you’re likely to do better in the course if you come than if you don’t.\nWhen you’re here, participate! You’ll get more out of the class if you do, and it’ll be more fun.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#assignments-and-grading",
    "href": "index.html#assignments-and-grading",
    "title": "Syllabus",
    "section": "Assignments and grading",
    "text": "Assignments and grading\n\nGrading\nYour work in this course will consist of nine (9) short written assignments—sometimes based on assigned readings—as well as an in-person final exam. Each written assignments will be a collection of responses to questions: some of these responses will have a mini-essay format, and some will be more like answers to problem sets (it’ll be clear in any given case what the expectation is). Each assignment will be available on this site, at the end of the set of notes to which the assignment corresponds. Please turn in a physical copy of your responses at the beginning of the class on which the assignment is due.\nThe written assignments will be graded on a 5-point scale, using the following rubric:\n\n5: you thoroughly engage with each question on the assignment. This means that you justify each of your responses, as well point out its implications, if appropriate. For example, if the truth of your response is likely to have any unintuitive consequences, you should note and explain these. Doing this well will sometimes be challenging and require you to think a lot.\nIf the assignment has questions requiring the application of analytical techniques that we have introduced in the course, “thorough engagement”, for current purposes, means applying these techniques in the right way.\n4: you thoroughly engage with most, but not all, of the questions.\n3: you thoroughly engage with only a small part of the assignment and give shallow or cursory answers to most of it.\n2: you give shallow or cursory answers to all of the questions on the assignment.\n1: you barely do the assignment.\n0: you don’t even barely do the assignment.\n\nThe written assignments will account for 90% of your grade, and the final exam, for 10%. The final will take place from 10am-12pm on December 10th (place TBD). It will be formatted similarly to the assignments.\nThe following grading scale will be used to determine course grades:\n\n\n\n93.5 ≤ A ≤ 100\n89.5 ≤ A- &lt; 93.5\n\n\n\n86.5 ≤ B+ &lt; 89.5\n83.5 ≤ B &lt; 86.5\n79.5 ≤ B- &lt; 83.5\n\n\n76.5 ≤ C+ &lt; 79.5\n74.5 ≤ C &lt; 76.5\n69.5 ≤ C &lt; 74.5\n\n\n\n\n\nCollaboration (including with chatbots)\nWe’ll talk about this in class. Basically:\n\nPlease actually do the readings yourself.\nPlease complete assignments yourself.\nIf you worked with someone else in the class (or a chatbot) on an assignment, please indicate that you did. If you used a chatbot, please include a note with your assignment describing how you used it.\n\n\n\nMissed or late work\nLate work will not be accepted unless you’ve arranged with me beforehand. If you need an extension, ask for one—just do it prior to the due date.\n\n\nExtra credit\nIf you like, you can get extra credit, accounting for up to an additional 2% of your course grade, in one of two ways:\n\nYou can earn up to two credits by participating in linguistics studies through the Linguistics Department’s SONA account (so, one percentage point of your grade per credit). This document has more information; so does this video. If you go this route, please participate in the studies before November 22.\nYou can read and summarize the main arguments of a (short) semantics paper, i.e., published in a journal, collection, or conference proceedings. Here are some paper suggestions (which I will continue to add to during the semester), but note that you can also summarize a paper of your choosing, so long as you consult with me about it first.\n\nGlass (2025)\nKarttunen (1971)\nKlecha (2014)\nSteinert-Threlkeld et al. (2023)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#zulip",
    "href": "index.html#zulip",
    "title": "Syllabus",
    "section": "Zulip",
    "text": "Zulip\nThere is a Zulip site for this course at uf-semantics-2025.zulipchat.com. We’ll use it to host discussions of the course material. You can post (and answer!) questions there. I will chime in, as well.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#university-honesty-policy",
    "href": "index.html#university-honesty-policy",
    "title": "Syllabus",
    "section": "University Honesty Policy",
    "text": "University Honesty Policy\nUF students are bound by The Honor Pledge which states, “We, the members of the University of Florida community, pledge to hold ourselves and our peers to the highest standards of honor and integrity by abiding by the Honor Code.” On all work submitted for credit by students at the University of Florida, the following pledge is either required or implied: “On my honor, I have neither given nor received unauthorized aid in doing this assignment.” The Honor Code (policy.ufl.edu/regulation/4-040/) specifies a number of behaviors that are in violation of this code and the possible sanctions. Furthermore, you are obligated to report any condition that facilitates academic misconduct to appropriate personnel. If you have any questions or concerns, please consult with the instructor.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#counseling-and-wellness-center",
    "href": "index.html#counseling-and-wellness-center",
    "title": "Syllabus",
    "section": "Counseling and Wellness Center",
    "text": "Counseling and Wellness Center\nContact information for the Counseling and Wellness Center:\n\ncounseling.ufl.edu\nPhone: 352.392.1575\n\nContact information for the University Police Department:\n\npolice.ufl.edu\nPhone: 352.392.1111",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#accommodations-for-students-with-disabilities",
    "href": "index.html#accommodations-for-students-with-disabilities",
    "title": "Syllabus",
    "section": "Accommodations for Students with Disabilities",
    "text": "Accommodations for Students with Disabilities\nStudents with disabilities requesting accommodations should first register with the Disability Resource Center (352.392.8565, disability.ufl.edu) by providing appropriate documentation. Once registered, students will receive an accommodation letter which must be presented to the instructor when requesting accommodation. Students with disabilities should follow this procedure as early as possible in the semester.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#academic-polices-and-resources",
    "href": "index.html#academic-polices-and-resources",
    "title": "Syllabus",
    "section": "Academic polices and resources",
    "text": "Academic polices and resources\nAvailable here:\n\nsyllabus.ufl.edu/syllabus-policy/uf-syllabus-policy-links/",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#calendar",
    "href": "index.html#calendar",
    "title": "Syllabus",
    "section": "Calendar",
    "text": "Calendar\n\n\n\nDate\nTopic\nReading\nAssignment\n\n\n\n\nFri., Aug. 22\nIntroduction\nFrankfurt (1986); Hicks et al. (2024)\n\n\n\nMon., Aug. 25\nConvention\nLewis (1975) (pp. 3–10)\n\n\n\nWed., Aug. 27\nConvention\nGrice (1957)\nView\n\n\nFri., Aug. 29\nSpeech acts\nBach (2006b) (pp. 147–155)\n\n\n\nWed., Sept. 3\nSpeech acts\n\n\n\n\nFri., Sept. 5\nSpeaker meaning\nGrice (1975)\n\n\n\nMon., Sept. 8\nSpeaker meaning\n\n\n\n\nWed., Sept 10\nDiagnosing inference\nWinter (2016) (Ch. 2, pp. 12–16)\n\n\n\nFri., Sept. 12\nDiagnosing inference\nCoppock and Champollion (2025) (Ch. 1, pp. 13–41; Ch. 8, pp. 315–322)\n\n\n\nMon., Sept. 15\nSets and set theory\n\nView\n\n\nWed., Sept. 17\nFunctions, relations, and languages\n\n\n\n\nFri., Sept. 19\nModels\nWinter (2016) (Ch. 2, pp. 17–27)\n\n\n\nMon., Sept. 22\nArabic numeral arithmetic\n\nView\n\n\nWed., Sept. 24\nArabic numeral arithmetic\n\n\n\n\nFri., Sept. 26\nIntransitive verbs\nWinter (2016) (Ch. 3, pp. 44–64)\n\n\n\nMon., Sept. 29\nIntransitive verbs\nWinter (2016) (Ch. 3, pp. 64-72)\n\n\n\nWed., Oct. 1\nWhat should go into a theory of meaning?\nBach (2006a); Szabó (2020) (§§3–4)\n\n\n\nFri., Oct. 3\nLambda notation and types\n\n\n\n\nMon., Oct. 6\nLambda notation and types\n\nView\n\n\nWed., Oct. 8\nTransitive verbs\n\n\n\n\nFri., Oct. 10\nTransitive verbs\n\n\n\n\nMon., Oct. 13\nApplicative categorial grammar\nJurafsky and Martin (2025) (Appendix E)\n\n\n\nWed., Oct. 15\nApplicative categorial grammar\n\n\n\n\nMon., Oct. 20\nGrammars and models\n\nView\n\n\nWed., Oct. 22\nVerbs and arguments redux\n\n\n\n\nFri., Oct. 24\nVerbs and arguments redux\n\n\n\n\nMon., Oct. 27\nCoordinators\nWinter (2016) (Ch. 3, pp. 74–80)\n\n\n\nWed., Oct. 29\nCoordinators\n\nView\n\n\nFri., Oct. 31\nAdjectives\nKennedy (2012)\n\n\n\nMon., Nov. 3\nAdjectives\n\n\n\n\nWed., Nov. 5\nAdjectives\n\nView\n\n\nFri., Nov. 7\nPronouns\n\n\n\n\nMon., Nov. 10\nPronouns\n\n\n\n\nWed., Nov. 12\nPronouns\n\nTBD\n\n\nFri., Nov. 14\nQuantifiers\nWinter (2016) (Ch. 4, pp. 99–108)\n\n\n\nMon., Nov. 17\nQuantifiers\n\n\n\n\nWed., Nov. 19\nQuantifiers\n\nTBD\n\n\nFri., Nov. 21\nNegative polarity items\n\n\n\n\nMon., Dec. 1\nQuantifiers and binding\n\n\n\n\nWed., Dec. 3\nQuantifiers and binding",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nListed as both LIN4803 and LIN6804 at one.uf.edu.↩︎",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "formal_preliminaries/sets.html",
    "href": "formal_preliminaries/sets.html",
    "title": "Sets",
    "section": "",
    "text": "What’s a set?\nA set is, intuitively speaking, a collection of objects, which we call the elements or members of the set. Any talk about sets is understood in the context of some universe of discourse. All sets inhabit this universe, as do any of the objects which we may consider to be members of a set. We may, for example, want to have things like words or natural numbers inhabit this universe. When analyzing the semantics of English, we may include other things too, like entities or individuals.\nFor any set \\(A\\) and any object \\(a\\) in the universe, either one of the following two things is true. \\[\\begin{align*}\na ∈ A \\tag{$a$ is a member of $A$}\\\\\na ∉ A \\tag{$a$ is not a member of $A$}\n\\end{align*}\\] Moreover, sets are defined by their members. That means that, for every \\(a\\) in the universe, if \\(a ∈ A\\) if and only if \\(a ∈ B\\), then \\(A\\) and \\(B\\) are the same set. This is the principle of extensionality for sets.\nWe will generally help ourselves to two kinds of notation for specifying sets: list notation and predicate (or “set-comprehension”, or “set-builder”) notation. In list notation, we define a set by enumerating its elements between curly braces. For example, \\[\\{1, 2, 3, George Washington\\}\\] is just the set \\(A\\) such that \\(1 ∈ A\\), \\(2 ∈ A\\), \\(3 ∈ A\\), and nothing else is \\(∈ A\\). Because of extensionality, \\(\\{1, 2, 3, GW\\}\\) is the same set as \\(\\{1, 1, 2, 3, GW\\}\\) is the same set as \\(\\{3, GW, 2, 2, 1\\}\\), etc. Using predicate notation, we can specify the very same set as \\[\\{x ∣ x = 1 \\text{ or } x = 2 \\text{ or } x = 3 \\text{ or } x = GW\\}\\] In general, in writing ‘\\(\\{a\\) \\(|\\) \\(φ\\}\\)’, \\(a\\) will be an expression with some number of variables occurring in it (things like \\(x\\), \\(y\\), \\(z\\), and so on), and \\(φ\\) will be a sentence expressing a condition on the values of these variables. Any number of the variables in \\(a\\), and generally only these, can occur in \\(φ\\). In the example above, \\(a\\) just was the variable \\(x\\), and the condition expressed was that the value of \\(x\\) be either 1, 2, 3, or George.\nBut we could also use predicate notation, as in \\[\\{x + 1 ∣ x = 1 \\text{ or } x = 2 \\text{ or } x = 3\\}\\] where \\(a\\) is the complex expression ‘\\(x +\n1\\)’, to specify the set \\(\\{2, 3, 4\\}\\).\nImportantly, there is an empty set, which we’ll write ‘\\(\\varnothing\\)’. It has no members. As a result, \\(a ∉ \\varnothing\\) for any \\(a\\) in the universe. The empty set can also be specified in list notation as `\\(\\{\\}\\)’.\n\n\nSet-forming operations\nGiven any two sets \\(A\\) and \\(B\\), we can use the following set-forming operations. \\[\\begin{align*}\n&a ∈ A \\cup B &\\textit{if $a ∈ A$ or $a ∈ B$}\\tag{union}\\\\\n&a ∈ A \\cap B &\\textit{if $a ∈ A$ and $a ∈ B$}\\tag{intersection}\\\\\n&a ∈ A - B &\\textit{if $a ∈ A$ and $a ∉ B$}\\tag{set difference}\n\\end{align*}\\] In addition, we say one set \\(A\\) is a subset of another set \\(B\\) \\[A ⊆ B\\] just in case every \\(a ∈ A\\) is also \\(∈ B\\). If, in addition, there is some \\(b ∈ B\\) such that \\(b ∉ A\\), we may write \\[A ⊂ B\\] (\\(A\\) is a proper subset of \\(B\\)) to indicate this. Note that because of extensionality, \\(A = B\\) just in case both \\(A ⊆ B\\) and \\(B ⊆ A\\).\nWe may sometimes refer to sets using notation for generalized intersection and union. If \\(S\\) is a set all of whose members are also sets, we write \\[⋃ S\\] to mean \\(\\{x\\) \\(|\\) \\(x ∈ A\\) for some \\(A ∈ S\\}\\); that is, we squash \\(S\\) into the union of all of its members.\nFor example, let’s define \\(ℕ = \\{0, 1, 2, ...\\}\\) (i.e., the set of “natural numbers”). Then, we can define the set \\(\\{\\{x\\} ∣ x ∈ ℕ\\}\\)—this is just the set of sets with only one member, and whose single member is one of the natural numbers; i.e., \\(\\{\\{0\\}, \\{1\\}, \\{3\\}, ...\\}\\). Now, we can define the set \\(⋃\\{\\{x\\} ∣ x ∈ ℕ\\}\\)—this set is just \\(ℕ\\)!\nLikewise, we write \\[⋂ S\\] to mean \\(\\{x\\) \\(|\\) \\(x ∈ A\\) for every \\(A ∈ S\\}\\); that is, we squash \\(S\\) into the intersection of all of its members. If we did this for the set defined above, i.e., \\(⋃\\{\\{x\\} ∣ x ∈ ℕ\\}\\), we’d get back the empty set (\\(\\varnothing\\))—none of the sets in the original set has any members in common!\n\n\nPowerset\nGiven a set \\(A\\), we may take its powerset, which is often written ‘\\(2^A\\)’ The powerset of \\(A\\) is defined as \\[2^A = \\{B ∣ B ⊆ A\\}\\] That is, it is the set of subsets of \\(A\\).\nThe power set of \\(ℕ\\), for example is \\(\\{\\{1\\}, \\{7, 223423, 2\\}, ... \\text{all the ways of putting natural numbers into sets}\\}\\). Indeed, the set \\(\\{\\{x\\} ∣ x ∈ ℕ\\}\\) that we talked about above is a subset of the powerset of \\(ℕ\\). (Why?)",
    "crumbs": [
      "Formal preliminaries",
      "Sets"
    ]
  },
  {
    "objectID": "formal_preliminaries/products.html",
    "href": "formal_preliminaries/products.html",
    "title": "Products",
    "section": "",
    "text": "Given two sets \\(A\\) and \\(B\\), we take their binary Cartesian product \\[(A × B)\\] to be the set of pairs of elements \\(⟨a, b⟩\\), where \\(a ∈ A\\) and \\(b ∈ B\\). We call \\(a\\) the first component (or projection) of such a pair and \\(b\\) the second component. Given a pair \\(p\\), we will sometimes write ‘\\(p_{1}\\)’ to refer to its first component and ‘\\(p_{2}\\)’ to refer to its second component. In particular: \\[⟨x, y⟩_{1} = x\\] \\[⟨x, y⟩_{2} = y\\]\nThe following statement is also true: \\[⟨x_{1}, x_{2}⟩ = x\\] That is, if you take the first and second components of a pair and pair them back up, you haven’t done anything!\nWe can generalize binary products to \\(n\\)-ary products by considering the former to correspond to the special case where \\(n = 2\\). For the general case \\[A₁ × ... × A_{n-1} × Aₙ\\] we can encode this product as \\[(...(A₁ × ... × A_{n-1}) × Aₙ)\\] For example, \\(A × B × C\\) is just \\(((A × B) × C)\\); its members are, therefore, pairs \\(⟨⟨a, b⟩, c⟩\\), which is just how we can encode 3-tuples \\(⟨a, b, c⟩\\).",
    "crumbs": [
      "Formal preliminaries",
      "Products"
    ]
  },
  {
    "objectID": "formal_preliminaries/relations.html",
    "href": "formal_preliminaries/relations.html",
    "title": "Relations",
    "section": "",
    "text": "If we have a collection of sets \\(A_{1}\\) through \\(A_{n}\\), then we can take an \\(n\\)-ary relation on those sets. An \\(n\\)-ary relation on the sets \\(A_{1}, ..., A_{n}\\) is a set \\(R\\) such that \\(R ⊆ A_{1} × ... × A_{n}\\). In the special case of a binary relation on two sets \\(A\\) and \\(B\\), \\(R\\) is a set of pairs \\(⟨a, b⟩\\), such that \\(a ∈ A\\) and \\(b ∈ B\\). That is, \\(R ⊆ A × B\\). For \\(n\\)-ary relations \\(R\\), if the \\(n\\)-tuple \\(⟨a₁, ..., aₙ⟩\\) is a member of \\(R\\), we can indicate this by writing \\[R(a₁, ..., aₙ)\\]\n\n\n\n\n\n\n\n\n\nAn important operation on relations is relation composition. Given a relation \\(R_{1} ⊂ A × B\\) and a relation \\(R_{2} ⊆ B × C\\), their composition \\(R_{2} ∘ R_{1} ⊆ A × C\\) is defined as \\[R_{2} ∘ R_{1} = \\{⟨x, y⟩ ∣ \\text{there is some $z$ such that } ⟨x, z⟩ ∈ R_{1} \\text{ and } ⟨z, y⟩ ∈ R_{2}\\}\\] That is, \\(R_{2} ∘ R_{1}\\) is gotten by relating an element of \\(A\\) to an element of \\(C\\) just in case there is some element of \\(B\\) related to the first element by \\(R_{1}\\) and to the second element by \\(R_{2}\\).",
    "crumbs": [
      "Formal preliminaries",
      "Relations"
    ]
  },
  {
    "objectID": "speaker_meaning/applying_maxims.html",
    "href": "speaker_meaning/applying_maxims.html",
    "title": "Applying the maxims",
    "section": "",
    "text": "Grice (1975) gives several examples of inferences that appear to result from the maxims, given particular utterances. These inferences are called conversational implicatures. Here, I’ll give a few of my own.",
    "crumbs": [
      "Speaker meaning",
      "Applying the maxims"
    ]
  },
  {
    "objectID": "speaker_meaning/applying_maxims.html#footnotes",
    "href": "speaker_meaning/applying_maxims.html#footnotes",
    "title": "Applying the maxims",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n When I say that Jo did all of the readings is stronger than Jo did some of the readings, what I mean is that the first sentence entails the second sentence, but the second does not entail the first. Viewing things from the other direction, one can say that Jo did some of the readings is weaker than Jo did all of the readings. ↩︎",
    "crumbs": [
      "Speaker meaning",
      "Applying the maxims"
    ]
  },
  {
    "objectID": "speaker_meaning/maxims.html",
    "href": "speaker_meaning/maxims.html",
    "title": "The Gricean maxims",
    "section": "",
    "text": "Since our general goal is to give an account of conventional meaning—the type of meaning that users of a language associate with expressions by convention, in a way that allows them to deploy those expressions to communicate in real conversational settings—it would be useful to first get a handle on some general conversational principles about how expressions with certain meanings are deployed. Having observed inferences in real-life language use, we can hopefully then work out which of those inferences are related to conventional meaning and which are related to general (and perhaps, to some extent, non-linguistic) social conventions about conversation itself.\nWith this aim partly in mind, Grice (1975) proposes a principle that he takes to regulate real-life conversation: the Cooperative Principle.\n\nThe Cooperative Principle:  “Make your conversational contribution, such as is required, at the stage at which it occurs, by the accepted purpose or direction of the talk exchange in which you are engaged” (p. 45).\n\n(Will do 😅!) More seriously, Grice thinks that this principle should be seen as implicitly regulating what people decide to say to one another, and he tries to give the principle a little bit of heft by saying what maxims people appear to follow in order to adhere to it.\nHe proposes four general maxims:\n\nQuality\nQuantity\nRelation\nManner\n\n\nQuality\nThe maxim of Quality has two parts: first, don’t say things that you think are false; and second, only say things that you have enough evidence for.\nGrice doesn’t say much about what, in general, constitutes “enough evidence”—presumably this is meant to be understood as somewhat context-dependent. For example, if you’re a surgeon in an operating room, the standard of evidence for certain kinds of assertions would likely be pretty high (“The tibial plateau is misaligned by three degrees.”), whereas if you’re a contestant on Jeopardy, it’s generally likely to be lower. Indeed (as is the case for all of Grice’s maxims), there are many details to fill in here! But we can still get a certain amount of reasoning off the ground about why certain utterances mean what they do, even with while leaving a lot underspecified.\n\n\nQuantity\nThe maxim of Quantity is also stated by Grice as having two parts: first, be as informative as necessary; second, don’t overdo it!\nAgain, Grice leaves out crucial information about how this maxim ought to be evaluated—what counts as “as informative as necessary” or “overdo[ing] it”? As we saw, it’s possible to give a more systematic account of what counts as too little or too much information if we have access to the notion of a question under discussion (QUD; Roberts (2012)). For example, if the question is how many children does John have?, an answer which is under-informative might be John has between three and five children, while an answer that might be over-informative might be John has exactly three children, and they all go to the same high school. Intriguingly, it is perhaps also possible for an answer to both under- and over-informative, e.g., John has between three and five children, and they all go to the same high school. The answer which is “just right”, given the question, might then be John has exactly three children.\nImportant to note is that this addition really only works if we have two things. First, we need a way of knowing when some question is the QUD of a particular conversation or discourse; and second, we need a definition of when a given statement answers a question, either partially or completely. The problem of identifying QUDs is a matter of ongoing research in both computational and experimental pragmatics—it’s hard (though language users might generally seem to solve it fairly effortlessly).\nMeanwhile, the problem of defining when a statement answers a question is perhaps somewhat easier, insofar as there are several theories of question meanings on the market, and any of them might in principle tell us what the possible complete and partial answers to a QUD are, once one has been identified.\n\n\nRelation\nThe maxim of Relation is stated as follows: be relevant.\nThat is, don’t be like zombie kid. This maxim can also be made a little more systematic by taking it to constraint the relationship between statements made in some conversation and the QUD of the conversation: it seems to say that one should make statements that answer the QUD, or at least partially answer it.\n\n\nManner\nThe maxim of Manner is taken by Grice to dictate that one should be clear, unambiguous, brief, and orderly.\nWhether or not a particular utterance adheres to this maxim might seem to be somewhat in the eye of the beholder. For example, what counts as “brief”—should we count the number of segments, syllables, words, constituent phrases used (does it depend?)? Ideally, such criteria would be open to empirical investigation.\n\n\n\n\n\n\n\nReferences\n\nGrice, H. P. 1975. “Logic and Conversation.” In Syntax and Semantics, edited by Peter Cole and Jerry L. Morgan, 3, Speech Acts:41–58. New York: Academic Press.\n\n\nRoberts, Craige. 2012. “Information Structure: Towards an Integrated Formal Theory of Pragmatics.” Semantics and Pragmatics 5 (December): 6:1–69. https://doi.org/10.3765/sp.5.6.",
    "crumbs": [
      "Speaker meaning",
      "The Gricean maxims"
    ]
  },
  {
    "objectID": "formal_preliminaries/assignment.html",
    "href": "formal_preliminaries/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "Due on Monday, September 29th.\n\nPart 1\nTaken from Partee, Meulen, and Wall (1990).\nConsider the following sets:\n\\[S_{1} = \\{\\{\\varnothing\\}, \\{A\\}, A\\}\\] \\[S_{2} = A\\] \\[S_{3} = \\{A\\}\\] \\[S_{4} = \\{\\{A\\}\\}\\] \\[S_{5} = \\{\\{A\\}, A\\}\\] \\[S_{6} = \\varnothing\\] \\[S_{7} = \\{\\varnothing\\}\\] \\[S_{8} = \\{\\{\\varnothing\\}\\}\\] \\[S_{9} = \\{\\varnothing, \\{\\varnothing\\}\\}\\]\n\nOf the sets \\(S_{1}\\) through \\(S_{9}\\), which are members of \\(S_{1}\\)?\nWhich are subsets of \\(S_{1}\\)?\nWhich are members of \\(S_{9}\\)?\nWhich are subsets of \\(S_{9}\\)?\nWhich are members of \\(S_{4}\\)?\nWhich are subsets of \\(S_{4}\\)?\n\n\n\nPart 2\nShow that \\(A ∩ B ⊆ A\\) for any arbitrary sets \\(A\\) and \\(B\\).\n\n\nPart 3\nShow that if \\(A ⊆ B\\), then \\(A = A ∩ B\\) for any arbitrary sets \\(A\\) and \\(B\\).\n\n\n\n\n\n\n\nReferences\n\nPartee, Barbara H., Alice ter Meulen, and Robert E. Wall. 1990. Mathematical Methods in Linguistics. Vol. 30. Studies in Linguistics and Philosophy. Dordrecht: Kluwer Academic Publishers.",
    "crumbs": [
      "Formal preliminaries",
      "Assignment"
    ]
  },
  {
    "objectID": "formal_preliminaries/languages.html",
    "href": "formal_preliminaries/languages.html",
    "title": "Languages",
    "section": "",
    "text": "We write `\\(A^n\\)’ to refer to the result of taking the product of the set \\(A\\) with itself \\(n\\) times. As a matter of convention, we will adopt the following equivalences. \\[\\begin{align*}\nA^0 &=_{def} \\{\\varnothing\\}\\\\\nA^1 &=_{def} A\\\\\nA^n &=_{def} A^{n-1} × A\n\\end{align*}\\] To define a language, we first define a set \\(Σ\\), which we call the alphabet, words, or lexicon of the language. These can be anything, in principle (phonetic forms, sets of syntactic features, blades of grass, etc.). Then, a language over the alphabet \\(Σ\\) is some set \\(L\\) such that \\[L ⊆ ⋃ \\{Σ^{i} ∣ i ∈ ℕ\\}\\] Thus \\(L\\) is a set of \\(n\\)-tuples of words from \\(Σ\\) of any length \\(n\\). When we talk about languages, we will refer to tuples as strings and adopt the convention of writing, e.g., the string ⟨ the, dog, is, friendly ⟩ simply as the dog is friendly (or as “the dog is friendly” when I’m writing on the chalkboard). Note that \\(L\\) may include \\(\\varnothing\\), which we will regard as the empty string (the string of length 0), which we write ‘\\(ϵ\\)’ in this context.",
    "crumbs": [
      "Formal preliminaries",
      "Languages"
    ]
  },
  {
    "objectID": "formal_preliminaries/functions.html",
    "href": "formal_preliminaries/functions.html",
    "title": "Functions",
    "section": "",
    "text": "A function from a set \\(A\\) to a set \\(B\\) is a map from elements of \\(A\\) to elements of \\(B\\) which pairs each element of \\(A\\) with exactly one element of \\(B\\). If \\(f\\) is such a function, we write \\[f : A → B\\] to indicate this. We call \\(A\\) the domain of the function, and we call \\(B\\) the codomain of the function. If \\(a ∈ A\\), then we write \\[f(a)\\] to pick out the unique \\(b ∈ B\\) that \\(f\\) pairs \\(a\\) up with. In case \\(f(a) = b\\) (for some \\(a\\) and \\(b\\)), we call \\(a\\) the argument of the function \\(f\\), and we call \\(b\\) the value of the function \\(f\\) on \\(a\\).\nKind of like with relations, the definition of (unary) functions can be generalized to that for n-ary functions by considering the latter to be a map from the product of \\(n\\) sets \\(A₁, ..., Aₙ\\) to a set \\(B\\). If \\(f\\) is such an \\(n\\)-ary function, we write \\[f : A_1 × ... × A_n → B\\] to indicate this. Given \\(n\\) arguments \\(a₁ ∈ A₁, ..., aₙ ∈ Aₙ\\), we write \\[f(a_1, ..., a_n)\\] to pick out out the \\(b ∈ B\\) that \\(f\\) maps \\(a₁, ..., aₙ\\) to.\n\n\n\n\n\n\n\n\n\n\n\n\nImportantly, there is a corresponding notion of function composition. We need only consider functions from \\(A\\) to \\(B\\) a type of relation on \\(A\\) and \\(B\\)—one that relations every element in \\(A\\) to exactly one element in \\(B\\). Given functions \\(f : A → B\\) and \\(g : B → C\\), their composition \\(g ∘ f : A → C\\) is such that \\((g ∘ f)(x) = g(f(x))\\), for any \\(x ∈ A\\).",
    "crumbs": [
      "Formal preliminaries",
      "Functions"
    ]
  },
  {
    "objectID": "formal_preliminaries/examples.html",
    "href": "formal_preliminaries/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of proofs of some basic facts about sets.\n\nFirst example\nShow that \\(A ⊆ A ∪ B\\).\nThe definition of subset says that this holds if everything in \\(A\\) is also in \\(A ∪ B\\). The definition of union says that any \\(x ∈ A ∪ B\\) if \\(x ∈ A\\) or \\(x ∈ B\\). Thus any \\(x ∈ A\\) is such that \\(x ∈ A ∪ B\\), as needed.\n\n\nSecond example\nLet \\(A, B ⊆ C\\), and let’s introduce the notation ‘\\(¬X\\)’ for \\(C - X\\).\nShow that \\(¬(A ∪ B) ⊆ ¬A ∩ ¬B\\).\nAssume \\(x ∈ ¬(A ∪ B)\\) (for arbitrary \\(x\\)); then, our goal is show that \\(x\\) must be in \\(¬A ∩ ¬B\\). By definition, \\(x ∈ C\\), but \\(x ∉ A ∪ B\\). Hence, \\(x ∉ A\\) (or else, we would have that \\(x ∈ A ∪ B\\)). But because, by hypothesis, \\(x ∈ C\\), we now have that \\(x ∈ ¬A\\). A similar argument applies to \\(¬B\\)! Thus \\(x ∈ ¬A ∩ ¬B\\). Since \\(x\\) is arbitrary, the argument extends to any element of \\(¬(A ∪ B)\\), as needed to show that the two sets are in a subset relation.",
    "crumbs": [
      "Formal preliminaries",
      "Examples"
    ]
  },
  {
    "objectID": "coordinators/assignment.html",
    "href": "coordinators/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\true{\\ct{T}}\n\\def\\false{\\ct{F}}\n\\]\n\nDue on Wednesday, November 12th.\n\nPart 1\nProvide a lexical entry for or that accounts for its occurrence in the sentence in (1), and give a derivation for (1).\n\nTigress or Po jumped.\n\nMake sure that you unfold any relevant definitions gotten from the scheme discussed in A general rule, and that you end up with a representation of a truth value as the meaning of (1).\n\n\nPart 2\nIn addition to subject noun phrases, we can also coordinate object noun phrases, as in (2).\n\nMantis punched Tigress and Po.\n\nProvide lexical entries for Tigress and Po that account for this fact, as well as the fact that (2) entails (3), and vice versa.\n\nMantis punched Tigress and Mantis punched Po.\n\nThe basic idea here is that you want an object noun phrase to take a transitive verb and give something back.\nYou don’t need to provide a derivation of (2) for this exercise, but it might help to!\n\n\nPart 3\nThe sentence in (4) is ambiguous; that is, it has two possible interpretations, or readings.\n\nTigress and Po jumped or slept.\n\nOn one reading, both Tigress and Po have to have done the same thing—that is, they both jumped or they both slept. To get at this reading, you might follow up (4) by saying, “Tigress and Po jumped or slept—but I don’t know which one they both did.” Call this reading of the sentence in (4) Reading A.\nThere is another reading of (4), however, which is weaker than Reading A. On this other reading, (4) can be true if Tigress and Po both did one of the two activities of jumping and sleeping, but not necessarily the same one. For example, maybe Tigress only jumped, while Po only slept. It seems like (4) can still be true in this scenario—it just means something different. Call this Reading B.\nDo the assumptions we have made about the meanings of and, or, and subject noun phrases allow us to account for the existence of both Reading A and Reading B? Or can we only explain one or the other? One way to figure out an answer to this question is by providing a derivation for (4).",
    "crumbs": [
      "Coordinators",
      "Assignment"
    ]
  },
  {
    "objectID": "coordinators/coordinating_nps.html",
    "href": "coordinators/coordinating_nps.html",
    "title": "Coordinating subject noun phrases",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\true{\\ct{T}}\n\\def\\false{\\ct{F}}\n\\]\n\nTo coordinate subject noun phrases like Po and Tigress in a sentence such as (1), we had to make a novel move.\n\nPo and Tigress slept.\n\nInstead of the usual category \\(np\\), we assigned these noun phrases the category \\((s/(np\\backslash s))\\). This means that they take a verb phrase to their right—something whose syntactic category is \\((np\\backslash s)\\)—and together with that verb phrase, they make a sentence (an \\(s\\)).\nThis is a strange reversal of roles: typically, it is the verb phrase that takes the subject as its argument. The verb phrase’s syntactic category even says that that’s what it does—it takes an \\(np\\) to its left, and with that \\(np\\), it makes a sentence. But alas, now there is no longer an \\(np\\) to the left of the verb phrase. What’s there instead is a souped up noun phrase—a taker of verb phrases, if you will. Someone saying, “Hey verb phrase, remember how you were going to take me as your syntactic argument on your left and make a sentence? Well, now it is I who takes you as my argument on my right (don’t worry, I’ll still use you to make a sentence)! (Evil laugh.)”\nImportantly, this is perfectly within the bounds of what we are allowed to do using our category system. If some expression has category \\(X\\), then we can also define an expression that takes an \\(X\\) to its right to make a sentence by assigning it the category \\((s/X)\\). We can still do this even if \\(X\\) happens to be the category of a verb phrase, i.e., \\((np\\backslash s)\\).\nThe whole reason we want to do this, of course, is in order to allow noun phrases to have a type that “ends in \\(t\\)”. Such types—which also include \\((e → t)\\) and \\((e → (e → t))\\)—allow noun phrases to be coordinated using the scheme in A general rule.\nIn particular, if noun phrases have the syntactic category \\((s/(np\\backslash s))\\), then they have the semantic type \\(((e → t) → t)\\). They denote functions from characteristic functions of sets of entities to truth values. Put differently, they are characteristic functions of sets of characteristic functions (of sets of entities). (See this discussion again for some review.)\nIf we forced noun phrases to have the syntactic category \\(np\\), we would also be limiting them to having the semantic type \\(e\\) of entities—things that are neither truth values, nor functions into truth values, nor functions into functions into truth values, etc. The scheme we developed to define the possible meanings of and and or would be unusable.\n\nLexical entries for subject noun phrases\nWe can assign Po and Tigress the lexical entries in (2)\n\n\n\\(⟨\\textit{Po}, (λf.f(\\ct{p}))⟩ ⊢ (s/(np\\backslash s))\\)\n\\(⟨\\textit{Tigress}, (λf.f(\\ct{ti}))⟩ ⊢ (s/(np\\backslash s))\\)\n\n\nEach of the meanings in these lexical entries takes a verb phrase meaning—a characteristic function—and obtains a truth value by applying that function to the entity the noun phrase usually denotes (i.e., \\(\\ct{p}\\) or \\(\\ct{ti}\\)). The idea is that the resulting sentence will denote \\(\\true\\) just in case the verb phrase characterizes a set that has that entity as one of its members.\nHere’s a derivation of Po slept to illustrate:\n\\[\\begin{prooftree}\n\\AxiomC{\\(⟨\\textit{Po}, (λf.f(\\ct{p}))⟩ ⊢ (s/(np\\backslash s))\\)}\n\\AxiomC{\\(⟨\\textit{slept}, (λx.\\ct{sleep}(x))⟩ ⊢ (np\\backslash s)\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(\\textit{Po slept}, (λf.f(\\ct{p}))(λx.\\ct{sleep}(x))⟩ ⊢ s\\)}\n\\end{prooftree}\\]\nThe resulting meaning representation can then be evaluated:\n\\[(λf.f(\\ct{p}))(λx.\\ct{sleep}(x))\\] \\[⇒ (λx.\\ct{sleep}(x))(\\ct{p})\\] \\[⇒ \\ct{sleep}(\\ct{p})\\]\n\n\nThe coordinator\nThe meaning for and that we want in order to coordinate noun phrases like that in (1) is given by the lexical entry in (3).\n\n\\(⟨\\textit{and}_{(s/(np\\backslash s))}, \\ct{and}_{((e → t) → t)}⟩ ⊢ (((s/(np\\backslash s))\\backslash(s/(np\\backslash s)))/(s/(np\\backslash s)))\\)\n\nWhat is \\(\\ct{and}_{((e → t) → t)}\\)? If we follow the scheme given in A general rule, we can see that it is defined as:\n\\[\\ct{and}_{((e → t) → t)} = (λf.(λg.(λx.\\ct{and}_{t}(f(x))(g(x)))))\\]\n\\[= (λf.(λg.(λx.f(x) = g(x) = \\true)))\\]\nHere, the \\(f\\) and \\(g\\) represent noun phrase meanings (functions from characteristic functions to truth values), and the \\(x\\) represents a verb phrase meaning (a characteristic function).\n\n\nAn example\nHere is an example derivation for the sentence in (1):\n\\[\\begin{prooftree}\n\\AxiomC{\\(⟨\\textit{Po}, (λf.f(\\ct{p}))⟩ ⊢ (s/(np\\backslash s))\\)}\n\\AxiomC{\\(⟨\\textit{and}_{(s/(np\\backslash s))}, \\ct{and}_{((e → t) → t)}⟩ ⊢ (((s/(np\\backslash s))\\backslash(s/(np\\backslash s)))/(s/(np\\backslash s)))\\)}\n\\AxiomC{\\(⟨\\textit{Tigress}, (λf.f(\\ct{ti}))⟩ ⊢ (s/(np\\backslash s))\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(⟨\\textit{and\\(_{(s/(np\\backslash s))}\\) Tigress}, \\ct{and}_{((e → t) → t)}((λf.f(\\ct{ti})))⟩ ⊢ ((s/(np\\backslash s))\\backslash(s/(np\\backslash s)))\\)}\n\\RightLabel{\\(\\backslash\\)}\\BinaryInfC{\\(⟨\\textit{Po and\\(_{(s/(np\\backslash s))}\\) Tigress}, \\ct{and}_{((e → t) → t)}((λf.f(\\ct{ti})))((λf.f(\\ct{p})))⟩ ⊢ (s/(np\\backslash s))\\)}\n\\AxiomC{\\(⟨\\textit{slept}, (λx.\\ct{sleep}(x))⟩ ⊢ (np\\backslash s)\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(⟨\\textit{Po and\\(_{(s/(np\\backslash s))}\\) Tigress slept}, \\ct{and}_{((e → t) → t)}((λf.f(\\ct{ti})))((λf.f(\\ct{p})))((λx.\\ct{sleep}(x)))⟩ ⊢ s\\)}\n\\end{prooftree}\\]\nLet’s evaluate the result by first unfolding the definition of \\(\\ct{and}_{((e → t) → t)}\\)\n\\[\\ct{and}_{((e → t) → t)}((λf.f(\\ct{ti})))((λf.f(\\ct{p})))((λx.\\ct{sleep}(x)))\\] \\[= (λf.(λg.(λx.f(x) = g(x) = \\true)))((λf.f(\\ct{ti})))((λf.f(\\ct{p})))((λx.\\ct{sleep}(x)))\\]\nand then reducing the resulting expression, one step at a time:\n\\[(λf.(λg.(λx.f(x) = g(x) = \\true)))((λf.f(\\ct{ti})))((λf.f(\\ct{p})))((λx.\\ct{sleep}(x)))\\] \\[⇒ (λg.(λx.(λf.f(\\ct{ti}))(x) = g(x) = \\true))((λf.f(\\ct{p})))((λx.\\ct{sleep}(x)))\\] \\[⇒ (λx.(λf.f(\\ct{ti}))(x) = (λf.f(\\ct{p}))(x) = \\true)((λx.\\ct{sleep}(x)))\\] \\[⇒ (λf.f(\\ct{ti}))((λx.\\ct{sleep}(x))) = (λf.f(\\ct{p}))((λx.\\ct{sleep}(x))) = \\true\\] \\[⇒ (λx.\\ct{sleep}(x))(\\ct{ti}) = (λf.f(\\ct{p}))((λx.\\ct{sleep}(x))) = \\true\\] \\[⇒ \\ct{sleep}(\\ct{ti}) = (λf.f(\\ct{p}))((λx.\\ct{sleep}(x))) = \\true\\] \\[⇒ \\ct{sleep}(\\ct{ti}) = (λx.\\ct{sleep}(x))(\\ct{p}) = \\true\\] \\[⇒ \\ct{sleep}(\\ct{ti}) = \\ct{sleep}(\\ct{p}) = \\true\\]\nThe truth value we get at the end is \\(\\true\\) just in case \\(\\ct{sleep}\\) is true of both \\(\\ct{ti}\\) and \\(\\ct{p}\\)—that is, if both Tigress and Po slept.\n(I would really recommend reading each of these lines carefully and ensuring that you see which particular reduction has taken place!)",
    "crumbs": [
      "Coordinators",
      "Coordinating subject noun phrases"
    ]
  },
  {
    "objectID": "coordinators/general_rule.html",
    "href": "coordinators/general_rule.html",
    "title": "A general rule",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\true{\\ct{T}}\n\\def\\false{\\ct{F}}\n\\]\n\nTaking stock of these distinct examples of expressions that can be coordinated—sentences, verb phrases, transitive verbs—one might wonder if there is a rule that might allow us to derive the meaning of a coordinator in any specific case, perhaps, from some collection of general principles.\n\nSentence-level coordinators\nStarting with the observation—made in Overview—that each sentence that contains a coordination using and or or has some corresponding sentence that means the same thing, let’s just start by repeating the sentence-coordinating meanings. We can call these meanings ‘\\(\\ct{and}_{t}\\)’ and ‘\\(\\ct{or}_{t}\\)’, respectively, to represent the fact that they coordinate sentences, whose meanings are truth values.\n\\[\\ct{and}_{t} = (λa.(λb.a = b = \\true))\\] \\[\\ct{or}_{t} = (λa.(λb.¬(a = b = \\false)))\\]\nIndeed, the lexical entries for sentence-coordinating and and or can now be stated a little bit more compactly. That is, instead of writing\n\\[⟨\\textit{and}_{s}, (λa.(λb.a = b = \\true))⟩ ⊢ ((s\\backslash s)/s)\\] \\[⟨\\textit{or}_{s}, (λa.(λb.¬(a = b = \\false)))⟩ ⊢ ((s\\backslash s)/s)\\]\nwe may simply write\n\\[⟨\\textit{and}_{s}, \\ct{and}_{t}⟩ ⊢ ((s\\backslash s)/s)\\] \\[⟨\\textit{or}_{s}, \\ct{or}_{t}⟩ ⊢ ((s\\backslash s)/s)\\]\n\n\nAll other coordinators\nWhat about coordinators of verb phrases, transitive verbs, etc.? We should define meanings for these coordinators that depend only on the semantic types of the expressions that they take as their arguments. Thus we can define totally generic meanings, as follows:\n\\[\\ct{and}_{(X → Y)} = (λf.(λg.(λx.\\ct{and}_{Y}(f(x))(g(x)))))\\] \\[\\ct{or}_{(X → Y)} = (λf.(λg.(λx.\\ct{or}_{Y}(f(x))(g(x)))))\\]\n\nExample: verb phrases\nSo, now, what is the meaning of and when it coordinates two verb phrases? We can just give it the following lexical entry:\n\\[⟨\\textit{and}_{(np\\backslash s)}, \\ct{and}_{(e → t)}⟩ ⊢ (((np\\backslash s)\\backslash(np\\backslash s))/(np\\backslash s))\\]\nWhy is its meaning \\(\\ct{and}_{(e → t)}\\)? Well, because it coordinates verb phrases, and \\((e → t)\\) is the semantic type of verb phrases!\nWhat is \\(\\ct{and}_{(e → t)}\\)? To figure this out, we just have to apply the rule given above. Specifically:\n\\[\\ct{and}_{(e → t)} = (λf.(λg.(λx.\\ct{and}_{t}(f(x))(g(x)))))\\]\nAnd now, if we substitute in the definition of \\(\\ct{and}_{t}\\), we get:\n\\[\\ct{and}_{(e → t)} = (λf.(λg.(λx.(λa.(λb.a = b = \\true))(f(x))(g(x)))))\\]\nThis looks complicated, but we can evaluate it. Specifically, we should get rid of the ‘\\(λa\\)’ and substitute \\(f(x)\\) in for \\(a\\):\n\\[\\ct{and}_{(e → t)} = (λf.(λg.(λx.(λb.f(x) = b = \\true)(g(x)))))\\]\nNow, we should get rid of the ‘\\(λb\\)’ and substitute \\(g(x)\\) in for \\(b\\):\n\\[\\ct{and}_{(e → t)} = (λf.(λg.(λx.f(x) = g(x) = \\true)))\\]\nFortunately, this is precisely the meaning of \\(\\textit{and}_{(np\\backslash s)}\\) provided in Verb phrase coordinators.\n\n\nExample: transitive verbs\nWhat about when it coordinates two transitive verbs? In this case, and should have the following lexical entry:\n\\[⟨\\textit{and}_{((np\\backslash s)/np)}, \\ct{and}_{(e → (e → t))}⟩ ⊢ ((((np\\backslash s)/np)\\backslash((np\\backslash s)/np))/((np\\backslash s)/np))\\]\nWhy \\(\\ct{and}_{(e → (e → t))}\\)? Well, because \\((e → (e → t))\\) is the semantic type of transitive verbs.\nAnd what is \\(\\ct{and}_{(e → (e → t))}\\). We can figure this one out, as well, by applying the rule above. The main difference is that this time, we apply the rule twice (once for each of the two \\(e\\)’s). Thus we get:\n\\[\\ct{and}_{(e → (e → t))} = (λf.(λg.(λx.\\ct{and}_{(e → t)}(f(x))(g(x)))))\\]\nThis meaning for and immediately uses the meaning we defined for verb-phrase-coordinator and—\\(\\ct{and}_{(e → t)}\\)—so let’s just that meaning in. Importantly, we should change up the names of our variables in the meaning we substitute, in order to help prevent them from getting mixed up with the other variables. So instead of writing in\n\\[(λf.(λg.(λx.f(x) = g(x) = \\true)))\\]\nas the definition of \\(\\ct{and}_{(e → t)}\\)\nwe can instead write the following:\n\\[(λf^{\\prime}.(λg^{\\prime}.(λy.f^{\\prime}(y) = g^{\\prime}(y) = \\true)))\\]\nImportantly, this is exactly the same function! We just changed the names of the variables.\nIf we substitute this in, we get:\n\\[\\ct{and}_{(e → (e → t))} = (λf.(λg.(λx.(λf^{\\prime}.(λg^{\\prime}.(λy.f^{\\prime}(y) = g^{\\prime}(y) = \\true)))(f(x))(g(x)))))\\]\nFollowing what we did above, we can simplify this definition a bit by performing some substitutions. First, let’s substitute \\(f(x)\\) in for \\(f^{\\prime}\\):\n\\[\\ct{and}_{(e → (e → t))} = (λf.(λg.(λx.(λg^{\\prime}.(λy.f(x)(y) = g^{\\prime}(y) = \\true))(g(x)))))\\]\nSecond, lets substitute \\(g(x)\\) in for \\(g^{\\prime}\\):\n\\[\\ct{and}_{(e → (e → t))} = (λf.(λg.(λx.(λy.f(x)(y) = g(x)(y) = \\true))))\\]\nWhat we are left with is exactly the meaning for \\(\\textit{and}_{((np\\backslash s)/np)}\\) that we posited in Transitive verb coordinators.",
    "crumbs": [
      "Coordinators",
      "A general rule"
    ]
  },
  {
    "objectID": "models/overview.html",
    "href": "models/overview.html",
    "title": "Overview",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\]\n\nOne of the central technical notions that we’ll rely on in this course is that of a model. A model is, in short, a way of associating the most basic expressions of whatever language we are studying with interpretations. It can be useful to think of a model as encoding a way that things could be, or—in the parlance of possible world semantics—a “possible world”. For example, if we are assigning meanings to a language that has the verb runs as one of its basic expressions, then we might associate this verb with a set—specifically, the set of people (or other entities) that run. In one model, Jo and Bo might be the ones who run (so that the set of people who run is \\(\\{\\ct{j}, \\ct{b}\\}\\)); in another, the interpretation of runs might be the set containing Bo and Mo (\\(\\{\\ct{b}, \\ct{m}\\}\\)); in yet another, maybe nobody runs (\\(\\varnothing\\)); and so on.\nTechnically, a model \\(\\mathcal{M}\\) is a structure that consists of a pair of a domain \\(D\\) and an interpretation function \\(I\\)—something which takes atoms (i.e., the most basic expressions, whatever those are) of the language onto elements of a set \\(M\\) (‘\\(M\\)’ for “meanings”):\n\nDefinition:  a model \\(\\mathcal{M}\\) is a tuple \\(⟨D, \\{\\True, \\False\\}, I⟩\\) of a non-empty set \\(D\\), the set of truth values \\(\\True\\) and \\(\\False\\), and a function \\(I : atom → M\\).\n\nIn (1), \\(atom\\) is just whatever the set of atoms is chosen to be.\nExactly how \\(M\\) is chosen will vary by case—but in general, it is something “constructed” out of \\(D\\), the domain. For example, it might be \\(D\\) itself; or it might be \\(D ∪ 2^{D}\\) (the set containing as members both all of the elements of \\(D\\) and all of the subsets of \\(D\\)); or it might be something else more complicated.\nFurther, the way the set \\(atom\\) is chosen will also vary by case. For example, if we are defining a language \\(L\\) over some alphaet \\(Σ\\), \\(atom\\) might be chosen to be \\(Σ\\) itself—or perhaps some subset of \\(Σ\\). The way we decide what \\(atom\\) is depends on how we decide we want to construct the interpretations of more complex expressions of the language out of the meanings of more basic expressions.",
    "crumbs": [
      "Models",
      "Overview"
    ]
  },
  {
    "objectID": "models/ana_syntax.html",
    "href": "models/ana_syntax.html",
    "title": "Arabic numeral arithmetic: syntax",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\]\n\nAs our first case study, we will look at the language of Arabic numeral arithmetic (‘\\(L_{ANA}\\)’, for short). This language has expressions like \\(⌜5⌝\\), \\(⌜(1 + 2)⌝\\), \\(⌜(37 = 43)⌝\\), \\(⌜(3 × 5) = 15⌝\\), and so on.\nNote that I’m using the corner brackets (\\(⌜\\) and \\(⌝\\)), here, in order to distinguish object language expressions (i.e., of Arabic numeral arithmetic) from actual numbers, like \\(1\\) and \\(2\\). The first are just symbols, currently meaningless, while the second are elements of the set \\(ℕ\\) of natural numbers. It’s important to keep these two distinct: while the former are things in the language—the set that we are trying to define and give an interpretation to—the latter will be elements of the domain of the model for the language.\n\\(L_{ANA}\\) has a few different kinds of expressions which are useful to distinguish. First, it has numerals. These are strings like \\(⌜1⌝\\), \\(⌜2⌝\\), \\(⌜57⌝\\), and so on. Second, it has complex expressions. These are strings containing the symbols for the addition and multiplication operators, like \\(⌜(1 + 2)⌝\\), \\(⌜((3 × 4) + 7)⌝\\), and so on. Finally, it has equations. These are strings containing the “equals” symbol, like \\(⌜37 = (5 × 6)⌝\\).\nThe syntax of \\(L_{ANA}\\) can be described by the rules in (2) (based on the alphabet in (1)). Note that we have four categories total: \\(atom\\), \\(n\\), \\(np\\), and \\(s\\).\nFirst, the alphabet:\n\n\\(Σ_{ANA} = \\{⌜0⌝, ..., ⌜9⌝, ⌜(⌝, ⌜)⌝, ⌜+⌝, ⌜×⌝\\}\\)\n\nNow, for the syntax:\n\nRule 1. \\(atom = \\{⌜0⌝, ..., ⌜9⌝\\}\\).  Rule 2. \\(atom ⊆ n\\).  Rule 3. If \\(x, y ∈ n\\), then \\(x^{⌢}y ∈ n\\).  Rule 4. \\(n ⊆ np\\).  Rule 5. If \\(x, y ∈ np\\), then \\(⌜(⌝^{⌢}x^{⌢}⌜+⌝^{⌢}y^{⌢}⌜)⌝, ⌜(⌝^{⌢}x^{⌢}⌜×⌝^{⌢}y^{⌢}⌜)⌝ ∈ np\\).  Rule 6. If \\(x, y ∈ np\\), then \\(x^{⌢}⌜{=}⌝^{⌢}y ∈ s\\).  Rule 7. Closure clause: nothing else is in \\(n\\), \\(np\\), or \\(s\\).\n\nIn words, these rules say (respectively):\n\nthat the atoms are \\(⌜1⌝\\) through \\(⌜9⌝\\);\nthat any atoms are also numerals;\nthat you can always take two numerals and concatenate them to make another numeral;\nthat any numerals are also noun phrases;\nthat you can always take two noun phrases and concatenate them around the addition or multiplication symbol (with parentheses) to make another noun phrase;\nthat you can always take two noun phrases and concatenate them around the equals symbol to make a sentence;\nand finally (the closure clause), that those things exhaust what you’re allowed to do—nothing else is in the language!\n\nWe can define the language of Arabic numeral arithmetic itself as follows:\n\n\\(L_{ANA} = s ∪ np\\):\n\nSo, it’s just the set of sentences unioned with the set of noun phrases; in plain English, something is in the language if and only if it is a sentence or a noun phrase.",
    "crumbs": [
      "Models",
      "Arabic numeral arithmetic: syntax"
    ]
  },
  {
    "objectID": "models/english_examples.html",
    "href": "models/english_examples.html",
    "title": "An English model and example",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\True{\\ct{T}}\n\\def\\False{\\ct{F}}\n\\]\n\n\nThe model\nLet’s now say a bit more about the model, \\(\\mathcal{M}_{\\textit{Eng.}}\\).\n\n\\(\\mathcal{M}_{\\textit{Eng.}} = ⟨D_{\\textit{Eng.}}, \\{\\True, \\False\\}, I_{\\textit{Eng.}}⟩\\)\n\nThe domain, \\(D_{\\textit{Eng.}}\\), we can take to be the set \\(\\{\\ct{z}, \\ct{b}\\}\\):\n\\[D_{\\textit{Eng.}} = \\{\\ct{z}, \\ct{b}\\}\\]\nLet us now also say a thing or two about the interpretation function, \\(I_{\\textit{Eng.}}\\). Consistent with our grammatical rules, we will:\n\ninterpret noun phrases as elements of the domain,\ninterpret verb phrases as characteristic functions of sets,\ninterpret auxiliaries as functions from characteristic functions to characteristic functions,\nand interpret connectives as functions from pairs of characteristic functions to characteristic functions.\n\nHere we go:\n\n\\(I_{\\textit{Eng.}}(\\textit{Ziggy}) = \\ct{z}\\)  \\(I_{\\textit{Eng.}}(\\textit{Bella}) = \\ct{b}\\)  \\(I_{\\textit{Eng.}}(\\textit{runs}) = \\{\\ct{z}, \\ct{b}\\}_{CF}\\)  \\(I_{\\textit{Eng.}}(\\textit{run}) = \\{\\ct{z}, \\ct{b}\\}_{CF}\\)  \\(I_{\\textit{Eng.}}(\\textit{jumps}) = \\{\\ct{z}\\}_{CF}\\)  \\(I_{\\textit{Eng.}}(\\textit{jump}) = \\{\\ct{z}\\}_{CF}\\)  \\(I_{\\textit{Eng.}}(\\textit{doesn't}) = (λf.(λx.¬f(x)))\\)  \\(I_{\\textit{Eng.}}(\\textit{and}) = (λ⟨f, g⟩.(λx.f(x) = g(x) = \\True))\\)\n\nRecall the notation \\((·)_{CF}\\), which we use to form characteristic functions out of sets. Given any subset (\\(S\\)) of the domain \\(D\\):\n\\[S_{CF} ≝ (λx.x ∈ S)\\]\nThat is, a characteristic function of such a subset \\(D\\) is a function whose domain is \\(D\\) itself—\\(\\{\\ct{z}, \\ct{b}\\}\\) above—and whose codomain is the set of truth values. Further, this function gives back \\(\\True\\) on any element of the domain which is a member of \\(S\\) (otherwise, it gives back \\(\\False\\)).\nAlso recall the “negation” function \\(¬ : \\{\\True, \\False\\} → \\{\\True, \\False\\}\\). This function just flips \\(\\True\\) to \\(\\False\\) and \\(\\False\\) to \\(\\True\\):\n\\[¬\\True = \\False\\] \\[¬\\False = \\True\\]\nAlright. It is time to apply our lexicon definition and grammatical rules from earlier—together with this model definition—to an example expression of English.\n\n\nDeriving an example\nHere’s a derivation of the expression \\(\\textit{Ziggy doesn't jump and runs}\\). (Note that you may need to scroll right to see everything.)\n\\[\\begin{prooftree}\n\\AxiomC{$⟨\\textit{Ziggy}, I_{\\textit{Eng.}}(Ziggy)⟩ ⊢ np$}\n\\AxiomC{$⟨\\textit{doesn't}, I_{\\textit{Eng.}}(doesn't)⟩ ⊢ aux$}\n\\AxiomC{$⟨\\textit{jump}, I_{\\textit{Eng.}}(jump)⟩ ⊢ bvp$}\n\\RightLabel{Rule 2}\\BinaryInfC{$⟨\\textit{doesn't jump}, I_{\\textit{Eng.}}(doesn't)(I_{\\textit{Eng.}}(jump))⟩ ⊢ vp$}\n\\AxiomC{$⟨\\textit{and}, I_{\\textit{Eng.}}(and)⟩ ⊢ con$}\n\\AxiomC{$⟨\\textit{runs}, I_{\\textit{Eng.}}(runs)⟩ ⊢ vp$}\n\\RightLabel{Rule 3}\\TrinaryInfC{$⟨\\textit{doesn't jump and runs}, I_{\\textit{Eng.}}(and)(I_{\\textit{Eng.}}(doesn't)(I_{\\textit{Eng.}}(jump)), I_{\\textit{Eng.}}(runs))⟩ ⊢ vp$}\n\\RightLabel{Rule 1}\\BinaryInfC{$⟨\\textit{Ziggy doesn't jump and runs}, I_{\\textit{Eng.}}(and)(I_{\\textit{Eng.}}(doesn't)(I_{\\textit{Eng.}}(jump)), I_{\\textit{Eng.}}(runs))(I_{\\textit{Eng.}}(Ziggy))⟩ ⊢ s$}\n\\end{prooftree}\\]\n\n\nSimplifying the result\nNow, all we have to do is substitute the cases in the definition of \\(I_{\\textit{Eng.}}\\) in (2) for their occurrences in the last line of this derivation. If we do this we get:\n\\[(λ⟨f, g⟩.(λx.f(x) = g(x) = \\True))((λf.(λx.¬f(x)))(\\{\\ct{z}\\}_{CF}), \\{\\ct{z}, \\ct{b}\\}_{CF})(\\ct{z})\\]\nThis is a monster of a metalanguage expression! Fear not: it is quite straightforward to reduce it into something simple, as long as we are careful. One thing we need to be careful about is where all of the parentheses are!\nNote that this expression can be divided up into three main parts. First, there is a function:\n\\[(λ⟨f, g⟩.(λx.f(x) = g(x) = \\True))\\]\nThat’s the first part. The second part is the function’s first argument, which is a pair:\n\\[⟨(λf.(λx.¬f(x)))(\\{\\ct{z}\\}_{CF}), \\{\\ct{z}, \\ct{b}\\}_{CF}⟩\\]\nAnd the third part is the function’s second argument, which is just Ziggy:\n\\[\\ct{z}\\]\nImportantly, the first argument of the function—the pair—is somewhat complex: it itself can be reduced into something simpler! Let’s talk about that.\n\n\nSimplifying the first argument\nThe first argument of the main expression above\n\\[⟨(λf.(λx.¬f(x)))(\\{\\ct{z}\\}_{CF}), \\{\\ct{z}, \\ct{b}\\}_{CF}⟩\\]\nis a pair, so it has two components. The first component of this pair is itself a function applied to an argument; specifically, it is the interpretation of doesn’t applied to the interpretation of jump:\n\\[(λf.(λx.¬f(x)))(\\{\\ct{z}\\}_{CF})\\]\nLet us simplify this expression by first getting rid of the \\(λf\\) at the beginning of the function and substituting the argument, \\(\\{\\ct{z}\\}_{CF}\\), for all occurrences of \\(f\\) that appear inside of the function. Indeed, there is only one occurrence, and we get:\n\\[(λx.¬\\{\\ct{z}\\}_{CF}(x))\\]\nGreat! If we put this result back into the pair we started with, we end up with the following pair, which is equivalent to the one we needed to simplify:\n\\[⟨(λx.¬\\{\\ct{z}\\}_{CF}(x)), \\{\\ct{z}, \\ct{b}\\}_{CF}⟩\\]\n\n\nSimplifying the main function applied to the first argument\nIf we apply the main function to its (now simplified) first argument, we obtain:\n\\[(λ⟨f, g⟩.(λx.f(x) = g(x) = \\True))((λx.¬\\{\\ct{z}\\}_{CF}(x)), \\{\\ct{z}, \\ct{b}\\}_{CF})\\]\nThis complex metalanguage expression may be reduced as well. In particular, we should substitute\n\\[(λx.¬\\{\\ct{z}\\}_{CF}(x))\\]\nin for the \\(f\\) component of the function’s pair-shaped argument; and we should substitute\n\\[\\{\\ct{z}, \\ct{b}\\}_{CF}\\]\nfor the \\(g\\) component of this argument. As before, the substitution should occur inside the function itself, after we get rid of the \\(λ⟨f, g⟩\\). If we do this complex substitution, we end up with\n\\[(λx.(λx.¬\\{\\ct{z}\\}_{CF}(x))(x) = \\{\\ct{z}, \\ct{b}\\}_{CF}(x) = \\True)\\]\nHaving performed this substitution, we end up with another expression that can be simplified—specifically, the one to the left of the first equals sign:\n\\[(λx.¬\\{\\ct{z}\\}_{CF}(x))(x)\\]\nThis, too, is a function—\\((λx.¬\\{\\ct{z}\\}_{CF}(x))\\)—applied to an argument—\\(x\\). In this case, we have to get rid of the \\(λx\\) and—perhaps, unintuitively—substitute \\(x\\) for the \\(x\\) inside the function! No problem. If we do that, we get\n\\[¬\\{\\ct{z}\\}_{CF}(x)\\]\nIf we now try to recover the original function applied to its first argument—that is by replacing \\((λx.¬\\{\\ct{z}\\}_{CF}(x))(x)\\) with its simplified version—we get\n\\[(λx.¬\\{\\ct{z}\\}_{CF}(x) = \\{\\ct{z}, \\ct{b}\\}_{CF}(x) = \\True)\\]\nIt’s much easier to see now that this is just a characteristic function of some set: it is the characteristic function of the set of elements of the domain (\\(\\{\\ct{z}, \\ct{b}\\}\\)) such that it is true that that element is not in the set \\(\\{\\ct{z}\\}\\) and it is also true that element is in the set \\(\\{\\ct{z}, \\ct{b}\\}\\). That’s just the characteristic function of the set containing \\(\\ct{b}\\)—\\(\\{\\ct{b}\\}_{CF}\\)!\nSince this is the result of applying the main function to its first argument, we now have to apply the main function—already applied to its first argument—to its second argument.\n\n\nSimplifying the main function (already applied to its first argument) applied to its second argument\nNow that we have the result of applying the main function to its first argument, we can recover the original function applied to both its arguments by replacing the relevant part with the result we got. If we do this, we get\n\\[(λx.¬\\{\\ct{z}\\}_{CF}(x) = \\{\\ct{z}, \\ct{b}\\}_{CF}(x) = \\True)(\\ct{z})\\]\nThus, all we have remaining is one more substitution. In this case, we need to get rid of the \\(λx\\) and substitute \\(\\ct{z}\\) inside the function, getting\n\\[¬\\{\\ct{z}\\}_{CF}(\\ct{z}) = \\{\\ct{z}, \\ct{b}\\}_{CF}(\\ct{z}) = \\True\\]\nThis whole expression evaluates to \\(\\True\\) just in case \\(¬\\{\\ct{z}\\}_{CF}(\\ct{z})\\) and \\(\\{\\ct{z}, \\ct{b}\\}_{CF}(\\ct{z})\\) are both true. Well… because \\(\\{\\ct{z}\\}_{CF}(\\ct{z})\\) is \\(\\True\\), we have that \\(¬\\{\\ct{z}\\}_{CF}(\\ct{z})\\) is \\(\\False\\). So, the meaning of Ziggy doesn’t jump and runs in \\(\\mathcal{M}_{\\textit{Eng.}}\\) is \\(\\False = \\True = \\True\\). Otherwise, known as \\(\\False\\)!\n\n\nFinal thoughts\nIt is worth going back up to the model definition and inspecting the meanings we provided for jump and runs. Hopefully, you can convince yourself that Ziggy doesn’t run and jumps really should be false—specifically, because Ziggy runs. I think it is then worth reflecting on the fact that we actually got this intuitive result in the end, simply by applying the rules of our grammar—carefully, but ultimately in a completely deterministic and mindless way—and then simplifying the resulting metalanguage expression.",
    "crumbs": [
      "Models",
      "An English model and example"
    ]
  },
  {
    "objectID": "convention/assignment.html",
    "href": "convention/assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "Due on Friday, September 5th.\n\nPart 1\nPick one of the other practices that were brought up in class, and give an analysis of whether or not it meets criteria 1-5 outlined in Defining convention. To remind you, these practices were:\n\nKnocking on the door before entering a room\nLooking both ways before crossing the street\nUsing an umbrella when it’s raining\nMaking WiFi free available to customers at coffee shops\n\nAlternatively, pick your own practice to analyze, and do the same. Whatever practice you choose need not be well-known, and we need not generally be consciously aware of its existence. It just needs to be something that people do within some community with which you’re familiar. (Please don’t use one of the ones that Lewis (1975) mentions, e.g., driving on a particular side of the road in a particular country.) You might use the discussion in these notes of ‘brushing your teeth’ as a model. Note—following those notes—that some aspects of the practice might appear to be conventional, while other aspects might seem less so. If you notice that this is true of the practice you analyze, please point it out.\nPlease consider and provide a response to the following question: is your analysis of the practice you’ve chosen consistent with your own intuitions about whether or not the practice is a convention?\n\n\nPart 2\nConsider the following state of affairs. Jo utters the sentence in (1) after her laptop freezes.\n\nThis damn computer is so slow!\n\nThere are a number of inferences you might draw based on this utterance. For example:\n\nThere is a computer.\nThe computer exceeds some degree of slowness, perhaps salient in the context of utterance, which is particularly high.\nJo is frustrated by her computer.\n\nThe last inference—that Jo is frustrated—appears to be triggered by her use of the word damn. In general, such words (called expressives) give rise to inferences of this kind, e.g., about an attitude held by the person who utters them; in this case, we draw an inference about Jo’s attitude toward her laptop.\nThis use of the word damn, which causes the frustration inference, appears, in some sense to mean that Jo is frustrated. Do you think that this notion of meaning is meaning\\(_{N}\\) or meaning\\(_{NN}\\)? That is, which of these relates utterances of (1)—or sentences containing the word damn, more generally—to inferences about frustration? Explain why, using the criteria for meaning\\(_{NN}\\) that Grice discusses.\n\n\nPart 3\nConsider the following different state of affairs. Bo maintains two forms of the verb be that he uses in the antecedents of counterfactual conditional sentences. For example, sometimes he utters sentences like (2)\n\nIf I was buying ice cream, I would probably get chocolate.\n\nwhile at other times he utters sentences like (3).\n\nIf I were buying ice cream, I would probably get chocolate.\n\nIn the first set of cases (exemplified by (2)), he inflects be as was when the subject is I, he, she, or a singular noun phrase, and as were when the subject is we, you, they, or a plural noun phrase. Meanwhile, in the second set of cases, (exemplified by (3)), he inflects be as were regardless of the person or number of the subject.\nImportantly, he tends to use these forms of be in this way in somewhat different social contexts. He is more likely to utter sentences like (2) (be -&gt; was) when he is with his friends or family or buying something at the grocery store (i.e., “informal” settings); while he is more likely to utter sentences like (3) (be -&gt; were) when he is answering a question in class or writing a paper (i.e., “formal” settings). In both cases, he appears to be signaling, in one way or another, something about his identity as a speaker of English and his relationship to his interlocutors. In the first set of contexts, he is not trying to speak “correctly” according to a received standard about spoken or written English, while in the second set of contexts, it is important to him that his English sound “correct”, and that his interlocutors draw the inference that he speaks English according to a particular formal standard.\nDo you think that the inference about Bo’s “correct” use of English in these contexts, i.e., when he uses were in cases in which he might otherwise use was is related to meaning\\(_{N}\\) or meaning\\(_{NN}\\)? In other words, in which sense of mean does Bo’s use of the form of the verb be mean that he uses English “correctly”? In the sense relevant to natural meaning, or in the sense relevant to non-natural meaning? Again, explain why, making reference to the criteria outlined by Grice.\n\n\n\n\n\n\n\nReferences\n\nLewis, David K. 1975. “Languages and Language.” In Arguing about Language, edited by Darragh Byrne and Max Kölbel. Arguing about Philosophy. New York: Routledge.",
    "crumbs": [
      "Convention",
      "Assignment"
    ]
  },
  {
    "objectID": "convention/defining_convention.html",
    "href": "convention/defining_convention.html",
    "title": "Defining convention",
    "section": "",
    "text": "Lewis’s goal in Lewis (1975) is to understand what is involved when a community of language users knows a language. Because he views language use in a community as the adoption by that community of a particular convention, he lays out a number of criteria that he thinks conventions, in general, ought to satisfy. According to Lewis, any given habit or practice that persists within some community is a convention just in case the following hold:\n\nEveryone conforms, i.e., participates in the habit.\nEveryone believes the other members of the community conform.\nThe other members of the community conforming provides a reason to conform.\nPeople generally prefer for the other members of the community to conform, rather than not conform.\nThe habit is optional or arbitrary, in the sense that it is selected from among some set of available alternative possible habits.\nThere is common knowledge within the community of the above criteria. That is, each community member knows (in some sense) about the convention, expects others to know about it, and expects others to have similar expectations.1\n\nHere, Lewis is engaged in what’s called “conceptual analysis”: he takes an ordinary English expression (convention), which English speakers regularly use with, perhaps at best, an implicit understanding of how it is used, and he attempts to elaborate necessary and sufficient conditions for its application. That is, Lewis takes conditions 1 through 6 to define the term convention. They all have to hold (they are necessary), and only they have to hold (they are sufficient), in order for something to be a convention. Why does he do this? Well in part, because he thinks the criteria in 1 - 6 isolate and shed light on an interesting array of phenomena that exist among our species—driving on the right vs. left of the road (depending on where one lives), greeting people in a particular way, knocking on doors to request entry, and a zillion other things; and in part, because as a philosopher of language, he thinks one of these kinds of phenomena—linguistic conventions—is particularly interesting. Does convention, when understood as abbreviating the conditions 1 through 6, have the same meaning as the ordinary English word convention? Maybe, maybe not. It doesn’t really matter very much for our purposes, since we just want a basic way of narrowing in on the phenomenon we’re studying (language, communication, meaning, etc.), along with, perhaps, what important features it has in common with certain other practices that people engage in.\nOne of the candidate conventions we looked at was teeth-brushing. I’ll go through the criteria here to see how well they apply to this particular practice.\n\n\n\nIt’s true that not everyone conforms (or always conforms), but not conforming is often noticed or frowned upon—something which at least appears to be true of genuine conventions, in general.\nWith certain exceptions, people expect each other to conform; this case doesn’t seem out of the ordinary for established conventions.\nIt is perhaps a little bit tricky to determine whether others brushing their teeth provides a reason for one to brush one’s own teeth. On the one hand, the main reasons people brush their teeth are to help prevent cavities and to not have bad breath. On the other hand, not having bad breath—while (or because) people believe it makes interactions with other people more pleasant—is often expected. In fact, there might exist a standard in societies in which teeth-brushing is common, according to which people shouldn’t be perceived as having bad breath; further, this standard may be reinforced when people actually do brush their teeth: the more people engage in the practice, the more social pressure there is to conform. If we entertain this possibility, it appears that other people brushing their teeth can provide a reason for one to brush one’s own teeth.\nThere appears to be a general preference that people brush their teeth. For one, people are often concerned about others’ hygiene because it affects their own experience (e.g., they don’t want to smell someone else’s bad breath). But also—insofar as standards of hygiene and cleanliness are arbitrary and culturally variable—I suspect people want to not have wasted their effort when they brush their own teeth. They might want their action to be justified, in part, by the existence of the standard. (Just think of how many people might stop brushing their teeth if the people around them stopped—probably at least a few?)\nWhether or not teeth-brushing is optional or arbitrary may also seem tricky to determine. On the one hand, it might not be clear what other practice would allow people to keep their teeth clean; on the other hand, it is a bit clearer what alternative practice might allow members of a community to adhere to a communal standard: the practice of not brushing your teeth. That is, we would just have to change the standard, and suddenly, people would be relieved of the social pressure to brush one’s teeth with which it coincides.\nInsofar as the above observations are accurate, they appear to be common knowledge. Common enough, at least, that you might expect to hear a reminder if you go to the dentist.\n\nI think there’s a good argument that teeth-brushing is (sometimes) a convention: it’s a convention when the community in which it is done adopts a standard (e.g., having healthy teeth, not being perceived as having bad breath)—one that teeth-brushing helps people meet. There might, of course, be other reasons to brush your teeth than the fact that it’s a convention.",
    "crumbs": [
      "Convention",
      "Defining convention"
    ]
  },
  {
    "objectID": "convention/defining_convention.html#footnotes",
    "href": "convention/defining_convention.html#footnotes",
    "title": "Defining convention",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Including the expectation that others have similar expectations!↩︎",
    "crumbs": [
      "Convention",
      "Defining convention"
    ]
  },
  {
    "objectID": "applicative_cg/overview.html",
    "href": "applicative_cg/overview.html",
    "title": "Overview",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\true{\\ct{T}}\n\\def\\false{\\ct{F}}\n\\]\n\nIn the last few weeks, we’ve introduced syntactic and semantic rules for English that aim to characterize two things at once: (i) the distributional properties of English expressions (e.g., that the \\(np\\) Ziggy can precede the \\(vp\\) meows, in order to make an \\(s\\)); and (ii) the semantic contributions these expressions make to the meanings of the larger expressions that contain them. Further, our rules are stated to be sensitive to the syntactic categories of the expressions whose behavior they characterize. For example, the rule\n\\[\\begin{prooftree}\n\\AxiomC{$⟨x, ⟦x⟧_{\\mathcal{M}}⟩ ⊢ np$}\n\\AxiomC{$⟨y, ⟦y⟧_{\\mathcal{M}}⟩ ⊢ vp$}\n\\BinaryInfC{$⟨x^{⌢}y, ⟦y⟧_{\\mathcal{M}}(⟦x⟧_{\\mathcal{M}})⟩ ⊢ s$}\n\\end{prooftree}\\]\nsays:\n\nthat you can form an \\(s\\) by sticking an \\(np\\) to the left of a \\(vp\\);\nthat you can get the meaning of the \\(s\\) by applying the meaning of the \\(vp\\) as a function to the meaning of the \\(np\\) as an argument.\n\nSo, syntactic categories have an important role: they say where things can go, and they also say what things can do (semantically) when they go there.\nThese notes further develop this idea, in two directions. First, we introduce a language of semantic types. Semantic types tell us what kind of meaning an expression can have—for example, whether it is an entity, a truth value, or some type of function (and if so, what type). Second, we introduce a language of syntactic categories (or syntactic types, if you like). Syntactic categories tell us what an expression’s distributional properties are—that is, what kinds of expressions can appear to its left and to its right. As we’ll see, there is a close relation between syntactic categories and semantic types. This relation holds because of the fact that where an expression can go inside some larger expression determines what type of meaning it can have.\nWe’ve already encoded a version of this idea in our grammar with categories like \\(np\\), \\(vp\\), and \\(tv\\), where we had rules that were specific to these categories. Our new language of categories will make the idea completely general: we’ll have an infinite number of possible syntactic categories, as well as an infinite number of possible semantic types, and each syntactic category will be seen to be associated with a semantic type. At the same time, we’ll be able to pare down the number of rules we have to only two!",
    "crumbs": [
      "Applicative categorial grammar",
      "Overview"
    ]
  },
  {
    "objectID": "applicative_cg/syn_types.html",
    "href": "applicative_cg/syn_types.html",
    "title": "Syntactic categories",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\true{\\ct{T}}\n\\def\\false{\\ct{F}}\n\\]\n\nThe collection of syntactic categories introduced in the last section is pretty minimal. So far, there are only three categories: \\(np\\), \\(pp\\), and \\(s\\)! In addition to noun phrases, prepositional phrases, and sentences, we would very much like to have syntactic categories for verb phrases, transitive verbs, ditranstive verbs—you name it. So, let’s introduce a system that allows us to take these three starting categories—which we can call base categories—and generate an infinite collection of possible syntactic categories. Each of these possible categories will encapsulate the syntactic distribution of whatever expression it is assigned to. Note that the way we treat syntactic categories here will be exactly analogous to what we did for semantic types, where we used \\(e\\) and \\(t\\) as our “base” (i.e., atomic) types and used the “arrow” operator to make new types out of old ones.\nSo, just like we had a rule for forming “arrow” types, let’s have a rule for forming what we can call “slash” categories.\n\nIf \\(α\\) and \\(β\\) are syntactic categories, then \\((α/β)\\) and \\((β\\backslash α)\\) are also syntactic categories.\n\nThus in addition to the base categories \\(np\\), \\(pp\\), and \\(s\\), we have categories like \\((pp/np)\\), \\((s\\backslash pp)\\), \\(((pp\\backslash np)/(s/s))\\), and so on.\nThe idea behind a slash category is that it represents either what kind of expression can occur to something’s right or what kind of expression can occur to something’s left. For example, a verb phrase (e.g., runs)—something whose category we have been calling \\(vp\\)—allows a noun phrase (e.g., Bella) to occur to its left; and when this happens, the result is a sentence (e.g., Bella runs). To capture this fact about verb phrases, we should assign them the syntactic category \\((np\\backslash s)\\). This category says: “if you stick an \\(np\\) (the thing to the left of the ‘\\(\\backslash\\)’) to my left, then together, we can make an \\(s\\)!”\nThis scheme for assigning categories to expressions allows them to (so to speak) announce what things they can occur to the left or to the right of. While a verb phrase takes an \\(np\\) to its left and, with it, makes an \\(s\\), a preposition (e.g., to) takes an \\(np\\) to its right and, with it, makes a \\(pp\\). Thus a preposition like to can be assigned the syntactic category \\((pp/np)\\). This category says: “if you stick an \\(np\\) (the thing to the right of the ‘\\(/\\)’) to my right, then together, we can make a \\(pp\\)!”\n\nThe syntactic structure of syntactic categories\nSyntactic categories formed with slashes (like \\((np\\backslash s)\\) and \\((pp/np)\\)) have an internal structure, which it is convenient to represent as a kind of syntactic tree structure (note that each slash annotating a node here is wrapped in parentheses for readability):\n  (\\)     (/)\n  / \\     / \\\n np  s   pp np\nCategories featuring multiple slashes can be represented this way, as well. Take the ditransitive verb introduced as an example, as in (2).\n\nJo introduced Bo to Mo.\n\nThis verb first takes a noun phrase to its right (introduced Bo); second, it takes a prepositional phrase to its right (introduced Bo to Mo); and third, it takes a noun phrase to its left (Jo introduced Bo to Mo). The verb introduced therefore has the following syntactic category, in tree form:\n      (/)\n      / \\\n    (/)  np\n    / \\\n  (\\) pp\n  / \\\n np  s\nThe way to read this tree is by starting at the very top (or root) node and looking at the direction of the slash. If it is a right slash (\\(/\\)), then the child node on the right is the category of the first argument of an expression with this category. If it is a left slash (\\(\\backslash\\)), then the child node on the left is the category of the first argument. Here, the root node is a right slash, so we look to the right child, which is \\(np\\)—that’s the thing introduce combines with first. Meanwhile, the left child is the category of the expression that results once you combine introduced with this noun phrase. Hence, the string introduced Bo has the category represented by the following tree:\n    (/)\n    / \\\n  (\\) pp\n  / \\\n np  s\nLikewise—assuming that to Mo has the category \\(pp\\)—introduced Bo to Mo has the following category:\n  (\\)\n  / \\\n np  s\nFinally—and in this case, we’re dealing with a left slash (\\(\\backslash\\))—we can combine this expression with Jo to get Jo introduced Bo to Mo, which therefore has the category \\(s\\).\nIn the more usual notation, we would write this very same syntactic category out as \\((((np\\backslash s)/pp)/np)\\). Both notations encode the order in which the verb takes its arguments, as well as whether each argument that it takes occurs to its left or to its right!\n\n\nThe correspondence between syntactic categories and semantic types\nSo now we have a whole language of syntactic categories, as well as a whole language of semantic types. Let’s keep developing the chart that we started in Semantic types, in order to incorporate a few more syntactic categories. Now, we will have not only noun phrases, prepositional phrases, and sentences, but also: verb phrases, transitive verbs, ditransitive verbs that take two noun phrases (e.g., show), ditransitive verbs that take a noun phrase and a prepositional phrase (e.g., introduce), and prepositions. Respectively:\n\n\n\n\n\n\n\n\nSyntactic category\nSemantic type\nDomain\n\n\n\n\n\\(np\\)\n\\(e\\)\n\\(D_{e} = D\\)\n\n\n\\(pp\\)\n\\(e\\)\n\\(D_{e} = D\\)\n\n\n\\(s\\)\n\\(t\\)\n\\(D_{t} = \\{\\true, \\false\\}\\)\n\n\n\\((np\\backslash s)\\)  (previously \\(vp\\))\n\\((e → t)\\)\n\n\n\n\\(((np\\backslash s)/np)\\)  (previously \\(tv\\))\n\\((e → (e → t))\\)\n\n\n\n\\((((np\\backslash s)/np)/np)\\)  (previously \\(dtv\\))\n\\((e → (e → (e → t)))\\)\n\n\n\n\\((((np\\backslash s)/pp)/np)\\)  (previously \\(datV\\))\n\\((e → (e → (e → t)))\\)\n\n\n\n\\((pp/np)\\)  (previously \\(p\\))\n\\((e → e)\\)\n\n\n\n\nBy lining syntactic categories up with their semantic types, we can begin to see a correspondence. Specifically we have something like the following set of rules:\n\nWherever there is an \\(np\\) or \\(pp\\) in the syntactic category, there is an \\(e\\) in the corresponding semantic type. And wherever there is an \\(s\\) in the syntactic category, there is a \\(t\\) in the semantic type.\nWherever there is a left slash (\\(\\backslash\\)) in the syntactic category, there is an arrow (\\(→\\)) in the semantic type.\nWherever there is a right slash (\\(/\\)) in the syntactic category, there is an arrow (\\(→\\)) in the semantic type; but this time, the arrow is “pointing the other way”—that is, the thing that goes to the right of the right slash goes to the left of the arrow.\n\nLet’s look at a couple of example of this correspondence, one simple, and one slightly more complex.\n\nFirst example\nFirst, let’s look at the very last line of the table—the one which relates the category of prepositions to the semantic type \\((e → e)\\). In tree notation, the syntactic category and the semantic type can be rendered as follows:\n (/)     ─&gt;\n / \\    / \\\npp np  e   e\nNote that in this example, the \\(e\\) which is the left child of the arrow corresponds to the \\(np\\) which is the right child of the slash, since it is a right slash. Both correspond, in some sense, to an argument: while \\(np\\) corresponds to the syntactic argument—the object of the preposition—\\(e\\) corresponds to the semantic argument of the function denoted by the preposition.\nMeanwhile, the \\(e\\) which is the right child of the arrow corresponds to the \\(pp\\) which is the left child of the slash. Both correspond, in some sonse, to the result—either the result of concatenating the two expressions (in the case of the syntactic category), or the result of applying the function (in the case of the semantic type).\n\n\nSecond example\nFor a slightly more complex example, let’s look at the fifth line of the table—the one for transitive verbs. In tree notation, we have the following syntactic category and semantic type:\n   (/)         ─&gt;\n   / \\        / \\\n (\\) np      e  ─&gt;\n / \\           / \\\nnp  s         e   t\nIn this example, the \\(e\\) which is the left child of the arrow at the root corresponds to the \\(np\\) which is the right child of the slash, since it is a right slash. Meanwhile, the left child of this same slash corresponds to the type \\((e → t)\\) which is the right child of the arrow at the root of the semantic type. Since this left child in the syntactic category is a category formed from a left slash, its left child—the \\(np\\)—corresponds to the \\(e\\), while its right child—the \\(s\\)—corresponds to the \\(t\\).\n\n\n\nRules\nNow that we have explored the structure of both syntactic categories and semantic types, as well as how these structures correspond to each other, let’s formalize the way that syntactic categories may actually be used. That is, how do we use them to derive complex expressions?\nWe need just two rules: one for allowing an expression to combine with something to its right (Right Application), and one for allowing an expression to combine with something to its left (Left Application).\n\nRight Application:  \\(\\begin{prooftree}\n\\AxiomC{\\(⟨x, ⟦x⟧_{\\mathcal{M}}⟩ ⊢ (a/b)\\)}\n\\AxiomC{\\(⟨y, ⟦y⟧_{\\mathcal{M}}⟩ ⊢ b\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(⟨x^{⌢}y, ⟦x⟧_{\\mathcal{M}}(⟦y⟧_{\\mathcal{M}})⟩ ⊢ a\\)}\n\\end{prooftree}\\)  Left application:  \\(\\begin{prooftree}\n\\AxiomC{\\(⟨x, ⟦x⟧_{\\mathcal{M}}⟩ ⊢ b\\)}\n\\AxiomC{\\(⟨y, ⟦y⟧_{\\mathcal{M}}⟩ ⊢ (b\\backslash a)\\)}\n\\RightLabel{\\(\\backslash\\)}\\BinaryInfC{\\(⟨x^{⌢}y, ⟦y⟧_{\\mathcal{M}}(⟦x⟧_{\\mathcal{M}})⟩ ⊢ a\\)}\n\\end{prooftree}\\)\n\nIn words, Right Application concatenates a string of category \\((a/b)\\) with a string of category \\(b\\) to form a string of category \\(a\\). At the same time, it applies the meaning of \\(x\\)—a function—to the meaning of \\(y\\)—its argument.\nLeft Application goes the other way: it concatenates a string of category \\(b\\) with a string of category \\((b\\backslash a)\\) to form a string of category \\(a\\). And it applies the meaning of \\(y\\)—a function—to the mean of \\(x\\)—its argument.\nImportantly, because of the correspondence between syntactic categories and their associated semantic types, we can actually guarantee that in each case, the function is the correct kind of thing to apply to the argument. That is, if the function denoted by \\(x\\) takes entity arguments (in the case of Right Application), then the meaning of \\(y\\) has to be an entity; if it takes truth value arguments, then the meaning of \\(y\\) has to be a truth value. The correspondence between syntactic categories and semantic types guarantees that these things will line up properly!\n\nExample derivation\nTo illustrate these rules, let’s derive the sentence in (4), which features the transitive verb bites (and allows Ziggy to get her revenge).\n\nZiggy bites Bella.\n\n\\[\\begin{prooftree}\n\\AxiomC{\\(⟨\\textit{Ziggy}, \\ct{z}⟩ ⊢ np\\)}\n\\AxiomC{\\(⟨\\textit{bites}, (λx.(λy.\\{⟨\\ct{b}, \\ct{z}⟩, ⟨\\ct{z}, \\ct{b}⟩\\}_{CF}(x, y)))⟩ ⊢ ((np\\backslash s)/np)\\)}\n\\AxiomC{\\(⟨\\textit{Bella}, \\ct{b}⟩ ⊢ np\\)}\n\\RightLabel{\\(/\\)}\\BinaryInfC{\\(⟨\\textit{bites Bella}, (λx.(λy.\\{⟨\\ct{b}, \\ct{z}⟩, ⟨\\ct{b}, \\ct{z}⟩\\}_{CF}(x, y)))(\\ct{b})⟩ ⊢ (np\\backslash s)\\)}\n\\RightLabel{\\(\\backslash\\)}\\BinaryInfC{\\(⟨\\textit{Ziggy bites Bella}, (λx.(λy.\\{⟨\\ct{b}, \\ct{z}⟩, ⟨\\ct{b}, \\ct{z}⟩\\}_{CF}(x, y)))(\\ct{b})(\\ct{z})⟩ ⊢ s\\)}\n\\end{prooftree}\\]\nNote that I have assumed a denotation for bites here that makes it the case that Ziggy and Bella bite each other. Thus if we evaluate the resulting expression\n\\[(λx.(λy.\\{⟨\\ct{b}, \\ct{z}⟩, ⟨\\ct{z}, \\ct{b}⟩\\}_{CF}(x, y)))(\\ct{b})(\\ct{z})\\]\nby first plugging \\(\\ct{b}\\) in for \\(x\\)—\n\\[(λy.\\{⟨\\ct{b}, \\ct{z}⟩, ⟨\\ct{z}, \\ct{b}⟩\\}_{CF}(\\ct{b}, y))(\\ct{z})\\]\n—and then plugging \\(\\ct{z}\\) in for \\(y\\)—we end up with\n\\[\\{⟨\\ct{b}, \\ct{z}⟩, ⟨\\ct{z}, \\ct{b}⟩\\}_{CF}(\\ct{b}, \\ct{z})\\]\nwhich is \\(\\true\\)!",
    "crumbs": [
      "Applicative categorial grammar",
      "Syntactic categories"
    ]
  },
  {
    "objectID": "diagnosing_inference/overview.html",
    "href": "diagnosing_inference/overview.html",
    "title": "Overview",
    "section": "",
    "text": "The data we tend to be interested in in semantics is inference judgment data. We can view an inference as being a pair of expressions of the language (or utterances of such expressions)—\\(E_{1}\\) and \\(E_{2}\\)—where \\(E_{2}\\) is generally a declarative sentence. In that case, we can ask if using \\(E_{1}\\) triggers an inference that \\(E_{2}\\) is true. If it does, then we say that \\(E_{2}\\) is an inference of \\(E_{1}\\) (or of an utterance of \\(E_{1}\\)).\nThere are four kinds of inference that we’d like to be able to classify. These are:\n\nentailments\nconversational implicatures\npresuppositions\nconventional implicatures\n\nEach of these kinds of inference can itself be viewed as a relation between expressions, or utterances of expressions. For example, we can say that sentence 1 entails sentence 2, or that it conversationally implicates it. Strictly speaking, it is really an utterance of a sentence that would trigger a conversational implicature—one wherefrom it is assumed that the person making the utterance is following certain conversational principles—but linguists often say that a sentence triggers a conversational implicature as a shorthand to mean that utterances of the sentence tend to give rise to the implicature.\nLikewise, expressions of different syntactic categories can presuppose certain sentences, or they can conventionally implicate them.",
    "crumbs": [
      "Diagnosing inference",
      "Overview"
    ]
  },
  {
    "objectID": "diagnosing_inference/entailments_conv_impl.html",
    "href": "diagnosing_inference/entailments_conv_impl.html",
    "title": "Entailments and presuppositions versus conversational implicatures",
    "section": "",
    "text": "The most prominent difference between entailments and presuppositions on the one hand, and conversational implicatures on the other, is that the latter tend to be defeasible, while the former tend not to be. Conversational implicatures are defined as arising from the maxims, after all—so you can always just claim that you weren’t actually following them!\n\nThe non-defeasability of entailments\n\n#Jo has at least three siblings, {and, but} she doesn’t have at least two siblings.\n#Jo is Bo’s sibling, {and, but} Jo and Bo aren’t siblings.\n#No one came to class, {and, but} it’s not the case that Julian didn’t come to class.\n\nEach of these examples seems to be weird and contradictory. That’s because an entailment of the first clause in each example is negated in the second clause. That is, Jo has at least three siblings entails Jo has at least two siblings; Jo is Bo’s sibling entails Jo and Bo are siblings; and no one came to class entails Julian didn’t come to class.\n\n\nThe non-defeasibility of presuppositions\nLike entailments, presuppositions also appear to be non-defeasible.\n\n#Jo stopped smoking, {and, but} she didn’t used to smoke.\n#Jo loves that it’s raining, {and, but} it’s not raining.\n#Bo failed the assignment, {and, but} there isn’t an assignment.\n#Bo failed the assignment again, {and, but} he hasn’t failed it before.\n\n\n\nThe defeasibility of conversational implicatures\n\nJo did some of the readings. In fact, she did all of them.\nI want something with lots of caffeine. In fact, I want a matcha latte.\nContext: Jo is writing a letter of recommendation for Bo for a linguistics instructor position.  Jo: Bo is very punctual. He has great handwriting. He kicked my ass in Hungry Hungry Hippos. Last, but not least, he’d be a great instructor in your linguistics program!\n\nThese examples illustrate that conversational implicatures, unlike entailments, may be canceled. In (8), the implicature that Jo did not do all of the readings is canceled; in (9), the ignorance implicature is canceled; and in (10), the implicature that Bo should not be hired as a linguistics instructor that arises when Jo flouts relation is canceled.",
    "crumbs": [
      "Diagnosing inference",
      "Entailments and presuppositions versus conversational implicatures"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site contains materials the course Introduction to Semantics given by Julian Grove at the University of Florida during the Fall 2025 semester.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#about-the-instructor",
    "href": "about.html#about-the-instructor",
    "title": "About",
    "section": "About the instructor",
    "text": "About the instructor\nJulian Grove is an Assistant Professor in the Linguistics Department. Long story short, he is interested in combining computational tools from Bayesian data analysis and programming language theory and bringing them to semantics. You can find out more about his research interests by checking out his papers, slides, or perhaps even his cv.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#about-the-site",
    "href": "about.html#about-the-site",
    "title": "About",
    "section": "About the site",
    "text": "About the site\nThe site itself is built using Quarto. The source files for this site are available on github at juliangrove/intro-semantics.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#license",
    "href": "about.html#license",
    "title": "About",
    "section": "License ",
    "text": "License \nIntroduction to Semantics by Julian Grove is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. Based on materials at https://github.com/juliangrove/intro-semantics.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "verbs/intransitives.html",
    "href": "verbs/intransitives.html",
    "title": "Intransitive verbs",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\true{\\ct{T}}\n\\def\\false{\\ct{F}}\n\\]\n\nIn Syntactic categories, we said that intransitive verbs have the syntactic category \\((np\\backslash s)\\): they take a noun phrase to their left, and together with it, they produce a sentence. And because they have this category, they have the semantic type \\((e → t)\\): they take an entity as their semantic argument, and they produce a truth value—that is, they denote characteristic functions.\nTake the verb sleeps as an example.\n\nPo sleeps.\n\nTo derive the sentence in (1), we can assign Po and sleeps the lexical entries in (2).\n\n\\(⟨\\textit{Po}, \\ct{p}⟩ ⊢ np\\)  \\(⟨\\textit{sleeps}, (λx.\\ct{sleep}(x))⟩ ⊢ (np\\backslash s)\\)\n\nAs usual, \\(\\ct{p}\\) is an entity—some element of the domain of whatever model we happen to be considering: \\[\\ct{p} ∈ D_{e}\\] Meanwhile, \\(\\ct{sleep}\\) is a characteristic function of a set of elements of this very same domain: \\[\\ct{sleep} : D_{e} → \\{\\true, \\false\\}\\] It takes an entity, and it gives you back a truth value (\\(\\true\\) or \\(\\false\\)).\nWhich characteristic function is it? Who knows. Or rather, we’re just using the expression ‘\\(\\ct{sleep}\\)’ to stand for \\(I_{\\mathcal{M}}(\\textit{sleeps})\\), for whatever the model \\(\\mathcal{M}\\) happens to be.\nUsing the lexical entries for Po and sleeps, we can now derive the sentence in (1):\n\\[\\begin{prooftree}\n\\AxiomC{\\(⟨\\textit{Po}, \\ct{p}⟩ ⊢ np\\)}\n\\AxiomC{\\(⟨\\textit{sleeps}, (λx.\\ct{sleep}(x))⟩ ⊢ (np\\backslash s)\\)}\n\\RightLabel{\\(\\backslash\\)}\\BinaryInfC{\\(⟨\\textit{Po sleeps}, (λx.\\ct{sleep}(x))(\\ct{p})⟩ ⊢ s\\)}\n\\end{prooftree}\\] As we’d expect from the semantic type of \\(s\\), the meaning derived for the entire sentence,\n\\[(λx.\\ct{sleep}(x))(\\ct{p})\\] \\[⇒ \\ct{sleep}(\\ct{p})\\]\nis a truth value—\\(\\true\\) or \\(\\false\\), depending on which of these values \\(\\ct{sleep}\\) maps \\(\\ct{p}\\) to.",
    "crumbs": [
      "Verbs",
      "Intransitive verbs"
    ]
  },
  {
    "objectID": "verbs/note.html",
    "href": "verbs/note.html",
    "title": "Characterizing entailments",
    "section": "",
    "text": "\\[\n\\newcommand{\\expr}[3]{\\begin{array}{c}\n#1 \\\\\n\\bbox[lightblue,5px]{#2}\n\\end{array} ⊢ #3}\n\\newcommand{\\ct}[1]{\\bbox[font-size: 0.8em]{\\mathsf{#1}}}\n\\newcommand{\\abbr}[1]{\\bbox[transform: scale(0.95)]{\\mathtt{#1}}}\n\\def\\true{\\ct{T}}\n\\def\\false{\\ct{F}}\n\\]\n\nIn other words, no matter what the model is, both of the sentences we analyzed in Ditransitive verbs\n\n\nPo showed noodles to Tigress. \nPo showed Tigress noodles.\n\n\ndenote the same truth value—whatever \\(\\ct{show}(\\ct{p}, \\ct{n}, \\ct{ti})\\) is. While this value is either \\(\\true\\) or \\(\\false\\), we cannot say which one until we know what model we are “in”. What we do know, however, is that whenever the meaning of (1-a) is \\(\\true\\) in some model, then so is the meaning of (1-b)—and vice versa.\nWhat’s important here is that we can account for the fact that the sentences in (1-a) and (1-b) entail each other. We account for this by ensuring that they mean the same thing no matter what model is used to interpret them. In particular we have the following relation between the sets of models in which (1-a) and (1-b) are true:\n\\[\\{\\mathcal{M} ∣ ⟦\\textit{Po showed noodles to Tigress}⟧_{\\mathcal{M}} = \\true\\} ⊆ \\{\\mathcal{M} ∣ ⟦\\textit{Po showed Tigress noodles}⟧_{\\mathcal{M}} = \\true\\}\\] \\[\\{\\mathcal{M} ∣ ⟦\\textit{Po showed Tigress noodles}⟧_{\\mathcal{M}} = \\true\\} ⊆ \\{\\mathcal{M} ∣ ⟦\\textit{Po showed noodles to Tigress}⟧_{\\mathcal{M}} = \\true\\}\\]\nIn words: the set of models in which the interpretation of (1-a) is \\(\\true\\) is a subset of the set of models in which the interpretation of (1-b) is \\(\\true\\)—and vice versa.\nIn general, when one sentence of the language we are studying (e.g., English) entails another sentence of that language, we would like our semantic analysis of the two sentences to ensure that the set of models in which the first sentence comes out true is a subset of the set of models in which the second sentence comes out true.",
    "crumbs": [
      "Verbs",
      "Characterizing entailments"
    ]
  }
]